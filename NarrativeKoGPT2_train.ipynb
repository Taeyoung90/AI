{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0p6JShGUnpZ"
   },
   "source": [
    "# NarrativeKoGPT2 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JV7_Ye-1UuS2"
   },
   "source": [
    "## 1.Google Drive 연동\n",
    "- 모델 파일과 학습 데이터가 저장 되어있는 구글 드라이브의 디렉토리와 Colab을 연동.  \n",
    "- 좌측상단 메뉴에서 런타임-> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU 선택 후 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18LqQI0SVNX9"
   },
   "source": [
    "### 1.1 GPU 연동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2666,
     "status": "ok",
     "timestamp": 1584698768724,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "CmKD5vgYUeTa",
    "outputId": "b0ad3aed-4402-47d1-baf0-fad7f14a6c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 17 13:15:45 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2080    Off  | 00000000:17:00.0 Off |                  N/A |\n",
      "| 18%   23C    P8    11W / 215W |     10MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 2080    Off  | 00000000:B3:00.0  On |                  N/A |\n",
      "| 18%   25C    P8    20W / 215W |    221MiB /  7974MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1206      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      1288      G   /usr/bin/gnome-shell                0MiB |\n",
      "|    0   N/A  N/A      1507      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      1610      G   ...mviewer/tv_bin/TeamViewer        0MiB |\n",
      "|    0   N/A  N/A      1637      G   /usr/bin/gnome-shell                0MiB |\n",
      "|    1   N/A  N/A      1206      G   /usr/lib/xorg/Xorg                 18MiB |\n",
      "|    1   N/A  N/A      1288      G   /usr/bin/gnome-shell               51MiB |\n",
      "|    1   N/A  N/A      1507      G   /usr/lib/xorg/Xorg                 84MiB |\n",
      "|    1   N/A  N/A      1610      G   ...mviewer/tv_bin/TeamViewer        2MiB |\n",
      "|    1   N/A  N/A      1637      G   /usr/bin/gnome-shell               59MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vi2gIIroVXeS"
   },
   "source": [
    "### 1.2 Google Drive 연동\n",
    "아래 코드를 실행후 나오는 URL을 클릭하여 나오는 인증 코드 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21888,
     "status": "ok",
     "timestamp": 1584698791269,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "n2tPgkJzUmBF",
    "outputId": "e36dff32-b78c-411c-d07e-247bcdc22715"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4377,
     "status": "ok",
     "timestamp": 1584682733302,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "Pg07ZiFiVjJU",
    "outputId": "31b99529-688a-498a-895c-396a96bac8d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KS8lBXaKR6TF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpCSwfFGkRx7"
   },
   "source": [
    "**Colab 디렉토리 아래 NarrativeKoGPT2 경로 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1584690917331,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "a7arJ4k2XLG_",
    "outputId": "b27dd97a-5cd5-4c7a-f9f4-c9fc5ec61476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: 'drive/My Drive/Colab Notebooks/'에 접근할 수 없습니다: 그런 파일이나 디렉터리가 없습니다\r\n"
     ]
    }
   ],
   "source": [
    "!ls drive/'My Drive'/'Colab Notebooks'/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVmhNd21kse2"
   },
   "source": [
    "**필요 패키지들 설치**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20771,
     "status": "ok",
     "timestamp": 1584698816577,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "FDrIL81uXPB0",
    "outputId": "adbcd3a7-235c-456f-ece6-04b4aa5f00b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonnlp>=0.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 5.1MB/s \n",
      "\u001b[?25hCollecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
      "\u001b[K     |████████████████████████████████| 68.7MB 44kB/s \n",
      "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 54.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 4)) (1.4.0)\n",
      "Collecting transformers>=2.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
      "\u001b[K     |████████████████████████████████| 501kB 51.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 6)) (4.38.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (1.18.2)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (0.29.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (20.3)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (1.12.23)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2019.12.20)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 56.7MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 57.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 2)) (2019.11.28)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (1.15.23)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.9.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.14.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/NarrativeKoGPT2/requirements.txt (line 5)) (2.8.1)\n",
      "Building wheels for collected packages: gluonnlp, sacremoses\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470974 sha256=1952a212eebb3e8949417908c0f399ab4d383f3dbc0acedd4f219c440260ba14\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=4cc8105063ef093f8c730b0326be3e98f77791a6f5aec4852170b12d4bcc695c\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built gluonnlp sacremoses\n",
      "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, tokenizers, sacremoses, transformers\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r drive/'My Drive'/'Colab Notebooks'/NarrativeKoGPT2/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1584698825809,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "vSCmVmaTlZ5S",
    "outputId": "0337cecb-5240-4c4e-8b6c-42114cb45d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hnSOCChk9lU"
   },
   "source": [
    "## 2.KoGPT2 Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL6xVLtHn6vK"
   },
   "source": [
    "### 2.1.Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13813,
     "status": "ok",
     "timestamp": 1584698842737,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "-IGI-Rcakhsw",
    "outputId": "43ac441c-c395-4de8-b814-af71aa13a731"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/pytorch/lib/python3.7/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NarrativeKoGPT2.kogpt2'; 'NarrativeKoGPT2' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5c5fbb33f895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m \u001b[0;31m# 데이터로더\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgluonnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencepieceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNarrativeKoGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkogpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNarrativeKoGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkogpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNarrativeKoGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI project/NarrativeKoGPT2/narrativeKoGPT2/NarrativeKoGPT2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgluonnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencepieceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNarrativeKoGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkogpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNarrativeKoGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkogpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNarrativeKoGPT2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NarrativeKoGPT2.kogpt2'; 'NarrativeKoGPT2' is not a package"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "from gluonnlp.data import SentencepieceTokenizer \n",
    "from NarrativeKoGPT2.kogpt2.utils import get_tokenizer\n",
    "from NarrativeKoGPT2.kogpt2.utils import download, tokenizer\n",
    "from NarrativeKoGPT2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "from NarrativeKoGPT2.util.data import NovelDataset\n",
    "import gluonnlp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "from gluonnlp.data import SentencepieceTokenizer \n",
    "from kogpt2.utils import get_tokenizer\n",
    "from kogpt2.utils import download, tokenizer\n",
    "from model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "from util.data import NovelDataset\n",
    "import gluonnlp\n",
    "from tqdm import tqdm\n",
    "#import 위치 맞춰줌 앞의 NarrativeKoGPT2 필요없음 현재 폴더 기준이므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement NarrativeKoGPT2\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for NarrativeKoGPT2\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install NarrativeKoGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01vGgfaNIDT_"
   },
   "source": [
    "**torch GPU 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1584688418157,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "ztdqTt3OIBPI",
    "outputId": "8991ed4a-104d-4ed5-f2bf-19a654fff0bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())  # GPU deviec count check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G20dHg4mn5x4"
   },
   "source": [
    "### 2.2. koGPT-2 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPoFzMKkk8eB"
   },
   "outputs": [],
   "source": [
    "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
    "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
    "epoch =30  # 학습 epoch\n",
    "save_path = '/home/piai/AI project/NarrativeKoGPT2/narrativeKoGPT2/checkpoint/'\n",
    "#use_cuda = True # Colab내 GPU 사용을 위한 값\n",
    "\n",
    "pytorch_kogpt2 = {\n",
    "    'url':\n",
    "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'chksum': '676e9bcfa7'\n",
    "}\n",
    "kogpt2_config = {\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"layer_norm_epsilon\": 1e-05,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_positions\": 1024,\n",
    "    \"vocab_size\": 50000 # 50000기본\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xIXykCtn45d"
   },
   "source": [
    "### 2.3 Model and Vocab Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57506,
     "status": "ok",
     "timestamp": 1584698905026,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "kvtLJGh5o0MZ",
    "outputId": "5ee66192-e5df-47c9-b62f-366da32cca71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# download model\n",
    "model_info = pytorch_kogpt2\n",
    "model_path = download(model_info['url'],\n",
    "                       model_info['fname'],\n",
    "                       model_info['chksum'],\n",
    "                       cachedir=cachedir)\n",
    "# download vocab\n",
    "vocab_info = tokenizer\n",
    "vocab_path = download(vocab_info['url'],\n",
    "                       vocab_info['fname'],\n",
    "                       vocab_info['chksum'],\n",
    "                       cachedir=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fu7-2csBpLQR"
   },
   "source": [
    "### 2.4.KoGPT-2 Model Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5AK_S6spqwQ"
   },
   "outputs": [],
   "source": [
    "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
    "kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "# model_path로부터 다운로드 받은 내용을 load_state_dict으로 업로드\n",
    "kogpt2model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device(ctx)\n",
    "kogpt2model.to(device)\n",
    "\n",
    "# kogpt2model.eval()\n",
    "# 추가로 학습하기 위해 .train() 사용\n",
    "kogpt2model.train()\n",
    "vocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
    "                                                     mask_token=None,\n",
    "                                                     sep_token=None,\n",
    "                                                     cls_token=None,\n",
    "                                                     unknown_token='<unk>',\n",
    "                                                     padding_token='<pad>',\n",
    "                                                     bos_token='<s>',\n",
    "                                                     eos_token='</s>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rnV010Wq9Xw"
   },
   "source": [
    "### 2.5. Get Batch Data using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2925,
     "status": "ok",
     "timestamp": 1584699184992,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "Ukfj9FPHpwfk",
    "outputId": "97d001d4-5220-4f26-fd12-c794b67c2510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tok_path = get_tokenizer()\n",
    "model, vocab = kogpt2model, vocab_b_obj\n",
    "sentencepieceTokenizer = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "#os.chdir(\"../\")\n",
    "data_file_path = '/home/piai/AI project/Dataset/ratings_train_document_noindex_1000.csv'\n",
    "batch_size = 1\n",
    "novel_dataset = NovelDataset(data_file_path, vocab,sentencepieceTokenizer)\n",
    "novel_data_loader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RFndCOIrLS0"
   },
   "source": [
    "### 2.6. Learning rate, Loss function, Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pY_o_C-qBhz"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMadZrwzXjbM"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    \"\"\"Get the current gpu usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage: dict\n",
    "        Keys are device ids as integers.\n",
    "        Values are memory usage as integers in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ], encoding='utf-8')\n",
    "    # Convert lines into a dictionary\n",
    "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "    return gpu_memory_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sgd28DRhthzo"
   },
   "source": [
    "### 2.7. KoGPT-2 Transfer Laerning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1584690255916,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "pbKCkcY63Y4a",
    "outputId": "f81332b5-4469-42e6-cb1c-da0324161bde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/home/piai/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1462,
     "status": "error",
     "timestamp": 1584759691454,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "yYkDU-cbrduY",
    "outputId": "ce32766a-431f-45fc-e784-518e670e7a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoGPT-2 Transfer Learning Start\n",
      "epoch no.0 train no.1  loss = 7.046463489532471\n",
      "epoch no.0 train no.11  loss = 5.954883575439453\n",
      "epoch no.0 train no.21  loss = 4.949443817138672\n",
      "epoch no.0 train no.31  loss = 4.883050918579102\n",
      "epoch no.0 train no.41  loss = 6.016555309295654\n",
      "epoch no.0 train no.51  loss = 2.9170634746551514\n",
      "epoch no.0 train no.61  loss = 5.77877140045166\n",
      "epoch no.0 train no.71  loss = 4.958684921264648\n",
      "epoch no.0 train no.81  loss = 4.843178749084473\n",
      "epoch no.0 train no.91  loss = 5.756586074829102\n",
      "epoch no.0 train no.101  loss = 6.17048454284668\n",
      "epoch no.0 train no.111  loss = 4.237728595733643\n",
      "epoch no.0 train no.121  loss = 5.130053997039795\n",
      "epoch no.0 train no.131  loss = 4.233777046203613\n",
      "epoch no.0 train no.141  loss = 3.838409900665283\n",
      "epoch no.0 train no.151  loss = 5.796337127685547\n",
      "epoch no.0 train no.161  loss = 5.366102695465088\n",
      "epoch no.0 train no.171  loss = 4.242149353027344\n",
      "epoch no.0 train no.181  loss = 5.539809703826904\n",
      "epoch no.0 train no.191  loss = 5.727437496185303\n",
      "epoch no.0 train no.201  loss = 3.490368127822876\n",
      "epoch no.0 train no.211  loss = 4.165024757385254\n",
      "epoch no.0 train no.221  loss = 5.587996006011963\n",
      "epoch no.0 train no.231  loss = 5.555393218994141\n",
      "epoch no.0 train no.241  loss = 5.453290939331055\n",
      "epoch no.0 train no.251  loss = 4.8206024169921875\n",
      "epoch no.0 train no.261  loss = 5.847075462341309\n",
      "epoch no.0 train no.271  loss = 5.068885326385498\n",
      "epoch no.0 train no.281  loss = 5.230132102966309\n",
      "epoch no.0 train no.291  loss = 4.463024139404297\n",
      "epoch no.0 train no.301  loss = 3.9423115253448486\n",
      "epoch no.0 train no.311  loss = 4.647444248199463\n",
      "epoch no.0 train no.321  loss = 4.14587926864624\n",
      "epoch no.0 train no.331  loss = 4.386711120605469\n",
      "epoch no.0 train no.341  loss = 5.706686019897461\n",
      "epoch no.0 train no.351  loss = 4.888789176940918\n",
      "epoch no.0 train no.361  loss = 3.8886666297912598\n",
      "epoch no.0 train no.371  loss = 5.881098747253418\n",
      "epoch no.0 train no.381  loss = 5.202062606811523\n",
      "epoch no.0 train no.391  loss = 5.620136737823486\n",
      "epoch no.0 train no.401  loss = 4.857631206512451\n",
      "epoch no.0 train no.411  loss = 3.161045789718628\n",
      "epoch no.0 train no.421  loss = 3.430562734603882\n",
      "epoch no.0 train no.431  loss = 4.312275409698486\n",
      "epoch no.0 train no.441  loss = 5.195554256439209\n",
      "epoch no.0 train no.451  loss = 5.038614273071289\n",
      "epoch no.0 train no.461  loss = 3.982588768005371\n",
      "epoch no.0 train no.471  loss = 5.380097389221191\n",
      "epoch no.0 train no.481  loss = 4.412037372589111\n",
      "epoch no.0 train no.491  loss = 4.763235092163086\n",
      "epoch no.0 train no.501  loss = 3.6401307582855225\n",
      "epoch no.0 train no.511  loss = 4.309579372406006\n",
      "epoch no.0 train no.521  loss = 4.36053466796875\n",
      "epoch no.0 train no.531  loss = 5.417447566986084\n",
      "epoch no.0 train no.541  loss = 3.979785442352295\n",
      "epoch no.0 train no.551  loss = 4.467719078063965\n",
      "epoch no.0 train no.561  loss = 4.951632499694824\n",
      "epoch no.0 train no.571  loss = 5.709056377410889\n",
      "epoch no.0 train no.581  loss = 4.75457763671875\n",
      "epoch no.0 train no.591  loss = 5.196364879608154\n",
      "epoch no.0 train no.601  loss = 5.049437999725342\n",
      "epoch no.0 train no.611  loss = 5.376339435577393\n",
      "epoch no.0 train no.621  loss = 5.54011869430542\n",
      "epoch no.0 train no.631  loss = 4.483762264251709\n",
      "epoch no.0 train no.641  loss = 3.576054811477661\n",
      "epoch no.0 train no.651  loss = 5.435659408569336\n",
      "epoch no.0 train no.661  loss = 4.797980785369873\n",
      "epoch no.0 train no.671  loss = 4.952757835388184\n",
      "epoch no.0 train no.681  loss = 4.831793308258057\n",
      "epoch no.0 train no.691  loss = 4.572750568389893\n",
      "epoch no.0 train no.701  loss = 4.03702974319458\n",
      "epoch no.0 train no.711  loss = 4.664037704467773\n",
      "epoch no.0 train no.721  loss = 5.558606147766113\n",
      "epoch no.0 train no.731  loss = 5.523199081420898\n",
      "epoch no.0 train no.741  loss = 4.694223403930664\n",
      "epoch no.0 train no.751  loss = 5.574324607849121\n",
      "epoch no.0 train no.761  loss = 5.531923770904541\n",
      "epoch no.0 train no.771  loss = 4.288594722747803\n",
      "epoch no.0 train no.781  loss = 5.16939640045166\n",
      "epoch no.0 train no.791  loss = 4.87678337097168\n",
      "epoch no.0 train no.801  loss = 5.813031196594238\n",
      "epoch no.0 train no.811  loss = 3.8096671104431152\n",
      "epoch no.0 train no.821  loss = 4.10585355758667\n",
      "epoch no.0 train no.831  loss = 2.966853618621826\n",
      "epoch no.0 train no.841  loss = 2.7229740619659424\n",
      "epoch no.0 train no.851  loss = 6.888176918029785\n",
      "epoch no.0 train no.861  loss = 5.358353614807129\n",
      "epoch no.0 train no.871  loss = 3.9723362922668457\n",
      "epoch no.0 train no.881  loss = 5.590279579162598\n",
      "epoch no.0 train no.891  loss = 4.796350002288818\n",
      "epoch no.0 train no.901  loss = 5.256099224090576\n",
      "epoch no.0 train no.911  loss = 5.227389812469482\n",
      "epoch no.0 train no.921  loss = 4.103480339050293\n",
      "epoch no.0 train no.931  loss = 2.710275888442993\n",
      "epoch no.0 train no.941  loss = 3.918062925338745\n",
      "epoch no.0 train no.951  loss = 5.480327606201172\n",
      "epoch no.0 train no.961  loss = 5.013179302215576\n",
      "epoch no.0 train no.971  loss = 4.505916118621826\n",
      "epoch no.0 train no.981  loss = 4.004889011383057\n",
      "epoch no.0 train no.991  loss = 5.4713616371154785\n",
      "epoch no.0 train no.1001  loss = 4.830897808074951\n",
      "epoch no.1 train no.1  loss = 4.448986053466797\n",
      "epoch no.1 train no.11  loss = 4.112234115600586\n",
      "epoch no.1 train no.21  loss = 3.7543673515319824\n",
      "epoch no.1 train no.31  loss = 3.1407713890075684\n",
      "epoch no.1 train no.41  loss = 4.095305442810059\n",
      "epoch no.1 train no.51  loss = 3.616110324859619\n",
      "epoch no.1 train no.61  loss = 4.268499374389648\n",
      "epoch no.1 train no.71  loss = 3.9336798191070557\n",
      "epoch no.1 train no.81  loss = 3.2385385036468506\n",
      "epoch no.1 train no.91  loss = 4.284397602081299\n",
      "epoch no.1 train no.101  loss = 3.710463762283325\n",
      "epoch no.1 train no.111  loss = 4.139413833618164\n",
      "epoch no.1 train no.121  loss = 3.601731538772583\n",
      "epoch no.1 train no.131  loss = 3.9480948448181152\n",
      "epoch no.1 train no.141  loss = 3.5752851963043213\n",
      "epoch no.1 train no.151  loss = 3.754310369491577\n",
      "epoch no.1 train no.161  loss = 4.15135383605957\n",
      "epoch no.1 train no.171  loss = 2.996147394180298\n",
      "epoch no.1 train no.181  loss = 3.5435643196105957\n",
      "epoch no.1 train no.191  loss = 3.192303419113159\n",
      "epoch no.1 train no.201  loss = 2.5496504306793213\n",
      "epoch no.1 train no.211  loss = 3.6943719387054443\n",
      "epoch no.1 train no.221  loss = 3.3996987342834473\n",
      "epoch no.1 train no.231  loss = 3.7017221450805664\n",
      "epoch no.1 train no.241  loss = 2.3327314853668213\n",
      "epoch no.1 train no.251  loss = 3.629225015640259\n",
      "epoch no.1 train no.261  loss = 3.4230263233184814\n",
      "epoch no.1 train no.271  loss = 3.355849027633667\n",
      "epoch no.1 train no.281  loss = 3.2412281036376953\n",
      "epoch no.1 train no.291  loss = 6.243813514709473\n",
      "epoch no.1 train no.301  loss = 4.0893235206604\n",
      "epoch no.1 train no.311  loss = 4.690454006195068\n",
      "epoch no.1 train no.321  loss = 2.4487075805664062\n",
      "epoch no.1 train no.331  loss = 2.6631038188934326\n",
      "epoch no.1 train no.341  loss = 4.616916656494141\n",
      "epoch no.1 train no.351  loss = 2.4751925468444824\n",
      "epoch no.1 train no.361  loss = 3.8231990337371826\n",
      "epoch no.1 train no.371  loss = 5.192106246948242\n",
      "epoch no.1 train no.381  loss = 2.648226022720337\n",
      "epoch no.1 train no.391  loss = 5.173538684844971\n",
      "epoch no.1 train no.401  loss = 4.681480884552002\n",
      "epoch no.1 train no.411  loss = 3.6068131923675537\n",
      "epoch no.1 train no.421  loss = 3.2538890838623047\n",
      "epoch no.1 train no.431  loss = 3.8139941692352295\n",
      "epoch no.1 train no.441  loss = 2.295523166656494\n",
      "epoch no.1 train no.451  loss = 3.0960426330566406\n",
      "epoch no.1 train no.461  loss = 3.948221206665039\n",
      "epoch no.1 train no.471  loss = 5.082206726074219\n",
      "epoch no.1 train no.481  loss = 3.3686017990112305\n",
      "epoch no.1 train no.491  loss = 5.223773956298828\n",
      "epoch no.1 train no.501  loss = 2.9717648029327393\n",
      "epoch no.1 train no.511  loss = 4.123692035675049\n",
      "epoch no.1 train no.521  loss = 4.919440269470215\n",
      "epoch no.1 train no.531  loss = 3.2247745990753174\n",
      "epoch no.1 train no.541  loss = 3.3941287994384766\n",
      "epoch no.1 train no.551  loss = 3.820354461669922\n",
      "epoch no.1 train no.561  loss = 3.3919947147369385\n",
      "epoch no.1 train no.571  loss = 4.073877811431885\n",
      "epoch no.1 train no.581  loss = 3.090883255004883\n",
      "epoch no.1 train no.591  loss = 4.225403785705566\n",
      "epoch no.1 train no.601  loss = 3.024005651473999\n",
      "epoch no.1 train no.611  loss = 4.4904561042785645\n",
      "epoch no.1 train no.621  loss = 2.5378472805023193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 train no.631  loss = 3.3132355213165283\n",
      "epoch no.1 train no.641  loss = 2.6580915451049805\n",
      "epoch no.1 train no.651  loss = 4.2739410400390625\n",
      "epoch no.1 train no.661  loss = 4.001410007476807\n",
      "epoch no.1 train no.671  loss = 4.013136863708496\n",
      "epoch no.1 train no.681  loss = 3.9678115844726562\n",
      "epoch no.1 train no.691  loss = 3.7463414669036865\n",
      "epoch no.1 train no.701  loss = 4.110046863555908\n",
      "epoch no.1 train no.711  loss = 2.8329477310180664\n",
      "epoch no.1 train no.721  loss = 3.436633586883545\n",
      "epoch no.1 train no.731  loss = 3.5188050270080566\n",
      "epoch no.1 train no.741  loss = 4.182027339935303\n",
      "epoch no.1 train no.751  loss = 3.8337669372558594\n",
      "epoch no.1 train no.761  loss = 4.339745998382568\n",
      "epoch no.1 train no.771  loss = 4.331291198730469\n",
      "epoch no.1 train no.781  loss = 3.0040550231933594\n",
      "epoch no.1 train no.791  loss = 3.496892213821411\n",
      "epoch no.1 train no.801  loss = 2.747851610183716\n",
      "epoch no.1 train no.811  loss = 2.708343029022217\n",
      "epoch no.1 train no.821  loss = 4.809868812561035\n",
      "epoch no.1 train no.831  loss = 2.408064365386963\n",
      "epoch no.1 train no.841  loss = 3.4384684562683105\n",
      "epoch no.1 train no.851  loss = 2.549380302429199\n",
      "epoch no.1 train no.861  loss = 2.654839277267456\n",
      "epoch no.1 train no.871  loss = 5.386695861816406\n",
      "epoch no.1 train no.881  loss = 4.4580769538879395\n",
      "epoch no.1 train no.891  loss = 3.993591070175171\n",
      "epoch no.1 train no.901  loss = 3.223083972930908\n",
      "epoch no.1 train no.911  loss = 4.440931797027588\n",
      "epoch no.1 train no.921  loss = 2.879666566848755\n",
      "epoch no.1 train no.931  loss = 5.488667964935303\n",
      "epoch no.1 train no.941  loss = 3.622321367263794\n",
      "epoch no.1 train no.951  loss = 4.08336067199707\n",
      "epoch no.1 train no.961  loss = 3.481834650039673\n",
      "epoch no.1 train no.971  loss = 4.459710597991943\n",
      "epoch no.1 train no.981  loss = 3.062391757965088\n",
      "epoch no.1 train no.991  loss = 3.6122350692749023\n",
      "epoch no.1 train no.1001  loss = 4.86870813369751\n",
      "epoch no.2 train no.1  loss = 2.7546560764312744\n",
      "epoch no.2 train no.11  loss = 2.0611584186553955\n",
      "epoch no.2 train no.21  loss = 2.0341155529022217\n",
      "epoch no.2 train no.31  loss = 2.556033134460449\n",
      "epoch no.2 train no.41  loss = 2.1426212787628174\n",
      "epoch no.2 train no.51  loss = 3.494112014770508\n",
      "epoch no.2 train no.61  loss = 4.513263702392578\n",
      "epoch no.2 train no.71  loss = 3.025885820388794\n",
      "epoch no.2 train no.81  loss = 2.3433258533477783\n",
      "epoch no.2 train no.91  loss = 2.9064595699310303\n",
      "epoch no.2 train no.101  loss = 2.1653928756713867\n",
      "epoch no.2 train no.111  loss = 2.970318555831909\n",
      "epoch no.2 train no.121  loss = 2.445223808288574\n",
      "epoch no.2 train no.131  loss = 3.9995615482330322\n",
      "epoch no.2 train no.141  loss = 2.5883424282073975\n",
      "epoch no.2 train no.151  loss = 3.3852179050445557\n",
      "epoch no.2 train no.161  loss = 2.8735742568969727\n",
      "epoch no.2 train no.171  loss = 1.595404863357544\n",
      "epoch no.2 train no.181  loss = 2.6604349613189697\n",
      "epoch no.2 train no.191  loss = 2.283540725708008\n",
      "epoch no.2 train no.201  loss = 3.2451157569885254\n",
      "epoch no.2 train no.211  loss = 4.677326202392578\n",
      "epoch no.2 train no.221  loss = 2.465878963470459\n",
      "epoch no.2 train no.231  loss = 1.896990180015564\n",
      "epoch no.2 train no.241  loss = 3.520634174346924\n",
      "epoch no.2 train no.251  loss = 2.9577956199645996\n",
      "epoch no.2 train no.261  loss = 2.9108314514160156\n",
      "epoch no.2 train no.271  loss = 3.767812967300415\n",
      "epoch no.2 train no.281  loss = 3.099900245666504\n",
      "epoch no.2 train no.291  loss = 2.381063222885132\n",
      "epoch no.2 train no.301  loss = 4.085465908050537\n",
      "epoch no.2 train no.311  loss = 2.7748310565948486\n",
      "epoch no.2 train no.321  loss = 2.9122767448425293\n",
      "epoch no.2 train no.331  loss = 2.9871299266815186\n",
      "epoch no.2 train no.341  loss = 2.407789468765259\n",
      "epoch no.2 train no.351  loss = 2.8321239948272705\n",
      "epoch no.2 train no.361  loss = 3.360809803009033\n",
      "epoch no.2 train no.371  loss = 3.787734270095825\n",
      "epoch no.2 train no.381  loss = 2.0597071647644043\n",
      "epoch no.2 train no.391  loss = 2.417013168334961\n",
      "epoch no.2 train no.401  loss = 2.5922164916992188\n",
      "epoch no.2 train no.411  loss = 2.7899482250213623\n",
      "epoch no.2 train no.421  loss = 2.031355381011963\n",
      "epoch no.2 train no.431  loss = 3.774646520614624\n",
      "epoch no.2 train no.441  loss = 2.8446407318115234\n",
      "epoch no.2 train no.451  loss = 1.808359146118164\n",
      "epoch no.2 train no.461  loss = 2.98466420173645\n",
      "epoch no.2 train no.471  loss = 4.993638515472412\n",
      "epoch no.2 train no.481  loss = 4.055857181549072\n",
      "epoch no.2 train no.491  loss = 3.4875142574310303\n",
      "epoch no.2 train no.501  loss = 3.290956974029541\n",
      "epoch no.2 train no.511  loss = 2.6476526260375977\n",
      "epoch no.2 train no.521  loss = 3.3575432300567627\n",
      "epoch no.2 train no.531  loss = 3.7569503784179688\n",
      "epoch no.2 train no.541  loss = 2.39753794670105\n",
      "epoch no.2 train no.551  loss = 3.1389195919036865\n",
      "epoch no.2 train no.561  loss = 2.7798006534576416\n",
      "epoch no.2 train no.571  loss = 3.87982177734375\n",
      "epoch no.2 train no.581  loss = 3.1706924438476562\n",
      "epoch no.2 train no.591  loss = 2.115464448928833\n",
      "epoch no.2 train no.601  loss = 2.427262783050537\n",
      "epoch no.2 train no.611  loss = 2.4235756397247314\n",
      "epoch no.2 train no.621  loss = 3.7131741046905518\n",
      "epoch no.2 train no.631  loss = 2.554095506668091\n",
      "epoch no.2 train no.641  loss = 2.576913595199585\n",
      "epoch no.2 train no.651  loss = 2.9888079166412354\n",
      "epoch no.2 train no.661  loss = 2.361525774002075\n",
      "epoch no.2 train no.671  loss = 3.530336380004883\n",
      "epoch no.2 train no.681  loss = 4.240228652954102\n",
      "epoch no.2 train no.691  loss = 1.91848623752594\n",
      "epoch no.2 train no.701  loss = 2.8545758724212646\n",
      "epoch no.2 train no.711  loss = 2.25980544090271\n",
      "epoch no.2 train no.721  loss = 1.9874964952468872\n",
      "epoch no.2 train no.731  loss = 2.4110820293426514\n",
      "epoch no.2 train no.741  loss = 2.450728178024292\n",
      "epoch no.2 train no.751  loss = 2.461717128753662\n",
      "epoch no.2 train no.761  loss = 2.2294251918792725\n",
      "epoch no.2 train no.771  loss = 2.34144926071167\n",
      "epoch no.2 train no.781  loss = 2.4921653270721436\n",
      "epoch no.2 train no.791  loss = 2.7036101818084717\n",
      "epoch no.2 train no.801  loss = 2.6166694164276123\n",
      "epoch no.2 train no.811  loss = 2.600862979888916\n",
      "epoch no.2 train no.821  loss = 2.0731968879699707\n",
      "epoch no.2 train no.831  loss = 1.974390983581543\n",
      "epoch no.2 train no.841  loss = 3.632417917251587\n",
      "epoch no.2 train no.851  loss = 4.827181339263916\n",
      "epoch no.2 train no.861  loss = 2.5056021213531494\n",
      "epoch no.2 train no.871  loss = 2.942837953567505\n",
      "epoch no.2 train no.881  loss = 2.4557483196258545\n",
      "epoch no.2 train no.891  loss = 2.881540298461914\n",
      "epoch no.2 train no.901  loss = 3.985992908477783\n",
      "epoch no.2 train no.911  loss = 3.345872640609741\n",
      "epoch no.2 train no.921  loss = 2.464905261993408\n",
      "epoch no.2 train no.931  loss = 1.6580630540847778\n",
      "epoch no.2 train no.941  loss = 3.5357415676116943\n",
      "epoch no.2 train no.951  loss = 2.3298745155334473\n",
      "epoch no.2 train no.961  loss = 3.1911656856536865\n",
      "epoch no.2 train no.971  loss = 3.6907529830932617\n",
      "epoch no.2 train no.981  loss = 1.8776473999023438\n",
      "epoch no.2 train no.991  loss = 2.7068989276885986\n",
      "epoch no.2 train no.1001  loss = 2.9616177082061768\n",
      "epoch no.3 train no.1  loss = 1.527640700340271\n",
      "epoch no.3 train no.11  loss = 1.2989445924758911\n",
      "epoch no.3 train no.21  loss = 2.1575722694396973\n",
      "epoch no.3 train no.31  loss = 2.2024707794189453\n",
      "epoch no.3 train no.41  loss = 1.50417160987854\n",
      "epoch no.3 train no.51  loss = 1.2771601676940918\n",
      "epoch no.3 train no.61  loss = 1.743850827217102\n",
      "epoch no.3 train no.71  loss = 1.650484561920166\n",
      "epoch no.3 train no.81  loss = 1.644736409187317\n",
      "epoch no.3 train no.91  loss = 1.6728535890579224\n",
      "epoch no.3 train no.101  loss = 1.4713655710220337\n",
      "epoch no.3 train no.111  loss = 1.8880853652954102\n",
      "epoch no.3 train no.121  loss = 3.0076513290405273\n",
      "epoch no.3 train no.131  loss = 1.9784821271896362\n",
      "epoch no.3 train no.141  loss = 2.149087905883789\n",
      "epoch no.3 train no.151  loss = 1.7864303588867188\n",
      "epoch no.3 train no.161  loss = 2.664979934692383\n",
      "epoch no.3 train no.171  loss = 2.479774236679077\n",
      "epoch no.3 train no.181  loss = 2.175008535385132\n",
      "epoch no.3 train no.191  loss = 1.5265552997589111\n",
      "epoch no.3 train no.201  loss = 2.953805446624756\n",
      "epoch no.3 train no.211  loss = 4.085303783416748\n",
      "epoch no.3 train no.221  loss = 1.2640831470489502\n",
      "epoch no.3 train no.231  loss = 1.2106592655181885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.3 train no.241  loss = 2.503761053085327\n",
      "epoch no.3 train no.251  loss = 1.9396915435791016\n",
      "epoch no.3 train no.261  loss = 3.450258255004883\n",
      "epoch no.3 train no.271  loss = 2.491464376449585\n",
      "epoch no.3 train no.281  loss = 2.050429105758667\n",
      "epoch no.3 train no.291  loss = 1.6706172227859497\n",
      "epoch no.3 train no.301  loss = 1.8905718326568604\n",
      "epoch no.3 train no.311  loss = 2.6311194896698\n",
      "epoch no.3 train no.321  loss = 2.465738296508789\n",
      "epoch no.3 train no.331  loss = 2.7076900005340576\n",
      "epoch no.3 train no.341  loss = 1.334758996963501\n",
      "epoch no.3 train no.351  loss = 2.721086025238037\n",
      "epoch no.3 train no.361  loss = 2.530245065689087\n",
      "epoch no.3 train no.371  loss = 2.2533369064331055\n",
      "epoch no.3 train no.381  loss = 2.424034595489502\n",
      "epoch no.3 train no.391  loss = 1.7883328199386597\n",
      "epoch no.3 train no.401  loss = 2.324291229248047\n",
      "epoch no.3 train no.411  loss = 1.9557912349700928\n",
      "epoch no.3 train no.421  loss = 3.0204977989196777\n",
      "epoch no.3 train no.431  loss = 2.6261096000671387\n",
      "epoch no.3 train no.441  loss = 1.1717064380645752\n",
      "epoch no.3 train no.451  loss = 2.904703140258789\n",
      "epoch no.3 train no.461  loss = 1.6143239736557007\n",
      "epoch no.3 train no.471  loss = 1.7962440252304077\n",
      "epoch no.3 train no.481  loss = 2.3970389366149902\n",
      "epoch no.3 train no.491  loss = 1.5515406131744385\n",
      "epoch no.3 train no.501  loss = 1.9004225730895996\n",
      "epoch no.3 train no.511  loss = 1.7250365018844604\n",
      "epoch no.3 train no.521  loss = 2.389434576034546\n",
      "epoch no.3 train no.531  loss = 2.2638113498687744\n",
      "epoch no.3 train no.541  loss = 1.9689277410507202\n",
      "epoch no.3 train no.551  loss = 2.7961511611938477\n",
      "epoch no.3 train no.561  loss = 3.0306808948516846\n",
      "epoch no.3 train no.571  loss = 2.348917007446289\n",
      "epoch no.3 train no.581  loss = 1.5145231485366821\n",
      "epoch no.3 train no.591  loss = 2.0282602310180664\n",
      "epoch no.3 train no.601  loss = 1.5933289527893066\n",
      "epoch no.3 train no.611  loss = 1.8512104749679565\n",
      "epoch no.3 train no.621  loss = 3.2365946769714355\n",
      "epoch no.3 train no.631  loss = 3.953277826309204\n",
      "epoch no.3 train no.641  loss = 2.14439058303833\n",
      "epoch no.3 train no.651  loss = 1.3441694974899292\n",
      "epoch no.3 train no.661  loss = 2.025803327560425\n",
      "epoch no.3 train no.671  loss = 2.9391226768493652\n",
      "epoch no.3 train no.681  loss = 3.4053561687469482\n",
      "epoch no.3 train no.691  loss = 3.484772205352783\n",
      "epoch no.3 train no.701  loss = 2.7117435932159424\n",
      "epoch no.3 train no.711  loss = 2.7518982887268066\n",
      "epoch no.3 train no.721  loss = 1.8683525323867798\n",
      "epoch no.3 train no.731  loss = 1.470724105834961\n",
      "epoch no.3 train no.741  loss = 2.5829944610595703\n",
      "epoch no.3 train no.751  loss = 3.0461580753326416\n",
      "epoch no.3 train no.761  loss = 1.871935486793518\n",
      "epoch no.3 train no.771  loss = 2.764876127243042\n",
      "epoch no.3 train no.781  loss = 2.33823299407959\n",
      "epoch no.3 train no.791  loss = 1.9849755764007568\n",
      "epoch no.3 train no.801  loss = 2.7479164600372314\n",
      "epoch no.3 train no.811  loss = 2.3485536575317383\n",
      "epoch no.3 train no.821  loss = 1.862657070159912\n",
      "epoch no.3 train no.831  loss = 2.496725559234619\n",
      "epoch no.3 train no.841  loss = 3.760718584060669\n",
      "epoch no.3 train no.851  loss = 1.9091992378234863\n",
      "epoch no.3 train no.861  loss = 1.5889700651168823\n",
      "epoch no.3 train no.871  loss = 2.8841443061828613\n",
      "epoch no.3 train no.881  loss = 1.7811847925186157\n",
      "epoch no.3 train no.891  loss = 2.674466371536255\n",
      "epoch no.3 train no.901  loss = 2.467790365219116\n",
      "epoch no.3 train no.911  loss = 3.021486520767212\n",
      "epoch no.3 train no.921  loss = 1.4509180784225464\n",
      "epoch no.3 train no.931  loss = 1.6621812582015991\n",
      "epoch no.3 train no.941  loss = 1.8436856269836426\n",
      "epoch no.3 train no.951  loss = 1.3213601112365723\n",
      "epoch no.3 train no.961  loss = 2.5292186737060547\n",
      "epoch no.3 train no.971  loss = 3.1212995052337646\n",
      "epoch no.3 train no.981  loss = 1.1774426698684692\n",
      "epoch no.3 train no.991  loss = 1.4406651258468628\n",
      "epoch no.3 train no.1001  loss = 3.382763624191284\n",
      "epoch no.4 train no.1  loss = 2.148463010787964\n",
      "epoch no.4 train no.11  loss = 1.0157337188720703\n",
      "epoch no.4 train no.21  loss = 1.6419835090637207\n",
      "epoch no.4 train no.31  loss = 0.8871626257896423\n",
      "epoch no.4 train no.41  loss = 1.3077021837234497\n",
      "epoch no.4 train no.51  loss = 0.8422524929046631\n",
      "epoch no.4 train no.61  loss = 1.8068273067474365\n",
      "epoch no.4 train no.71  loss = 2.187044382095337\n",
      "epoch no.4 train no.81  loss = 1.2795078754425049\n",
      "epoch no.4 train no.91  loss = 1.64129638671875\n",
      "epoch no.4 train no.101  loss = 1.4432765245437622\n",
      "epoch no.4 train no.111  loss = 1.0842238664627075\n",
      "epoch no.4 train no.121  loss = 1.4814841747283936\n",
      "epoch no.4 train no.131  loss = 1.8393902778625488\n",
      "epoch no.4 train no.141  loss = 3.0031545162200928\n",
      "epoch no.4 train no.151  loss = 1.5868138074874878\n",
      "epoch no.4 train no.161  loss = 1.6292873620986938\n",
      "epoch no.4 train no.171  loss = 1.5239341259002686\n",
      "epoch no.4 train no.181  loss = 1.2650176286697388\n",
      "epoch no.4 train no.191  loss = 2.040647506713867\n",
      "epoch no.4 train no.201  loss = 1.6634981632232666\n",
      "epoch no.4 train no.211  loss = 1.14826500415802\n",
      "epoch no.4 train no.221  loss = 2.034524440765381\n",
      "epoch no.4 train no.231  loss = 1.1882836818695068\n",
      "epoch no.4 train no.241  loss = 2.496511697769165\n",
      "epoch no.4 train no.251  loss = 1.6918418407440186\n",
      "epoch no.4 train no.261  loss = 1.3706858158111572\n",
      "epoch no.4 train no.271  loss = 2.6154086589813232\n",
      "epoch no.4 train no.281  loss = 1.0489319562911987\n",
      "epoch no.4 train no.291  loss = 1.8873376846313477\n",
      "epoch no.4 train no.301  loss = 1.8232436180114746\n",
      "epoch no.4 train no.311  loss = 1.2723103761672974\n",
      "epoch no.4 train no.321  loss = 1.2974563837051392\n",
      "epoch no.4 train no.331  loss = 1.4288164377212524\n",
      "epoch no.4 train no.341  loss = 1.7631518840789795\n",
      "epoch no.4 train no.351  loss = 1.7783042192459106\n",
      "epoch no.4 train no.361  loss = 1.5465270280838013\n",
      "epoch no.4 train no.371  loss = 1.190254807472229\n",
      "epoch no.4 train no.381  loss = 1.8199106454849243\n",
      "epoch no.4 train no.391  loss = 1.419897437095642\n",
      "epoch no.4 train no.401  loss = 1.3411673307418823\n",
      "epoch no.4 train no.411  loss = 1.7168147563934326\n",
      "epoch no.4 train no.421  loss = 1.69856595993042\n",
      "epoch no.4 train no.431  loss = 1.190455675125122\n",
      "epoch no.4 train no.441  loss = 1.3824108839035034\n",
      "epoch no.4 train no.451  loss = 1.328989028930664\n",
      "epoch no.4 train no.461  loss = 1.9373018741607666\n",
      "epoch no.4 train no.471  loss = 1.319357991218567\n",
      "epoch no.4 train no.481  loss = 1.096465826034546\n",
      "epoch no.4 train no.491  loss = 1.6680060625076294\n",
      "epoch no.4 train no.501  loss = 3.138563632965088\n",
      "epoch no.4 train no.511  loss = 1.5440269708633423\n",
      "epoch no.4 train no.521  loss = 2.0889089107513428\n",
      "epoch no.4 train no.531  loss = 1.2166259288787842\n",
      "epoch no.4 train no.541  loss = 1.8730138540267944\n",
      "epoch no.4 train no.551  loss = 2.6667284965515137\n",
      "epoch no.4 train no.561  loss = 1.3859663009643555\n",
      "epoch no.4 train no.571  loss = 1.6767734289169312\n",
      "epoch no.4 train no.581  loss = 1.2063544988632202\n",
      "epoch no.4 train no.591  loss = 1.423943042755127\n",
      "epoch no.4 train no.601  loss = 1.069813847541809\n",
      "epoch no.4 train no.611  loss = 1.7464451789855957\n",
      "epoch no.4 train no.621  loss = 1.306357979774475\n",
      "epoch no.4 train no.631  loss = 1.1984647512435913\n",
      "epoch no.4 train no.641  loss = 2.588412284851074\n",
      "epoch no.4 train no.651  loss = 1.8996280431747437\n",
      "epoch no.4 train no.661  loss = 1.0006250143051147\n",
      "epoch no.4 train no.671  loss = 3.2575550079345703\n",
      "epoch no.4 train no.681  loss = 1.9027773141860962\n",
      "epoch no.4 train no.691  loss = 2.252129077911377\n",
      "epoch no.4 train no.701  loss = 2.20632004737854\n",
      "epoch no.4 train no.711  loss = 1.1671297550201416\n",
      "epoch no.4 train no.721  loss = 2.6694695949554443\n",
      "epoch no.4 train no.731  loss = 0.8663699626922607\n",
      "epoch no.4 train no.741  loss = 2.4857020378112793\n",
      "epoch no.4 train no.751  loss = 2.2545180320739746\n",
      "epoch no.4 train no.761  loss = 1.259651780128479\n",
      "epoch no.4 train no.771  loss = 1.233311414718628\n",
      "epoch no.4 train no.781  loss = 1.435813069343567\n",
      "epoch no.4 train no.791  loss = 1.89705491065979\n",
      "epoch no.4 train no.801  loss = 1.579540491104126\n",
      "epoch no.4 train no.811  loss = 1.692549467086792\n",
      "epoch no.4 train no.821  loss = 3.3413808345794678\n",
      "epoch no.4 train no.831  loss = 1.9239686727523804\n",
      "epoch no.4 train no.841  loss = 1.4760061502456665\n",
      "epoch no.4 train no.851  loss = 2.643864154815674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.4 train no.861  loss = 1.3994948863983154\n",
      "epoch no.4 train no.871  loss = 1.7780930995941162\n",
      "epoch no.4 train no.881  loss = 2.1215174198150635\n",
      "epoch no.4 train no.891  loss = 1.953765869140625\n",
      "epoch no.4 train no.901  loss = 1.9037717580795288\n",
      "epoch no.4 train no.911  loss = 2.0179367065429688\n",
      "epoch no.4 train no.921  loss = 1.5265127420425415\n",
      "epoch no.4 train no.931  loss = 1.5942691564559937\n",
      "epoch no.4 train no.941  loss = 1.2496191263198853\n",
      "epoch no.4 train no.951  loss = 1.5432502031326294\n",
      "epoch no.4 train no.961  loss = 1.8202329874038696\n",
      "epoch no.4 train no.971  loss = 1.2416030168533325\n",
      "epoch no.4 train no.981  loss = 1.5664693117141724\n",
      "epoch no.4 train no.991  loss = 1.917365312576294\n",
      "epoch no.4 train no.1001  loss = 2.254119634628296\n",
      "epoch no.5 train no.1  loss = 1.3908140659332275\n",
      "epoch no.5 train no.11  loss = 1.6050950288772583\n",
      "epoch no.5 train no.21  loss = 1.163466215133667\n",
      "epoch no.5 train no.31  loss = 0.7416223883628845\n",
      "epoch no.5 train no.41  loss = 1.037582278251648\n",
      "epoch no.5 train no.51  loss = 0.9585870504379272\n",
      "epoch no.5 train no.61  loss = 1.6899114847183228\n",
      "epoch no.5 train no.71  loss = 2.8484439849853516\n",
      "epoch no.5 train no.81  loss = 2.1657607555389404\n",
      "epoch no.5 train no.91  loss = 1.039038062095642\n",
      "epoch no.5 train no.101  loss = 0.9108589291572571\n",
      "epoch no.5 train no.111  loss = 1.0085420608520508\n",
      "epoch no.5 train no.121  loss = 1.6263173818588257\n",
      "epoch no.5 train no.131  loss = 2.02341890335083\n",
      "epoch no.5 train no.141  loss = 2.2836267948150635\n",
      "epoch no.5 train no.151  loss = 1.2430232763290405\n",
      "epoch no.5 train no.161  loss = 1.436275601387024\n",
      "epoch no.5 train no.171  loss = 1.2999508380889893\n",
      "epoch no.5 train no.181  loss = 1.8482475280761719\n",
      "epoch no.5 train no.191  loss = 0.7943652272224426\n",
      "epoch no.5 train no.201  loss = 1.1805263757705688\n",
      "epoch no.5 train no.211  loss = 1.036484956741333\n",
      "epoch no.5 train no.221  loss = 1.1359272003173828\n",
      "epoch no.5 train no.231  loss = 2.105761766433716\n",
      "epoch no.5 train no.241  loss = 1.9293185472488403\n",
      "epoch no.5 train no.251  loss = 1.2149758338928223\n",
      "epoch no.5 train no.261  loss = 1.361538290977478\n",
      "epoch no.5 train no.271  loss = 1.6673357486724854\n",
      "epoch no.5 train no.281  loss = 0.9442141652107239\n",
      "epoch no.5 train no.291  loss = 0.9907068610191345\n",
      "epoch no.5 train no.301  loss = 1.365714192390442\n",
      "epoch no.5 train no.311  loss = 1.7160365581512451\n",
      "epoch no.5 train no.321  loss = 0.7746017575263977\n",
      "epoch no.5 train no.331  loss = 1.0385984182357788\n",
      "epoch no.5 train no.341  loss = 0.9251456260681152\n",
      "epoch no.5 train no.351  loss = 2.7941575050354004\n",
      "epoch no.5 train no.361  loss = 1.6974860429763794\n",
      "epoch no.5 train no.371  loss = 1.1883835792541504\n",
      "epoch no.5 train no.381  loss = 2.2286148071289062\n",
      "epoch no.5 train no.391  loss = 2.0372707843780518\n",
      "epoch no.5 train no.401  loss = 1.3484017848968506\n",
      "epoch no.5 train no.411  loss = 1.3494417667388916\n",
      "epoch no.5 train no.421  loss = 1.2603048086166382\n",
      "epoch no.5 train no.431  loss = 1.768474817276001\n",
      "epoch no.5 train no.441  loss = 1.6076868772506714\n",
      "epoch no.5 train no.451  loss = 1.2251533269882202\n",
      "epoch no.5 train no.461  loss = 1.6015905141830444\n",
      "epoch no.5 train no.471  loss = 1.8921719789505005\n",
      "epoch no.5 train no.481  loss = 1.7864799499511719\n",
      "epoch no.5 train no.491  loss = 2.5219616889953613\n",
      "epoch no.5 train no.501  loss = 1.2613269090652466\n",
      "epoch no.5 train no.511  loss = 1.121961236000061\n",
      "epoch no.5 train no.521  loss = 2.347383975982666\n",
      "epoch no.5 train no.531  loss = 0.998565137386322\n",
      "epoch no.5 train no.541  loss = 1.0408672094345093\n",
      "epoch no.5 train no.551  loss = 2.045368194580078\n",
      "epoch no.5 train no.561  loss = 1.0878403186798096\n",
      "epoch no.5 train no.571  loss = 1.0375676155090332\n",
      "epoch no.5 train no.581  loss = 1.6566883325576782\n",
      "epoch no.5 train no.591  loss = 1.6052742004394531\n",
      "epoch no.5 train no.601  loss = 0.9089970588684082\n",
      "epoch no.5 train no.611  loss = 1.442575216293335\n",
      "epoch no.5 train no.621  loss = 1.5586897134780884\n",
      "epoch no.5 train no.631  loss = 2.4969444274902344\n",
      "epoch no.5 train no.641  loss = 1.0737005472183228\n",
      "epoch no.5 train no.651  loss = 2.4476821422576904\n",
      "epoch no.5 train no.661  loss = 1.8866281509399414\n",
      "epoch no.5 train no.671  loss = 1.9593873023986816\n",
      "epoch no.5 train no.681  loss = 1.3173872232437134\n",
      "epoch no.5 train no.691  loss = 1.024764895439148\n",
      "epoch no.5 train no.701  loss = 0.9400078058242798\n",
      "epoch no.5 train no.711  loss = 1.2782647609710693\n",
      "epoch no.5 train no.721  loss = 1.6958173513412476\n",
      "epoch no.5 train no.731  loss = 1.5010573863983154\n",
      "epoch no.5 train no.741  loss = 1.5157008171081543\n",
      "epoch no.5 train no.751  loss = 1.2643530368804932\n",
      "epoch no.5 train no.761  loss = 1.3545514345169067\n",
      "epoch no.5 train no.771  loss = 2.236848831176758\n",
      "epoch no.5 train no.781  loss = 0.9956017136573792\n",
      "epoch no.5 train no.791  loss = 1.5301107168197632\n",
      "epoch no.5 train no.801  loss = 1.2213383913040161\n",
      "epoch no.5 train no.811  loss = 2.2079737186431885\n",
      "epoch no.5 train no.821  loss = 1.1774989366531372\n",
      "epoch no.5 train no.831  loss = 1.4720145463943481\n",
      "epoch no.5 train no.841  loss = 2.871987819671631\n",
      "epoch no.5 train no.851  loss = 2.684901475906372\n",
      "epoch no.5 train no.861  loss = 1.090981125831604\n",
      "epoch no.5 train no.871  loss = 2.1019325256347656\n",
      "epoch no.5 train no.881  loss = 1.7752811908721924\n",
      "epoch no.5 train no.891  loss = 1.213573932647705\n",
      "epoch no.5 train no.901  loss = 1.602187156677246\n",
      "epoch no.5 train no.911  loss = 1.88228178024292\n",
      "epoch no.5 train no.921  loss = 1.333486557006836\n",
      "epoch no.5 train no.931  loss = 1.653133511543274\n",
      "epoch no.5 train no.941  loss = 1.337404727935791\n",
      "epoch no.5 train no.951  loss = 1.4528005123138428\n",
      "epoch no.5 train no.961  loss = 1.645392894744873\n",
      "epoch no.5 train no.971  loss = 1.611363172531128\n",
      "epoch no.5 train no.981  loss = 0.8963567614555359\n",
      "epoch no.5 train no.991  loss = 1.4553563594818115\n",
      "epoch no.5 train no.1001  loss = 1.3738994598388672\n",
      "epoch no.6 train no.1  loss = 1.192407250404358\n",
      "epoch no.6 train no.11  loss = 0.7449286580085754\n",
      "epoch no.6 train no.21  loss = 0.9271566271781921\n",
      "epoch no.6 train no.31  loss = 0.8590437173843384\n",
      "epoch no.6 train no.41  loss = 0.826589822769165\n",
      "epoch no.6 train no.51  loss = 1.0466147661209106\n",
      "epoch no.6 train no.61  loss = 1.9853496551513672\n",
      "epoch no.6 train no.71  loss = 1.1543281078338623\n",
      "epoch no.6 train no.81  loss = 0.8457534909248352\n",
      "epoch no.6 train no.91  loss = 1.0877491235733032\n",
      "epoch no.6 train no.101  loss = 1.0410943031311035\n",
      "epoch no.6 train no.111  loss = 1.3402799367904663\n",
      "epoch no.6 train no.121  loss = 0.8890593647956848\n",
      "epoch no.6 train no.131  loss = 0.8613188862800598\n",
      "epoch no.6 train no.141  loss = 0.915915310382843\n",
      "epoch no.6 train no.151  loss = 0.9409540891647339\n",
      "epoch no.6 train no.161  loss = 1.0603424310684204\n",
      "epoch no.6 train no.171  loss = 2.0127158164978027\n",
      "epoch no.6 train no.181  loss = 1.2674689292907715\n",
      "epoch no.6 train no.191  loss = 0.8984246253967285\n",
      "epoch no.6 train no.201  loss = 0.7498969435691833\n",
      "epoch no.6 train no.211  loss = 1.407299280166626\n",
      "epoch no.6 train no.221  loss = 1.939097285270691\n",
      "epoch no.6 train no.231  loss = 1.136533498764038\n",
      "epoch no.6 train no.241  loss = 1.2102928161621094\n",
      "epoch no.6 train no.251  loss = 1.0994592905044556\n",
      "epoch no.6 train no.261  loss = 1.147457480430603\n",
      "epoch no.6 train no.271  loss = 1.0804030895233154\n",
      "epoch no.6 train no.281  loss = 1.1225974559783936\n",
      "epoch no.6 train no.291  loss = 1.2269275188446045\n",
      "epoch no.6 train no.301  loss = 0.9750431776046753\n",
      "epoch no.6 train no.311  loss = 1.1691062450408936\n",
      "epoch no.6 train no.321  loss = 1.348284363746643\n",
      "epoch no.6 train no.331  loss = 0.9104489088058472\n",
      "epoch no.6 train no.341  loss = 1.464182734489441\n",
      "epoch no.6 train no.351  loss = 1.0261168479919434\n",
      "epoch no.6 train no.361  loss = 1.775065302848816\n",
      "epoch no.6 train no.371  loss = 0.9051634073257446\n",
      "epoch no.6 train no.381  loss = 1.2938432693481445\n",
      "epoch no.6 train no.391  loss = 1.4994301795959473\n",
      "epoch no.6 train no.401  loss = 0.9947420954704285\n",
      "epoch no.6 train no.411  loss = 1.8901042938232422\n",
      "epoch no.6 train no.421  loss = 1.4133377075195312\n",
      "epoch no.6 train no.431  loss = 3.2101681232452393\n",
      "epoch no.6 train no.441  loss = 0.7914836406707764\n",
      "epoch no.6 train no.451  loss = 0.7930471301078796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.6 train no.461  loss = 0.9893984198570251\n",
      "epoch no.6 train no.471  loss = 1.320799469947815\n",
      "epoch no.6 train no.481  loss = 1.7558101415634155\n",
      "epoch no.6 train no.491  loss = 1.0248117446899414\n",
      "epoch no.6 train no.501  loss = 1.4232122898101807\n",
      "epoch no.6 train no.511  loss = 0.769058346748352\n",
      "epoch no.6 train no.521  loss = 1.6908988952636719\n",
      "epoch no.6 train no.531  loss = 1.8844220638275146\n",
      "epoch no.6 train no.541  loss = 1.1760354042053223\n",
      "epoch no.6 train no.551  loss = 2.1328938007354736\n",
      "epoch no.6 train no.561  loss = 2.4283065795898438\n",
      "epoch no.6 train no.571  loss = 1.673911213874817\n",
      "epoch no.6 train no.581  loss = 0.6497252583503723\n",
      "epoch no.6 train no.591  loss = 0.8961417078971863\n",
      "epoch no.6 train no.601  loss = 0.8632442355155945\n",
      "epoch no.6 train no.611  loss = 1.830298900604248\n",
      "epoch no.6 train no.621  loss = 2.5848562717437744\n",
      "epoch no.6 train no.631  loss = 1.4887547492980957\n",
      "epoch no.6 train no.641  loss = 1.0304055213928223\n",
      "epoch no.6 train no.651  loss = 0.9760885834693909\n",
      "epoch no.6 train no.661  loss = 1.14699387550354\n",
      "epoch no.6 train no.671  loss = 1.180026650428772\n",
      "epoch no.6 train no.681  loss = 0.9792462587356567\n",
      "epoch no.6 train no.691  loss = 1.2701587677001953\n",
      "epoch no.6 train no.701  loss = 0.913800835609436\n",
      "epoch no.6 train no.711  loss = 0.908614456653595\n",
      "epoch no.6 train no.721  loss = 1.6323491334915161\n",
      "epoch no.6 train no.731  loss = 1.5554956197738647\n",
      "epoch no.6 train no.741  loss = 1.115308403968811\n",
      "epoch no.6 train no.751  loss = 1.482134461402893\n",
      "epoch no.6 train no.761  loss = 2.188403367996216\n",
      "epoch no.6 train no.771  loss = 0.8899406790733337\n",
      "epoch no.6 train no.781  loss = 1.4100829362869263\n",
      "epoch no.6 train no.791  loss = 1.422214388847351\n",
      "epoch no.6 train no.801  loss = 0.9694408774375916\n",
      "epoch no.6 train no.811  loss = 1.1627166271209717\n",
      "epoch no.6 train no.821  loss = 1.0821139812469482\n",
      "epoch no.6 train no.831  loss = 0.7377305030822754\n",
      "epoch no.6 train no.841  loss = 1.334015130996704\n",
      "epoch no.6 train no.851  loss = 1.4960752725601196\n",
      "epoch no.6 train no.861  loss = 1.3175748586654663\n",
      "epoch no.6 train no.871  loss = 1.7308151721954346\n",
      "epoch no.6 train no.881  loss = 0.8469106554985046\n",
      "epoch no.6 train no.891  loss = 1.7274943590164185\n",
      "epoch no.6 train no.901  loss = 1.2040404081344604\n",
      "epoch no.6 train no.911  loss = 1.0374345779418945\n",
      "epoch no.6 train no.921  loss = 1.1272777318954468\n",
      "epoch no.6 train no.931  loss = 2.233938217163086\n",
      "epoch no.6 train no.941  loss = 1.041366457939148\n",
      "epoch no.6 train no.951  loss = 1.5913488864898682\n",
      "epoch no.6 train no.961  loss = 1.3106672763824463\n",
      "epoch no.6 train no.971  loss = 0.8347697854042053\n",
      "epoch no.6 train no.981  loss = 1.1031670570373535\n",
      "epoch no.6 train no.991  loss = 1.1129709482192993\n",
      "epoch no.6 train no.1001  loss = 1.1350213289260864\n",
      "epoch no.7 train no.1  loss = 1.3744877576828003\n",
      "epoch no.7 train no.11  loss = 0.9652718901634216\n",
      "epoch no.7 train no.21  loss = 1.2678780555725098\n",
      "epoch no.7 train no.31  loss = 0.8616967797279358\n",
      "epoch no.7 train no.41  loss = 1.1142292022705078\n",
      "epoch no.7 train no.51  loss = 1.087750792503357\n",
      "epoch no.7 train no.61  loss = 1.0766562223434448\n",
      "epoch no.7 train no.71  loss = 0.4267294406890869\n",
      "epoch no.7 train no.81  loss = 1.1175639629364014\n",
      "epoch no.7 train no.91  loss = 0.877765417098999\n",
      "epoch no.7 train no.101  loss = 1.2113564014434814\n",
      "epoch no.7 train no.111  loss = 0.8903310298919678\n",
      "epoch no.7 train no.121  loss = 0.7642194032669067\n",
      "epoch no.7 train no.131  loss = 1.6375534534454346\n",
      "epoch no.7 train no.141  loss = 1.1108378171920776\n",
      "epoch no.7 train no.151  loss = 1.151986002922058\n",
      "epoch no.7 train no.161  loss = 1.1264046430587769\n",
      "epoch no.7 train no.171  loss = 0.9960111975669861\n",
      "epoch no.7 train no.181  loss = 0.5208216905593872\n",
      "epoch no.7 train no.191  loss = 1.0862836837768555\n",
      "epoch no.7 train no.201  loss = 0.7529247403144836\n",
      "epoch no.7 train no.211  loss = 1.5919774770736694\n",
      "epoch no.7 train no.221  loss = 1.3597660064697266\n",
      "epoch no.7 train no.231  loss = 1.1279866695404053\n",
      "epoch no.7 train no.241  loss = 0.8424496054649353\n",
      "epoch no.7 train no.251  loss = 1.557938575744629\n",
      "epoch no.7 train no.261  loss = 0.9391947984695435\n",
      "epoch no.7 train no.271  loss = 1.367118239402771\n",
      "epoch no.7 train no.281  loss = 1.055438756942749\n",
      "epoch no.7 train no.291  loss = 1.0344808101654053\n",
      "epoch no.7 train no.301  loss = 1.2548753023147583\n",
      "epoch no.7 train no.311  loss = 0.9513671398162842\n",
      "epoch no.7 train no.321  loss = 1.0246765613555908\n",
      "epoch no.7 train no.331  loss = 1.1745613813400269\n",
      "epoch no.7 train no.341  loss = 1.23619544506073\n",
      "epoch no.7 train no.351  loss = 1.1340330839157104\n",
      "epoch no.7 train no.361  loss = 0.8932918906211853\n",
      "epoch no.7 train no.371  loss = 1.2104878425598145\n",
      "epoch no.7 train no.381  loss = 1.1822774410247803\n",
      "epoch no.7 train no.391  loss = 0.8467704057693481\n",
      "epoch no.7 train no.401  loss = 0.607928991317749\n",
      "epoch no.7 train no.411  loss = 0.7358039021492004\n",
      "epoch no.7 train no.421  loss = 0.8411740660667419\n",
      "epoch no.7 train no.431  loss = 0.9892295598983765\n",
      "epoch no.7 train no.441  loss = 0.8619006872177124\n",
      "epoch no.7 train no.451  loss = 0.9252700209617615\n",
      "epoch no.7 train no.461  loss = 0.8340939879417419\n",
      "epoch no.7 train no.471  loss = 1.0076693296432495\n",
      "epoch no.7 train no.481  loss = 0.8361767530441284\n",
      "epoch no.7 train no.491  loss = 1.651694416999817\n",
      "epoch no.7 train no.501  loss = 0.8432652950286865\n",
      "epoch no.7 train no.511  loss = 1.1206010580062866\n",
      "epoch no.7 train no.521  loss = 0.8163692355155945\n",
      "epoch no.7 train no.531  loss = 0.9911355376243591\n",
      "epoch no.7 train no.541  loss = 1.3702876567840576\n",
      "epoch no.7 train no.551  loss = 1.108151912689209\n",
      "epoch no.7 train no.561  loss = 1.2804501056671143\n",
      "epoch no.7 train no.571  loss = 1.0579679012298584\n",
      "epoch no.7 train no.581  loss = 0.8912953734397888\n",
      "epoch no.7 train no.591  loss = 0.6941706538200378\n",
      "epoch no.7 train no.601  loss = 0.6397859454154968\n",
      "epoch no.7 train no.611  loss = 0.7901101112365723\n",
      "epoch no.7 train no.621  loss = 1.0556691884994507\n",
      "epoch no.7 train no.631  loss = 1.6640245914459229\n",
      "epoch no.7 train no.641  loss = 1.7425013780593872\n",
      "epoch no.7 train no.651  loss = 0.6956267356872559\n",
      "epoch no.7 train no.661  loss = 0.8654727935791016\n",
      "epoch no.7 train no.671  loss = 1.0282435417175293\n",
      "epoch no.7 train no.681  loss = 2.423696517944336\n",
      "epoch no.7 train no.691  loss = 0.6922207474708557\n",
      "epoch no.7 train no.701  loss = 0.9145662188529968\n",
      "epoch no.7 train no.711  loss = 1.4318830966949463\n",
      "epoch no.7 train no.721  loss = 0.9628379344940186\n",
      "epoch no.7 train no.731  loss = 1.074925184249878\n",
      "epoch no.7 train no.741  loss = 1.0444672107696533\n",
      "epoch no.7 train no.751  loss = 0.9393801689147949\n",
      "epoch no.7 train no.761  loss = 0.6743729710578918\n",
      "epoch no.7 train no.771  loss = 1.574176549911499\n",
      "epoch no.7 train no.781  loss = 0.8927963376045227\n",
      "epoch no.7 train no.791  loss = 1.134653925895691\n",
      "epoch no.7 train no.801  loss = 0.637285590171814\n",
      "epoch no.7 train no.811  loss = 0.7483705878257751\n",
      "epoch no.7 train no.821  loss = 1.2459845542907715\n",
      "epoch no.7 train no.831  loss = 1.2031232118606567\n",
      "epoch no.7 train no.841  loss = 1.645979881286621\n",
      "epoch no.7 train no.851  loss = 0.9526965022087097\n",
      "epoch no.7 train no.861  loss = 0.751334547996521\n",
      "epoch no.7 train no.871  loss = 1.3152148723602295\n",
      "epoch no.7 train no.881  loss = 0.8877426385879517\n",
      "epoch no.7 train no.891  loss = 1.3254896402359009\n",
      "epoch no.7 train no.901  loss = 0.9815797209739685\n",
      "epoch no.7 train no.911  loss = 1.1184321641921997\n",
      "epoch no.7 train no.921  loss = 0.808771014213562\n",
      "epoch no.7 train no.931  loss = 0.9832507967948914\n",
      "epoch no.7 train no.941  loss = 1.0404257774353027\n",
      "epoch no.7 train no.951  loss = 1.501049518585205\n",
      "epoch no.7 train no.961  loss = 1.1816984415054321\n",
      "epoch no.7 train no.971  loss = 0.7658323645591736\n",
      "epoch no.7 train no.981  loss = 0.9378474354743958\n",
      "epoch no.7 train no.991  loss = 0.592903196811676\n",
      "epoch no.7 train no.1001  loss = 0.651707112789154\n",
      "epoch no.8 train no.1  loss = 0.7466546893119812\n",
      "epoch no.8 train no.11  loss = 1.0565468072891235\n",
      "epoch no.8 train no.21  loss = 0.7448561191558838\n",
      "epoch no.8 train no.31  loss = 0.514543354511261\n",
      "epoch no.8 train no.41  loss = 0.9647429585456848\n",
      "epoch no.8 train no.51  loss = 0.7726001143455505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.8 train no.61  loss = 1.184761643409729\n",
      "epoch no.8 train no.71  loss = 0.7587185502052307\n",
      "epoch no.8 train no.81  loss = 1.5116862058639526\n",
      "epoch no.8 train no.91  loss = 0.7726315855979919\n",
      "epoch no.8 train no.101  loss = 0.6460467576980591\n",
      "epoch no.8 train no.111  loss = 0.8987693786621094\n",
      "epoch no.8 train no.121  loss = 0.6520708799362183\n",
      "epoch no.8 train no.131  loss = 1.6125361919403076\n",
      "epoch no.8 train no.141  loss = 1.1687064170837402\n",
      "epoch no.8 train no.151  loss = 0.5930695533752441\n",
      "epoch no.8 train no.161  loss = 1.0385807752609253\n",
      "epoch no.8 train no.171  loss = 1.1283330917358398\n",
      "epoch no.8 train no.181  loss = 0.6884483695030212\n",
      "epoch no.8 train no.191  loss = 0.582840621471405\n",
      "epoch no.8 train no.201  loss = 0.5995208621025085\n",
      "epoch no.8 train no.211  loss = 1.03341543674469\n",
      "epoch no.8 train no.221  loss = 0.5929312705993652\n",
      "epoch no.8 train no.231  loss = 0.5350284576416016\n",
      "epoch no.8 train no.241  loss = 0.9930479526519775\n",
      "epoch no.8 train no.251  loss = 0.5218977332115173\n",
      "epoch no.8 train no.261  loss = 0.6957331895828247\n",
      "epoch no.8 train no.271  loss = 1.0258607864379883\n",
      "epoch no.8 train no.281  loss = 0.8292499780654907\n",
      "epoch no.8 train no.291  loss = 0.6846821904182434\n",
      "epoch no.8 train no.301  loss = 0.6750446557998657\n",
      "epoch no.8 train no.311  loss = 0.7657371163368225\n",
      "epoch no.8 train no.321  loss = 1.3943397998809814\n",
      "epoch no.8 train no.331  loss = 0.9592751860618591\n",
      "epoch no.8 train no.341  loss = 0.8905715346336365\n",
      "epoch no.8 train no.351  loss = 0.6899285912513733\n",
      "epoch no.8 train no.361  loss = 1.1611244678497314\n",
      "epoch no.8 train no.371  loss = 1.1422624588012695\n",
      "epoch no.8 train no.381  loss = 0.7649654150009155\n",
      "epoch no.8 train no.391  loss = 0.6155803203582764\n",
      "epoch no.8 train no.401  loss = 0.8357916474342346\n",
      "epoch no.8 train no.411  loss = 0.8687589764595032\n",
      "epoch no.8 train no.421  loss = 1.1128193140029907\n",
      "epoch no.8 train no.431  loss = 0.9257358908653259\n",
      "epoch no.8 train no.441  loss = 0.8633152842521667\n",
      "epoch no.8 train no.451  loss = 0.7101983428001404\n",
      "epoch no.8 train no.461  loss = 1.016005039215088\n",
      "epoch no.8 train no.471  loss = 1.1322083473205566\n",
      "epoch no.8 train no.481  loss = 0.9988843202590942\n",
      "epoch no.8 train no.491  loss = 0.8639776110649109\n",
      "epoch no.8 train no.501  loss = 0.8102633357048035\n",
      "epoch no.8 train no.511  loss = 0.8879567384719849\n",
      "epoch no.8 train no.521  loss = 0.6260221004486084\n",
      "epoch no.8 train no.531  loss = 0.8948285579681396\n",
      "epoch no.8 train no.541  loss = 0.5911304354667664\n",
      "epoch no.8 train no.551  loss = 0.6863231062889099\n",
      "epoch no.8 train no.561  loss = 0.8360755443572998\n",
      "epoch no.8 train no.571  loss = 0.9555537700653076\n",
      "epoch no.8 train no.581  loss = 0.7616322040557861\n",
      "epoch no.8 train no.591  loss = 0.7248082160949707\n",
      "epoch no.8 train no.601  loss = 0.5171343088150024\n",
      "epoch no.8 train no.611  loss = 1.5672920942306519\n",
      "epoch no.8 train no.621  loss = 0.9324018359184265\n",
      "epoch no.8 train no.631  loss = 0.8810461759567261\n",
      "epoch no.8 train no.641  loss = 0.5382398962974548\n",
      "epoch no.8 train no.651  loss = 1.1903475522994995\n",
      "epoch no.8 train no.661  loss = 0.8348453044891357\n",
      "epoch no.8 train no.671  loss = 0.7465693354606628\n",
      "epoch no.8 train no.681  loss = 0.9707845449447632\n",
      "epoch no.8 train no.691  loss = 1.0867516994476318\n",
      "epoch no.8 train no.701  loss = 0.9525793194770813\n",
      "epoch no.8 train no.711  loss = 1.17917799949646\n",
      "epoch no.8 train no.721  loss = 0.9379761815071106\n",
      "epoch no.8 train no.731  loss = 1.0102028846740723\n",
      "epoch no.8 train no.741  loss = 1.0958540439605713\n",
      "epoch no.8 train no.751  loss = 0.7765183448791504\n",
      "epoch no.8 train no.761  loss = 1.081923007965088\n",
      "epoch no.8 train no.771  loss = 0.9024706482887268\n",
      "epoch no.8 train no.781  loss = 0.9883384704589844\n",
      "epoch no.8 train no.791  loss = 0.8108991980552673\n",
      "epoch no.8 train no.801  loss = 0.6637448072433472\n",
      "epoch no.8 train no.811  loss = 0.9220293164253235\n",
      "epoch no.8 train no.821  loss = 0.6749359965324402\n",
      "epoch no.8 train no.831  loss = 0.6503073573112488\n",
      "epoch no.8 train no.841  loss = 0.8610484600067139\n",
      "epoch no.8 train no.851  loss = 1.0737369060516357\n",
      "epoch no.8 train no.861  loss = 0.5742052793502808\n",
      "epoch no.8 train no.871  loss = 1.168091893196106\n",
      "epoch no.8 train no.881  loss = 0.8464839458465576\n",
      "epoch no.8 train no.891  loss = 0.6580559015274048\n",
      "epoch no.8 train no.901  loss = 1.2487106323242188\n",
      "epoch no.8 train no.911  loss = 0.8767297267913818\n",
      "epoch no.8 train no.921  loss = 1.6250691413879395\n",
      "epoch no.8 train no.931  loss = 0.7128115296363831\n",
      "epoch no.8 train no.941  loss = 0.96785968542099\n",
      "epoch no.8 train no.951  loss = 0.9115909934043884\n",
      "epoch no.8 train no.961  loss = 1.283062219619751\n",
      "epoch no.8 train no.971  loss = 1.322481393814087\n",
      "epoch no.8 train no.981  loss = 1.315432071685791\n",
      "epoch no.8 train no.991  loss = 0.6028147339820862\n",
      "epoch no.8 train no.1001  loss = 0.5882017612457275\n",
      "epoch no.9 train no.1  loss = 0.6992377638816833\n",
      "epoch no.9 train no.11  loss = 1.439455509185791\n",
      "epoch no.9 train no.21  loss = 0.4215198755264282\n",
      "epoch no.9 train no.31  loss = 0.921759307384491\n",
      "epoch no.9 train no.41  loss = 1.1167912483215332\n",
      "epoch no.9 train no.51  loss = 0.7676782608032227\n",
      "epoch no.9 train no.61  loss = 0.6734234094619751\n",
      "epoch no.9 train no.71  loss = 0.5149699449539185\n",
      "epoch no.9 train no.81  loss = 0.8323894739151001\n",
      "epoch no.9 train no.91  loss = 0.7319366931915283\n",
      "epoch no.9 train no.101  loss = 0.7330653071403503\n",
      "epoch no.9 train no.111  loss = 0.5795673727989197\n",
      "epoch no.9 train no.121  loss = 1.2579294443130493\n",
      "epoch no.9 train no.131  loss = 0.8773263096809387\n",
      "epoch no.9 train no.141  loss = 0.7886855006217957\n",
      "epoch no.9 train no.151  loss = 1.2385976314544678\n",
      "epoch no.9 train no.161  loss = 0.38481786847114563\n",
      "epoch no.9 train no.171  loss = 1.0834152698516846\n",
      "epoch no.9 train no.181  loss = 0.5076658129692078\n",
      "epoch no.9 train no.191  loss = 0.5243760347366333\n",
      "epoch no.9 train no.201  loss = 0.40688037872314453\n",
      "epoch no.9 train no.211  loss = 1.1835039854049683\n",
      "epoch no.9 train no.221  loss = 0.9375881552696228\n",
      "epoch no.9 train no.231  loss = 0.6306480169296265\n",
      "epoch no.9 train no.241  loss = 0.8342871069908142\n",
      "epoch no.9 train no.251  loss = 0.6260030269622803\n",
      "epoch no.9 train no.261  loss = 0.8517509698867798\n",
      "epoch no.9 train no.271  loss = 1.0639288425445557\n",
      "epoch no.9 train no.281  loss = 1.2111115455627441\n",
      "epoch no.9 train no.291  loss = 0.654540479183197\n",
      "epoch no.9 train no.301  loss = 0.5478498339653015\n",
      "epoch no.9 train no.311  loss = 1.1530232429504395\n",
      "epoch no.9 train no.321  loss = 1.2512506246566772\n",
      "epoch no.9 train no.331  loss = 0.6106359362602234\n",
      "epoch no.9 train no.341  loss = 0.7349624633789062\n",
      "epoch no.9 train no.351  loss = 0.980779230594635\n",
      "epoch no.9 train no.361  loss = 0.8329697847366333\n",
      "epoch no.9 train no.371  loss = 0.8623424768447876\n",
      "epoch no.9 train no.381  loss = 0.7078283429145813\n",
      "epoch no.9 train no.391  loss = 0.6652724742889404\n",
      "epoch no.9 train no.401  loss = 0.9997652769088745\n",
      "epoch no.9 train no.411  loss = 0.5487903952598572\n",
      "epoch no.9 train no.421  loss = 0.8202102184295654\n",
      "epoch no.9 train no.431  loss = 0.6994975805282593\n",
      "epoch no.9 train no.441  loss = 0.6680265069007874\n",
      "epoch no.9 train no.451  loss = 0.8238157629966736\n",
      "epoch no.9 train no.461  loss = 0.9562029242515564\n",
      "epoch no.9 train no.471  loss = 0.4876861274242401\n",
      "epoch no.9 train no.481  loss = 0.7627169489860535\n",
      "epoch no.9 train no.491  loss = 1.4518587589263916\n",
      "epoch no.9 train no.501  loss = 0.6202123761177063\n",
      "epoch no.9 train no.511  loss = 0.7145438194274902\n",
      "epoch no.9 train no.521  loss = 0.60102778673172\n",
      "epoch no.9 train no.531  loss = 0.33021658658981323\n",
      "epoch no.9 train no.541  loss = 0.7388336658477783\n",
      "epoch no.9 train no.551  loss = 0.8907111287117004\n",
      "epoch no.9 train no.561  loss = 0.5821322202682495\n",
      "epoch no.9 train no.571  loss = 0.7961889505386353\n",
      "epoch no.9 train no.581  loss = 1.3078092336654663\n",
      "epoch no.9 train no.591  loss = 0.616472065448761\n",
      "epoch no.9 train no.601  loss = 0.7193850874900818\n",
      "epoch no.9 train no.611  loss = 0.9194574356079102\n",
      "epoch no.9 train no.621  loss = 0.7682265043258667\n",
      "epoch no.9 train no.631  loss = 0.787294864654541\n",
      "epoch no.9 train no.641  loss = 0.7181754112243652\n",
      "epoch no.9 train no.651  loss = 0.7632085084915161\n",
      "epoch no.9 train no.661  loss = 0.7130590081214905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.9 train no.671  loss = 0.48311758041381836\n",
      "epoch no.9 train no.681  loss = 1.2176669836044312\n",
      "epoch no.9 train no.691  loss = 0.7440375685691833\n",
      "epoch no.9 train no.701  loss = 0.6844147443771362\n",
      "epoch no.9 train no.711  loss = 0.5976671576499939\n",
      "epoch no.9 train no.721  loss = 0.9860909581184387\n",
      "epoch no.9 train no.731  loss = 0.7132629752159119\n",
      "epoch no.9 train no.741  loss = 1.5446348190307617\n",
      "epoch no.9 train no.751  loss = 0.8999874591827393\n",
      "epoch no.9 train no.761  loss = 1.3657379150390625\n",
      "epoch no.9 train no.771  loss = 0.7879794239997864\n",
      "epoch no.9 train no.781  loss = 1.0477503538131714\n",
      "epoch no.9 train no.791  loss = 0.7683079838752747\n",
      "epoch no.9 train no.801  loss = 1.0347017049789429\n",
      "epoch no.9 train no.811  loss = 1.1858642101287842\n",
      "epoch no.9 train no.821  loss = 0.8323380947113037\n",
      "epoch no.9 train no.831  loss = 0.6175355911254883\n",
      "epoch no.9 train no.841  loss = 0.8120747804641724\n",
      "epoch no.9 train no.851  loss = 0.47904297709465027\n",
      "epoch no.9 train no.861  loss = 1.1994258165359497\n",
      "epoch no.9 train no.871  loss = 0.9283351898193359\n",
      "epoch no.9 train no.881  loss = 0.6688762307167053\n",
      "epoch no.9 train no.891  loss = 0.7245249152183533\n",
      "epoch no.9 train no.901  loss = 1.0878623723983765\n",
      "epoch no.9 train no.911  loss = 0.7381327748298645\n",
      "epoch no.9 train no.921  loss = 1.3980151414871216\n",
      "epoch no.9 train no.931  loss = 0.7846485376358032\n",
      "epoch no.9 train no.941  loss = 0.9875733256340027\n",
      "epoch no.9 train no.951  loss = 0.7622747421264648\n",
      "epoch no.9 train no.961  loss = 0.7883592247962952\n",
      "epoch no.9 train no.971  loss = 0.6358325481414795\n",
      "epoch no.9 train no.981  loss = 1.135955572128296\n",
      "epoch no.9 train no.991  loss = 0.7974492907524109\n",
      "epoch no.9 train no.1001  loss = 0.6868162751197815\n",
      "epoch no.10 train no.1  loss = 0.5772624611854553\n",
      "epoch no.10 train no.11  loss = 0.5492730736732483\n",
      "epoch no.10 train no.21  loss = 0.5951568484306335\n",
      "epoch no.10 train no.31  loss = 0.774960458278656\n",
      "epoch no.10 train no.41  loss = 0.9348545670509338\n",
      "epoch no.10 train no.51  loss = 0.5061306357383728\n",
      "epoch no.10 train no.61  loss = 0.815246045589447\n",
      "epoch no.10 train no.71  loss = 0.8921751379966736\n",
      "epoch no.10 train no.81  loss = 1.0786187648773193\n",
      "epoch no.10 train no.91  loss = 0.8289539813995361\n",
      "epoch no.10 train no.101  loss = 0.42284533381462097\n",
      "epoch no.10 train no.111  loss = 0.6074703931808472\n",
      "epoch no.10 train no.121  loss = 0.720146119594574\n",
      "epoch no.10 train no.131  loss = 1.1131762266159058\n",
      "epoch no.10 train no.141  loss = 0.8081526756286621\n",
      "epoch no.10 train no.151  loss = 0.6373323798179626\n",
      "epoch no.10 train no.161  loss = 1.1926872730255127\n",
      "epoch no.10 train no.171  loss = 0.8537125587463379\n",
      "epoch no.10 train no.181  loss = 0.7629559636116028\n",
      "epoch no.10 train no.191  loss = 0.6446396112442017\n",
      "epoch no.10 train no.201  loss = 0.5161131024360657\n",
      "epoch no.10 train no.211  loss = 0.7392320036888123\n",
      "epoch no.10 train no.221  loss = 0.8999424576759338\n",
      "epoch no.10 train no.231  loss = 1.088494896888733\n",
      "epoch no.10 train no.241  loss = 0.8245477676391602\n",
      "epoch no.10 train no.251  loss = 0.5362076759338379\n",
      "epoch no.10 train no.261  loss = 0.6755123734474182\n",
      "epoch no.10 train no.271  loss = 0.8687455058097839\n",
      "epoch no.10 train no.281  loss = 0.6250958442687988\n",
      "epoch no.10 train no.291  loss = 0.6829578876495361\n",
      "epoch no.10 train no.301  loss = 0.44791293144226074\n",
      "epoch no.10 train no.311  loss = 0.9704901576042175\n",
      "epoch no.10 train no.321  loss = 0.6259223222732544\n",
      "epoch no.10 train no.331  loss = 0.5938396453857422\n",
      "epoch no.10 train no.341  loss = 0.6258575916290283\n",
      "epoch no.10 train no.351  loss = 0.5614670515060425\n",
      "epoch no.10 train no.361  loss = 0.6256554126739502\n",
      "epoch no.10 train no.371  loss = 0.5236127376556396\n",
      "epoch no.10 train no.381  loss = 0.9278897643089294\n",
      "epoch no.10 train no.391  loss = 0.5060377717018127\n",
      "epoch no.10 train no.401  loss = 0.8388924598693848\n",
      "epoch no.10 train no.411  loss = 0.6754957437515259\n",
      "epoch no.10 train no.421  loss = 0.9426364898681641\n",
      "epoch no.10 train no.431  loss = 0.5583227872848511\n",
      "epoch no.10 train no.441  loss = 0.3440049886703491\n",
      "epoch no.10 train no.451  loss = 0.6867008805274963\n",
      "epoch no.10 train no.461  loss = 0.5329223275184631\n",
      "epoch no.10 train no.471  loss = 0.39084386825561523\n",
      "epoch no.10 train no.481  loss = 0.9833536744117737\n",
      "epoch no.10 train no.491  loss = 0.7681222558021545\n",
      "epoch no.10 train no.501  loss = 0.7803868651390076\n",
      "epoch no.10 train no.511  loss = 0.8739259839057922\n",
      "epoch no.10 train no.521  loss = 0.8382263779640198\n",
      "epoch no.10 train no.531  loss = 0.3216923475265503\n",
      "epoch no.10 train no.541  loss = 1.07658052444458\n",
      "epoch no.10 train no.551  loss = 0.9166777729988098\n",
      "epoch no.10 train no.561  loss = 0.6752973198890686\n",
      "epoch no.10 train no.571  loss = 0.5547308325767517\n",
      "epoch no.10 train no.581  loss = 0.41868993639945984\n",
      "epoch no.10 train no.591  loss = 0.5275812745094299\n",
      "epoch no.10 train no.601  loss = 0.6461315751075745\n",
      "epoch no.10 train no.611  loss = 1.0169415473937988\n",
      "epoch no.10 train no.621  loss = 0.6020260453224182\n",
      "epoch no.10 train no.631  loss = 1.3563867807388306\n",
      "epoch no.10 train no.641  loss = 0.5518091320991516\n",
      "epoch no.10 train no.651  loss = 1.5748049020767212\n",
      "epoch no.10 train no.661  loss = 0.42759203910827637\n",
      "epoch no.10 train no.671  loss = 1.1873149871826172\n",
      "epoch no.10 train no.681  loss = 0.7572329640388489\n",
      "epoch no.10 train no.691  loss = 1.071123719215393\n",
      "epoch no.10 train no.701  loss = 0.8394629955291748\n",
      "epoch no.10 train no.711  loss = 0.6296687126159668\n",
      "epoch no.10 train no.721  loss = 0.5410032272338867\n",
      "epoch no.10 train no.731  loss = 0.4193227291107178\n",
      "epoch no.10 train no.741  loss = 1.1023261547088623\n",
      "epoch no.10 train no.751  loss = 0.5757431983947754\n",
      "epoch no.10 train no.761  loss = 0.7686988115310669\n",
      "epoch no.10 train no.771  loss = 0.4605623185634613\n",
      "epoch no.10 train no.781  loss = 0.7063338756561279\n",
      "epoch no.10 train no.791  loss = 0.5458813309669495\n",
      "epoch no.10 train no.801  loss = 0.6586911082267761\n",
      "epoch no.10 train no.811  loss = 1.2426071166992188\n",
      "epoch no.10 train no.821  loss = 1.0169978141784668\n",
      "epoch no.10 train no.831  loss = 0.6653006672859192\n",
      "epoch no.10 train no.841  loss = 0.591162383556366\n",
      "epoch no.10 train no.851  loss = 0.763615071773529\n",
      "epoch no.10 train no.861  loss = 0.9713237285614014\n",
      "epoch no.10 train no.871  loss = 0.4126800000667572\n",
      "epoch no.10 train no.881  loss = 0.4892667233943939\n",
      "epoch no.10 train no.891  loss = 0.9707808494567871\n",
      "epoch no.10 train no.901  loss = 0.8275496959686279\n",
      "epoch no.10 train no.911  loss = 0.7956365942955017\n",
      "epoch no.10 train no.921  loss = 0.7770251631736755\n",
      "epoch no.10 train no.931  loss = 0.6698796153068542\n",
      "epoch no.10 train no.941  loss = 0.5420524477958679\n",
      "epoch no.10 train no.951  loss = 0.6750981211662292\n",
      "epoch no.10 train no.961  loss = 0.78125\n",
      "epoch no.10 train no.971  loss = 0.7065879702568054\n",
      "epoch no.10 train no.981  loss = 1.1447416543960571\n",
      "epoch no.10 train no.991  loss = 0.6795295476913452\n",
      "epoch no.10 train no.1001  loss = 0.8089144825935364\n",
      "epoch no.11 train no.1  loss = 0.4027034044265747\n",
      "epoch no.11 train no.11  loss = 0.672507107257843\n",
      "epoch no.11 train no.21  loss = 0.4950064718723297\n",
      "epoch no.11 train no.31  loss = 0.5394080281257629\n",
      "epoch no.11 train no.41  loss = 0.941040575504303\n",
      "epoch no.11 train no.51  loss = 0.6023724675178528\n",
      "epoch no.11 train no.61  loss = 0.7761081457138062\n",
      "epoch no.11 train no.71  loss = 0.5630462169647217\n",
      "epoch no.11 train no.81  loss = 0.5240895748138428\n",
      "epoch no.11 train no.91  loss = 0.5063856244087219\n",
      "epoch no.11 train no.101  loss = 0.3024471700191498\n",
      "epoch no.11 train no.111  loss = 1.21425199508667\n",
      "epoch no.11 train no.121  loss = 1.2067562341690063\n",
      "epoch no.11 train no.131  loss = 0.9915658831596375\n",
      "epoch no.11 train no.141  loss = 0.5955911874771118\n",
      "epoch no.11 train no.151  loss = 0.34730228781700134\n",
      "epoch no.11 train no.161  loss = 0.7558037638664246\n",
      "epoch no.11 train no.171  loss = 0.36652687191963196\n",
      "epoch no.11 train no.181  loss = 0.5692791938781738\n",
      "epoch no.11 train no.191  loss = 0.6354643106460571\n",
      "epoch no.11 train no.201  loss = 0.7350884079933167\n",
      "epoch no.11 train no.211  loss = 0.7767115831375122\n",
      "epoch no.11 train no.221  loss = 0.6629198789596558\n",
      "epoch no.11 train no.231  loss = 0.7122227549552917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.11 train no.241  loss = 0.5850225687026978\n",
      "epoch no.11 train no.251  loss = 0.6648749709129333\n",
      "epoch no.11 train no.261  loss = 0.8204333782196045\n",
      "epoch no.11 train no.271  loss = 0.6522020697593689\n",
      "epoch no.11 train no.281  loss = 0.4984000325202942\n",
      "epoch no.11 train no.291  loss = 0.5767327547073364\n",
      "epoch no.11 train no.301  loss = 0.43401509523391724\n",
      "epoch no.11 train no.311  loss = 0.8022764921188354\n",
      "epoch no.11 train no.321  loss = 0.3753451108932495\n",
      "epoch no.11 train no.331  loss = 0.364397794008255\n",
      "epoch no.11 train no.341  loss = 0.4705328047275543\n",
      "epoch no.11 train no.351  loss = 0.6018828749656677\n",
      "epoch no.11 train no.361  loss = 0.46776634454727173\n",
      "epoch no.11 train no.371  loss = 0.9227294325828552\n",
      "epoch no.11 train no.381  loss = 0.4391266405582428\n",
      "epoch no.11 train no.391  loss = 1.1453801393508911\n",
      "epoch no.11 train no.401  loss = 0.7808457612991333\n",
      "epoch no.11 train no.411  loss = 0.5700615048408508\n",
      "epoch no.11 train no.421  loss = 0.8592618107795715\n",
      "epoch no.11 train no.431  loss = 0.460880309343338\n",
      "epoch no.11 train no.441  loss = 1.0926313400268555\n",
      "epoch no.11 train no.451  loss = 0.5182254314422607\n",
      "epoch no.11 train no.461  loss = 0.6479503512382507\n",
      "epoch no.11 train no.471  loss = 0.95716792345047\n",
      "epoch no.11 train no.481  loss = 0.5943440794944763\n",
      "epoch no.11 train no.491  loss = 0.5435850620269775\n",
      "epoch no.11 train no.501  loss = 0.7774733901023865\n",
      "epoch no.11 train no.511  loss = 0.5277606248855591\n",
      "epoch no.11 train no.521  loss = 0.6957929730415344\n",
      "epoch no.11 train no.531  loss = 1.3530917167663574\n",
      "epoch no.11 train no.541  loss = 0.647100031375885\n",
      "epoch no.11 train no.551  loss = 0.5951515436172485\n",
      "epoch no.11 train no.561  loss = 0.6798861026763916\n",
      "epoch no.11 train no.571  loss = 0.5464420914649963\n",
      "epoch no.11 train no.581  loss = 0.43915116786956787\n",
      "epoch no.11 train no.591  loss = 1.5913915634155273\n",
      "epoch no.11 train no.601  loss = 0.7952705025672913\n",
      "epoch no.11 train no.611  loss = 0.4679638743400574\n",
      "epoch no.11 train no.621  loss = 0.7703371644020081\n",
      "epoch no.11 train no.631  loss = 1.1994960308074951\n",
      "epoch no.11 train no.641  loss = 0.7069406509399414\n",
      "epoch no.11 train no.651  loss = 1.436574101448059\n",
      "epoch no.11 train no.661  loss = 0.9674696326255798\n",
      "epoch no.11 train no.671  loss = 0.9745683073997498\n",
      "epoch no.11 train no.681  loss = 0.4914702773094177\n",
      "epoch no.11 train no.691  loss = 0.8481013774871826\n",
      "epoch no.11 train no.701  loss = 0.6233755946159363\n",
      "epoch no.11 train no.711  loss = 0.5568516254425049\n",
      "epoch no.11 train no.721  loss = 1.0875173807144165\n",
      "epoch no.11 train no.731  loss = 0.5449857711791992\n",
      "epoch no.11 train no.741  loss = 0.7193737626075745\n",
      "epoch no.11 train no.751  loss = 0.47315239906311035\n",
      "epoch no.11 train no.761  loss = 0.7193475961685181\n",
      "epoch no.11 train no.771  loss = 0.9217253923416138\n",
      "epoch no.11 train no.781  loss = 0.6596624255180359\n",
      "epoch no.11 train no.791  loss = 0.5403181910514832\n",
      "epoch no.11 train no.801  loss = 0.813642144203186\n",
      "epoch no.11 train no.811  loss = 0.4596758186817169\n",
      "epoch no.11 train no.821  loss = 0.6299866437911987\n",
      "epoch no.11 train no.831  loss = 0.8189141154289246\n",
      "epoch no.11 train no.841  loss = 0.8695409893989563\n",
      "epoch no.11 train no.851  loss = 0.9346238374710083\n",
      "epoch no.11 train no.861  loss = 0.3565603792667389\n",
      "epoch no.11 train no.871  loss = 0.5322562456130981\n",
      "epoch no.11 train no.881  loss = 0.9031778573989868\n",
      "epoch no.11 train no.891  loss = 0.5536608099937439\n",
      "epoch no.11 train no.901  loss = 1.0062282085418701\n",
      "epoch no.11 train no.911  loss = 0.5417535305023193\n",
      "epoch no.11 train no.921  loss = 0.3675859272480011\n",
      "epoch no.11 train no.931  loss = 0.6906412243843079\n",
      "epoch no.11 train no.941  loss = 0.6833952069282532\n",
      "epoch no.11 train no.951  loss = 1.038667917251587\n",
      "epoch no.11 train no.961  loss = 0.7104189395904541\n",
      "epoch no.11 train no.971  loss = 1.0288628339767456\n",
      "epoch no.11 train no.981  loss = 0.7878545522689819\n",
      "epoch no.11 train no.991  loss = 0.7403719425201416\n",
      "epoch no.11 train no.1001  loss = 1.1645045280456543\n",
      "epoch no.12 train no.1  loss = 0.31124147772789\n",
      "epoch no.12 train no.11  loss = 0.7552018761634827\n",
      "epoch no.12 train no.21  loss = 0.36830970644950867\n",
      "epoch no.12 train no.31  loss = 0.617660403251648\n",
      "epoch no.12 train no.41  loss = 0.5304158926010132\n",
      "epoch no.12 train no.51  loss = 0.39027950167655945\n",
      "epoch no.12 train no.61  loss = 0.33785292506217957\n",
      "epoch no.12 train no.71  loss = 0.4696965217590332\n",
      "epoch no.12 train no.81  loss = 0.6364768147468567\n",
      "epoch no.12 train no.91  loss = 0.45558804273605347\n",
      "epoch no.12 train no.101  loss = 0.33011355996131897\n",
      "epoch no.12 train no.111  loss = 0.42803123593330383\n",
      "epoch no.12 train no.121  loss = 0.6406907439231873\n",
      "epoch no.12 train no.131  loss = 0.5607500076293945\n",
      "epoch no.12 train no.141  loss = 0.5753943920135498\n",
      "epoch no.12 train no.151  loss = 0.44151344895362854\n",
      "epoch no.12 train no.161  loss = 0.5785179138183594\n",
      "epoch no.12 train no.171  loss = 0.58290696144104\n",
      "epoch no.12 train no.181  loss = 0.5583840012550354\n",
      "epoch no.12 train no.191  loss = 0.45519959926605225\n",
      "epoch no.12 train no.201  loss = 0.36544862389564514\n",
      "epoch no.12 train no.211  loss = 0.3042001724243164\n",
      "epoch no.12 train no.221  loss = 0.30997273325920105\n",
      "epoch no.12 train no.231  loss = 0.8600078225135803\n",
      "epoch no.12 train no.241  loss = 0.6156796216964722\n",
      "epoch no.12 train no.251  loss = 0.6961183547973633\n",
      "epoch no.12 train no.261  loss = 0.7357821464538574\n",
      "epoch no.12 train no.271  loss = 0.2750052511692047\n",
      "epoch no.12 train no.281  loss = 0.6343802809715271\n",
      "epoch no.12 train no.291  loss = 1.2378822565078735\n",
      "epoch no.12 train no.301  loss = 0.4400862157344818\n",
      "epoch no.12 train no.311  loss = 0.7021541595458984\n",
      "epoch no.12 train no.321  loss = 0.40061241388320923\n",
      "epoch no.12 train no.331  loss = 0.37584975361824036\n",
      "epoch no.12 train no.341  loss = 0.3921477794647217\n",
      "epoch no.12 train no.351  loss = 0.4105502665042877\n",
      "epoch no.12 train no.361  loss = 0.5031287670135498\n",
      "epoch no.12 train no.371  loss = 0.4851436913013458\n",
      "epoch no.12 train no.381  loss = 0.5807356238365173\n",
      "epoch no.12 train no.391  loss = 0.612476110458374\n",
      "epoch no.12 train no.401  loss = 0.32231613993644714\n",
      "epoch no.12 train no.411  loss = 0.45054295659065247\n",
      "epoch no.12 train no.421  loss = 0.6992508172988892\n",
      "epoch no.12 train no.431  loss = 0.6218140721321106\n",
      "epoch no.12 train no.441  loss = 0.49996447563171387\n",
      "epoch no.12 train no.451  loss = 0.3773682117462158\n",
      "epoch no.12 train no.461  loss = 0.6067730784416199\n",
      "epoch no.12 train no.471  loss = 0.7209352850914001\n",
      "epoch no.12 train no.481  loss = 0.5503261685371399\n",
      "epoch no.12 train no.491  loss = 0.8323146104812622\n",
      "epoch no.12 train no.501  loss = 0.5964440703392029\n",
      "epoch no.12 train no.511  loss = 0.36900514364242554\n",
      "epoch no.12 train no.521  loss = 0.41770580410957336\n",
      "epoch no.12 train no.531  loss = 0.7930148243904114\n",
      "epoch no.12 train no.541  loss = 0.6638539433479309\n",
      "epoch no.12 train no.551  loss = 0.4662931263446808\n",
      "epoch no.12 train no.561  loss = 0.5519960522651672\n",
      "epoch no.12 train no.571  loss = 0.5463979840278625\n",
      "epoch no.12 train no.581  loss = 1.026711106300354\n",
      "epoch no.12 train no.591  loss = 1.6317459344863892\n",
      "epoch no.12 train no.601  loss = 0.5034999251365662\n",
      "epoch no.12 train no.611  loss = 0.38204964995384216\n",
      "epoch no.12 train no.621  loss = 0.6190212368965149\n",
      "epoch no.12 train no.631  loss = 0.6225271821022034\n",
      "epoch no.12 train no.641  loss = 0.29008591175079346\n",
      "epoch no.12 train no.651  loss = 0.6387434601783752\n",
      "epoch no.12 train no.661  loss = 0.4185728132724762\n",
      "epoch no.12 train no.671  loss = 0.68843013048172\n",
      "epoch no.12 train no.681  loss = 1.3407388925552368\n",
      "epoch no.12 train no.691  loss = 0.46988511085510254\n",
      "epoch no.12 train no.701  loss = 0.9195123910903931\n",
      "epoch no.12 train no.711  loss = 0.3358481228351593\n",
      "epoch no.12 train no.721  loss = 0.5414775609970093\n",
      "epoch no.12 train no.731  loss = 0.6143578886985779\n",
      "epoch no.12 train no.741  loss = 0.5095864534378052\n",
      "epoch no.12 train no.751  loss = 0.4845358729362488\n",
      "epoch no.12 train no.761  loss = 0.7154123187065125\n",
      "epoch no.12 train no.771  loss = 0.39874109625816345\n",
      "epoch no.12 train no.781  loss = 0.2680607736110687\n",
      "epoch no.12 train no.791  loss = 0.5740748643875122\n",
      "epoch no.12 train no.801  loss = 0.715631365776062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.12 train no.811  loss = 0.7213579416275024\n",
      "epoch no.12 train no.821  loss = 0.391644150018692\n",
      "epoch no.12 train no.831  loss = 0.47321468591690063\n",
      "epoch no.12 train no.841  loss = 1.4739477634429932\n",
      "epoch no.12 train no.851  loss = 0.5411823987960815\n",
      "epoch no.12 train no.861  loss = 0.5725919008255005\n",
      "epoch no.12 train no.871  loss = 0.5967793464660645\n",
      "epoch no.12 train no.881  loss = 0.39058586955070496\n",
      "epoch no.12 train no.891  loss = 0.5859951972961426\n",
      "epoch no.12 train no.901  loss = 0.7341110706329346\n",
      "epoch no.12 train no.911  loss = 0.5936465263366699\n",
      "epoch no.12 train no.921  loss = 0.7097305655479431\n",
      "epoch no.12 train no.931  loss = 0.44478392601013184\n",
      "epoch no.12 train no.941  loss = 0.6939183473587036\n",
      "epoch no.12 train no.951  loss = 0.8143746256828308\n",
      "epoch no.12 train no.961  loss = 0.635025680065155\n",
      "epoch no.12 train no.971  loss = 0.8394742012023926\n",
      "epoch no.12 train no.981  loss = 0.4072543680667877\n",
      "epoch no.12 train no.991  loss = 0.5079439878463745\n",
      "epoch no.12 train no.1001  loss = 0.5240214467048645\n",
      "epoch no.13 train no.1  loss = 0.5266969799995422\n",
      "epoch no.13 train no.11  loss = 0.35628965497016907\n",
      "epoch no.13 train no.21  loss = 0.731423020362854\n",
      "epoch no.13 train no.31  loss = 0.7906666398048401\n",
      "epoch no.13 train no.41  loss = 0.5575569868087769\n",
      "epoch no.13 train no.51  loss = 0.33010146021842957\n",
      "epoch no.13 train no.61  loss = 0.4817866384983063\n",
      "epoch no.13 train no.71  loss = 1.3138282299041748\n",
      "epoch no.13 train no.81  loss = 0.43276458978652954\n",
      "epoch no.13 train no.91  loss = 0.3513997495174408\n",
      "epoch no.13 train no.101  loss = 0.6329500079154968\n",
      "epoch no.13 train no.111  loss = 0.556447446346283\n",
      "epoch no.13 train no.121  loss = 0.5705028176307678\n",
      "epoch no.13 train no.131  loss = 0.34803810715675354\n",
      "epoch no.13 train no.141  loss = 0.4767550528049469\n",
      "epoch no.13 train no.151  loss = 0.5458106994628906\n",
      "epoch no.13 train no.161  loss = 0.7671296000480652\n",
      "epoch no.13 train no.171  loss = 0.5736661553382874\n",
      "epoch no.13 train no.181  loss = 0.9760551452636719\n",
      "epoch no.13 train no.191  loss = 0.37302982807159424\n",
      "epoch no.13 train no.201  loss = 0.6138351559638977\n",
      "epoch no.13 train no.211  loss = 0.2772262692451477\n",
      "epoch no.13 train no.221  loss = 0.692488431930542\n",
      "epoch no.13 train no.231  loss = 0.48780205845832825\n",
      "epoch no.13 train no.241  loss = 1.4350167512893677\n",
      "epoch no.13 train no.251  loss = 1.0157461166381836\n",
      "epoch no.13 train no.261  loss = 0.45667293667793274\n",
      "epoch no.13 train no.271  loss = 0.31000199913978577\n",
      "epoch no.13 train no.281  loss = 0.8085671663284302\n",
      "epoch no.13 train no.291  loss = 0.6711365580558777\n",
      "epoch no.13 train no.301  loss = 0.39329618215560913\n",
      "epoch no.13 train no.311  loss = 0.2405005693435669\n",
      "epoch no.13 train no.321  loss = 0.525646984577179\n",
      "epoch no.13 train no.331  loss = 0.5759941339492798\n",
      "epoch no.13 train no.341  loss = 0.4194867014884949\n",
      "epoch no.13 train no.351  loss = 0.33885428309440613\n",
      "epoch no.13 train no.361  loss = 0.8021968007087708\n",
      "epoch no.13 train no.371  loss = 0.4464772939682007\n",
      "epoch no.13 train no.381  loss = 0.4065130949020386\n",
      "epoch no.13 train no.391  loss = 0.45909008383750916\n",
      "epoch no.13 train no.401  loss = 0.5233370661735535\n",
      "epoch no.13 train no.411  loss = 1.3867589235305786\n",
      "epoch no.13 train no.421  loss = 0.5861039757728577\n",
      "epoch no.13 train no.431  loss = 0.35065770149230957\n",
      "epoch no.13 train no.441  loss = 0.7853293418884277\n",
      "epoch no.13 train no.451  loss = 0.7114785313606262\n",
      "epoch no.13 train no.461  loss = 0.3931742012500763\n",
      "epoch no.13 train no.471  loss = 0.7814729809761047\n",
      "epoch no.13 train no.481  loss = 0.6841458082199097\n",
      "epoch no.13 train no.491  loss = 0.623783528804779\n",
      "epoch no.13 train no.501  loss = 0.5048863887786865\n",
      "epoch no.13 train no.511  loss = 0.5794387459754944\n",
      "epoch no.13 train no.521  loss = 1.9779061079025269\n",
      "epoch no.13 train no.531  loss = 0.5131627321243286\n",
      "epoch no.13 train no.541  loss = 0.5179113149642944\n",
      "epoch no.13 train no.551  loss = 0.39961186051368713\n",
      "epoch no.13 train no.561  loss = 0.44013455510139465\n",
      "epoch no.13 train no.571  loss = 0.5028979182243347\n",
      "epoch no.13 train no.581  loss = 0.3741830885410309\n",
      "epoch no.13 train no.591  loss = 0.5128475427627563\n",
      "epoch no.13 train no.601  loss = 0.8484248518943787\n",
      "epoch no.13 train no.611  loss = 0.7920079827308655\n",
      "epoch no.13 train no.621  loss = 0.5568356513977051\n",
      "epoch no.13 train no.631  loss = 0.5785263180732727\n",
      "epoch no.13 train no.641  loss = 0.9617196917533875\n",
      "epoch no.13 train no.651  loss = 1.6912251710891724\n",
      "epoch no.13 train no.661  loss = 0.5199565887451172\n",
      "epoch no.13 train no.671  loss = 0.7627605199813843\n",
      "epoch no.13 train no.681  loss = 0.6441319584846497\n",
      "epoch no.13 train no.691  loss = 0.34935319423675537\n",
      "epoch no.13 train no.701  loss = 0.3908449411392212\n",
      "epoch no.13 train no.711  loss = 0.2651873826980591\n",
      "epoch no.13 train no.721  loss = 0.5943058729171753\n",
      "epoch no.13 train no.731  loss = 0.4885769784450531\n",
      "epoch no.13 train no.741  loss = 0.7738755941390991\n",
      "epoch no.13 train no.751  loss = 0.6172676682472229\n",
      "epoch no.13 train no.761  loss = 1.6538997888565063\n",
      "epoch no.13 train no.771  loss = 0.2825218737125397\n",
      "epoch no.13 train no.781  loss = 0.4971376657485962\n",
      "epoch no.13 train no.791  loss = 0.8084123134613037\n",
      "epoch no.13 train no.801  loss = 0.7034711837768555\n",
      "epoch no.13 train no.811  loss = 0.3793601095676422\n",
      "epoch no.13 train no.821  loss = 0.5901214480400085\n",
      "epoch no.13 train no.831  loss = 0.37685972452163696\n",
      "epoch no.13 train no.841  loss = 0.8052516579627991\n",
      "epoch no.13 train no.851  loss = 0.6575021743774414\n",
      "epoch no.13 train no.861  loss = 0.4357626438140869\n",
      "epoch no.13 train no.871  loss = 1.2361787557601929\n",
      "epoch no.13 train no.881  loss = 0.45704385638237\n",
      "epoch no.13 train no.891  loss = 0.9130865931510925\n",
      "epoch no.13 train no.901  loss = 0.881498396396637\n",
      "epoch no.13 train no.911  loss = 0.4949977993965149\n",
      "epoch no.13 train no.921  loss = 0.6292012929916382\n",
      "epoch no.13 train no.931  loss = 0.9110223650932312\n",
      "epoch no.13 train no.941  loss = 0.6162707209587097\n",
      "epoch no.13 train no.951  loss = 0.45522341132164\n",
      "epoch no.13 train no.961  loss = 0.6919354796409607\n",
      "epoch no.13 train no.971  loss = 0.6253014206886292\n",
      "epoch no.13 train no.981  loss = 0.992111086845398\n",
      "epoch no.13 train no.991  loss = 0.5041934847831726\n",
      "epoch no.13 train no.1001  loss = 0.875883162021637\n",
      "epoch no.14 train no.1  loss = 1.3338792324066162\n",
      "epoch no.14 train no.11  loss = 0.4266773760318756\n",
      "epoch no.14 train no.21  loss = 0.42081478238105774\n",
      "epoch no.14 train no.31  loss = 0.5291614532470703\n",
      "epoch no.14 train no.41  loss = 0.31644970178604126\n",
      "epoch no.14 train no.51  loss = 0.37558096647262573\n",
      "epoch no.14 train no.61  loss = 0.384648859500885\n",
      "epoch no.14 train no.71  loss = 0.45160213112831116\n",
      "epoch no.14 train no.81  loss = 0.348832368850708\n",
      "epoch no.14 train no.91  loss = 0.4603610932826996\n",
      "epoch no.14 train no.101  loss = 0.38627657294273376\n",
      "epoch no.14 train no.111  loss = 0.38638564944267273\n",
      "epoch no.14 train no.121  loss = 0.3597099483013153\n",
      "epoch no.14 train no.131  loss = 0.80226731300354\n",
      "epoch no.14 train no.141  loss = 1.0840407609939575\n",
      "epoch no.14 train no.151  loss = 0.2819838523864746\n",
      "epoch no.14 train no.161  loss = 0.37044021487236023\n",
      "epoch no.14 train no.171  loss = 1.0967525243759155\n",
      "epoch no.14 train no.181  loss = 0.5486096143722534\n",
      "epoch no.14 train no.191  loss = 0.43781477212905884\n",
      "epoch no.14 train no.201  loss = 0.5102865695953369\n",
      "epoch no.14 train no.211  loss = 0.9376787543296814\n",
      "epoch no.14 train no.221  loss = 0.43206483125686646\n",
      "epoch no.14 train no.231  loss = 0.5418516397476196\n",
      "epoch no.14 train no.241  loss = 0.5624956488609314\n",
      "epoch no.14 train no.251  loss = 0.6862643957138062\n",
      "epoch no.14 train no.261  loss = 0.6796655058860779\n",
      "epoch no.14 train no.271  loss = 0.42846906185150146\n",
      "epoch no.14 train no.281  loss = 0.703393816947937\n",
      "epoch no.14 train no.291  loss = 0.854690670967102\n",
      "epoch no.14 train no.301  loss = 0.6340166330337524\n",
      "epoch no.14 train no.311  loss = 1.073683500289917\n",
      "epoch no.14 train no.321  loss = 0.3495267331600189\n",
      "epoch no.14 train no.331  loss = 0.3680052161216736\n",
      "epoch no.14 train no.341  loss = 0.4592306911945343\n",
      "epoch no.14 train no.351  loss = 0.30114561319351196\n",
      "epoch no.14 train no.361  loss = 0.2734645903110504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.14 train no.371  loss = 0.41149699687957764\n",
      "epoch no.14 train no.381  loss = 0.46649882197380066\n",
      "epoch no.14 train no.391  loss = 0.8197914361953735\n",
      "epoch no.14 train no.401  loss = 0.5667065978050232\n",
      "epoch no.14 train no.411  loss = 0.6270541548728943\n",
      "epoch no.14 train no.421  loss = 0.6916685700416565\n",
      "epoch no.14 train no.431  loss = 0.6211625933647156\n",
      "epoch no.14 train no.441  loss = 0.6750686168670654\n",
      "epoch no.14 train no.451  loss = 0.5859547257423401\n",
      "epoch no.14 train no.461  loss = 0.8958561420440674\n",
      "epoch no.14 train no.471  loss = 0.45667505264282227\n",
      "epoch no.14 train no.481  loss = 0.36927372217178345\n",
      "epoch no.14 train no.491  loss = 1.0471059083938599\n",
      "epoch no.14 train no.501  loss = 0.5653105974197388\n",
      "epoch no.14 train no.511  loss = 0.44660332798957825\n",
      "epoch no.14 train no.521  loss = 0.4300178289413452\n",
      "epoch no.14 train no.531  loss = 0.6571694016456604\n",
      "epoch no.14 train no.541  loss = 0.37391364574432373\n",
      "epoch no.14 train no.551  loss = 0.4517187774181366\n",
      "epoch no.14 train no.561  loss = 0.33446335792541504\n",
      "epoch no.14 train no.571  loss = 0.7927261590957642\n",
      "epoch no.14 train no.581  loss = 0.37142306566238403\n",
      "epoch no.14 train no.591  loss = 0.4897673726081848\n",
      "epoch no.14 train no.601  loss = 0.6166766285896301\n",
      "epoch no.14 train no.611  loss = 0.4829649031162262\n",
      "epoch no.14 train no.621  loss = 0.6040025949478149\n",
      "epoch no.14 train no.631  loss = 0.5443047881126404\n",
      "epoch no.14 train no.641  loss = 1.0730245113372803\n",
      "epoch no.14 train no.651  loss = 0.32851696014404297\n",
      "epoch no.14 train no.661  loss = 1.4430811405181885\n",
      "epoch no.14 train no.671  loss = 0.5104749798774719\n",
      "epoch no.14 train no.681  loss = 0.3478669822216034\n",
      "epoch no.14 train no.691  loss = 0.5806285738945007\n",
      "epoch no.14 train no.701  loss = 0.5106618404388428\n",
      "epoch no.14 train no.711  loss = 0.7267534136772156\n",
      "epoch no.14 train no.721  loss = 0.27579039335250854\n",
      "epoch no.14 train no.731  loss = 0.34184888005256653\n",
      "epoch no.14 train no.741  loss = 0.48091650009155273\n",
      "epoch no.14 train no.751  loss = 0.6729533672332764\n",
      "epoch no.14 train no.761  loss = 0.597435474395752\n",
      "epoch no.14 train no.771  loss = 0.5213541388511658\n",
      "epoch no.14 train no.781  loss = 0.8963301777839661\n",
      "epoch no.14 train no.791  loss = 0.5430224537849426\n",
      "epoch no.14 train no.801  loss = 0.34067121148109436\n",
      "epoch no.14 train no.811  loss = 0.6060554385185242\n",
      "epoch no.14 train no.821  loss = 0.6468448042869568\n",
      "epoch no.14 train no.831  loss = 0.644913911819458\n",
      "epoch no.14 train no.841  loss = 0.7731426954269409\n",
      "epoch no.14 train no.851  loss = 0.43825632333755493\n",
      "epoch no.14 train no.861  loss = 0.579938530921936\n",
      "epoch no.14 train no.871  loss = 0.29214271903038025\n",
      "epoch no.14 train no.881  loss = 0.39974942803382874\n",
      "epoch no.14 train no.891  loss = 0.8668597340583801\n",
      "epoch no.14 train no.901  loss = 0.6473312377929688\n",
      "epoch no.14 train no.911  loss = 0.3836856484413147\n",
      "epoch no.14 train no.921  loss = 0.8380454778671265\n",
      "epoch no.14 train no.931  loss = 0.34181079268455505\n",
      "epoch no.14 train no.941  loss = 0.978561282157898\n",
      "epoch no.14 train no.951  loss = 0.34803223609924316\n",
      "epoch no.14 train no.961  loss = 0.7624784111976624\n",
      "epoch no.14 train no.971  loss = 1.5152571201324463\n",
      "epoch no.14 train no.981  loss = 0.7302641868591309\n",
      "epoch no.14 train no.991  loss = 0.5727107524871826\n",
      "epoch no.14 train no.1001  loss = 0.39256051182746887\n",
      "epoch no.15 train no.1  loss = 0.2514941692352295\n",
      "epoch no.15 train no.11  loss = 0.32975950837135315\n",
      "epoch no.15 train no.21  loss = 0.4494231641292572\n",
      "epoch no.15 train no.31  loss = 0.7109499573707581\n",
      "epoch no.15 train no.41  loss = 0.6113002300262451\n",
      "epoch no.15 train no.51  loss = 0.6675077676773071\n",
      "epoch no.15 train no.61  loss = 0.48657098412513733\n",
      "epoch no.15 train no.71  loss = 0.5162253975868225\n",
      "epoch no.15 train no.81  loss = 0.5767713189125061\n",
      "epoch no.15 train no.91  loss = 0.33851543068885803\n",
      "epoch no.15 train no.101  loss = 0.7464062571525574\n",
      "epoch no.15 train no.111  loss = 0.5084690451622009\n",
      "epoch no.15 train no.121  loss = 1.057989478111267\n",
      "epoch no.15 train no.131  loss = 0.6430940628051758\n",
      "epoch no.15 train no.141  loss = 1.0828561782836914\n",
      "epoch no.15 train no.151  loss = 0.3479515016078949\n",
      "epoch no.15 train no.161  loss = 0.42970094084739685\n",
      "epoch no.15 train no.171  loss = 0.33058294653892517\n",
      "epoch no.15 train no.181  loss = 0.31199753284454346\n",
      "epoch no.15 train no.191  loss = 0.6699602603912354\n",
      "epoch no.15 train no.201  loss = 0.41812822222709656\n",
      "epoch no.15 train no.211  loss = 1.007330060005188\n",
      "epoch no.15 train no.221  loss = 1.0128087997436523\n",
      "epoch no.15 train no.231  loss = 0.5459026098251343\n",
      "epoch no.15 train no.241  loss = 0.5197942852973938\n",
      "epoch no.15 train no.251  loss = 0.7281959056854248\n",
      "epoch no.15 train no.261  loss = 0.16086171567440033\n",
      "epoch no.15 train no.271  loss = 0.5984228849411011\n",
      "epoch no.15 train no.281  loss = 0.3358823359012604\n",
      "epoch no.15 train no.291  loss = 0.2527720034122467\n",
      "epoch no.15 train no.301  loss = 0.33733299374580383\n",
      "epoch no.15 train no.311  loss = 0.35873591899871826\n",
      "epoch no.15 train no.321  loss = 0.3047911524772644\n",
      "epoch no.15 train no.331  loss = 0.7787531614303589\n",
      "epoch no.15 train no.341  loss = 0.4773455560207367\n",
      "epoch no.15 train no.351  loss = 0.6269576549530029\n",
      "epoch no.15 train no.361  loss = 0.31100934743881226\n",
      "epoch no.15 train no.371  loss = 0.565270721912384\n",
      "epoch no.15 train no.381  loss = 0.45368677377700806\n",
      "epoch no.15 train no.391  loss = 0.6660305261611938\n",
      "epoch no.15 train no.401  loss = 0.5161898136138916\n",
      "epoch no.15 train no.411  loss = 0.46893516182899475\n",
      "epoch no.15 train no.421  loss = 0.27570852637290955\n",
      "epoch no.15 train no.431  loss = 0.641046941280365\n",
      "epoch no.15 train no.441  loss = 0.2430141717195511\n",
      "epoch no.15 train no.451  loss = 0.37781593203544617\n",
      "epoch no.15 train no.461  loss = 0.3642660081386566\n",
      "epoch no.15 train no.471  loss = 0.32948750257492065\n",
      "epoch no.15 train no.481  loss = 0.3427133858203888\n",
      "epoch no.15 train no.491  loss = 0.8458895087242126\n",
      "epoch no.15 train no.501  loss = 0.8216748833656311\n",
      "epoch no.15 train no.511  loss = 0.6116832494735718\n",
      "epoch no.15 train no.521  loss = 0.5142354965209961\n",
      "epoch no.15 train no.531  loss = 1.2499769926071167\n",
      "epoch no.15 train no.541  loss = 0.2918227016925812\n",
      "epoch no.15 train no.551  loss = 0.9232879281044006\n",
      "epoch no.15 train no.561  loss = 0.8140527009963989\n",
      "epoch no.15 train no.571  loss = 0.3333926796913147\n",
      "epoch no.15 train no.581  loss = 0.38264626264572144\n",
      "epoch no.15 train no.591  loss = 0.366085410118103\n",
      "epoch no.15 train no.601  loss = 0.4923114776611328\n",
      "epoch no.15 train no.611  loss = 0.6954976320266724\n",
      "epoch no.15 train no.621  loss = 0.6240701079368591\n",
      "epoch no.15 train no.631  loss = 0.20125718414783478\n",
      "epoch no.15 train no.641  loss = 0.5514050722122192\n",
      "epoch no.15 train no.651  loss = 0.6382208466529846\n",
      "epoch no.15 train no.661  loss = 0.5849161744117737\n",
      "epoch no.15 train no.671  loss = 0.2585408091545105\n",
      "epoch no.15 train no.681  loss = 0.49767813086509705\n",
      "epoch no.15 train no.691  loss = 0.5435346961021423\n",
      "epoch no.15 train no.701  loss = 0.622790515422821\n",
      "epoch no.15 train no.711  loss = 0.5473898649215698\n",
      "epoch no.15 train no.721  loss = 0.741841197013855\n",
      "epoch no.15 train no.731  loss = 0.4367418587207794\n",
      "epoch no.15 train no.741  loss = 0.9181903600692749\n",
      "epoch no.15 train no.751  loss = 0.5994030833244324\n",
      "epoch no.15 train no.761  loss = 0.7894327044487\n",
      "epoch no.15 train no.771  loss = 0.5903103351593018\n",
      "epoch no.15 train no.781  loss = 0.42772120237350464\n",
      "epoch no.15 train no.791  loss = 0.5146265625953674\n",
      "epoch no.15 train no.801  loss = 0.43663376569747925\n",
      "epoch no.15 train no.811  loss = 0.5315672159194946\n",
      "epoch no.15 train no.821  loss = 0.328732430934906\n",
      "epoch no.15 train no.831  loss = 0.8222639560699463\n",
      "epoch no.15 train no.841  loss = 0.45872849225997925\n",
      "epoch no.15 train no.851  loss = 0.9310958981513977\n",
      "epoch no.15 train no.861  loss = 0.3904085159301758\n",
      "epoch no.15 train no.871  loss = 1.2196455001831055\n",
      "epoch no.15 train no.881  loss = 0.9693474173545837\n",
      "epoch no.15 train no.891  loss = 0.5534497499465942\n",
      "epoch no.15 train no.901  loss = 0.8910942077636719\n",
      "epoch no.15 train no.911  loss = 1.0542277097702026\n",
      "epoch no.15 train no.921  loss = 0.6363056898117065\n",
      "epoch no.15 train no.931  loss = 0.2972468435764313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.15 train no.941  loss = 0.3532019257545471\n",
      "epoch no.15 train no.951  loss = 0.8541954159736633\n",
      "epoch no.15 train no.961  loss = 0.43602442741394043\n",
      "epoch no.15 train no.971  loss = 0.9853440523147583\n",
      "epoch no.15 train no.981  loss = 0.4134882390499115\n",
      "epoch no.15 train no.991  loss = 0.401978462934494\n",
      "epoch no.15 train no.1001  loss = 1.520142674446106\n",
      "epoch no.16 train no.1  loss = 0.493407279253006\n",
      "epoch no.16 train no.11  loss = 0.6070377230644226\n",
      "epoch no.16 train no.21  loss = 0.39761874079704285\n",
      "epoch no.16 train no.31  loss = 0.5617303848266602\n",
      "epoch no.16 train no.41  loss = 0.27813851833343506\n",
      "epoch no.16 train no.51  loss = 0.6584776043891907\n",
      "epoch no.16 train no.61  loss = 0.556361973285675\n",
      "epoch no.16 train no.71  loss = 0.25069648027420044\n",
      "epoch no.16 train no.81  loss = 0.5180324912071228\n",
      "epoch no.16 train no.91  loss = 0.3306179940700531\n",
      "epoch no.16 train no.101  loss = 0.2680095434188843\n",
      "epoch no.16 train no.111  loss = 0.36591455340385437\n",
      "epoch no.16 train no.121  loss = 0.4259580969810486\n",
      "epoch no.16 train no.131  loss = 0.5610979199409485\n",
      "epoch no.16 train no.141  loss = 0.4396996796131134\n",
      "epoch no.16 train no.151  loss = 0.345839262008667\n",
      "epoch no.16 train no.161  loss = 0.20562024414539337\n",
      "epoch no.16 train no.171  loss = 0.5414509177207947\n",
      "epoch no.16 train no.181  loss = 0.23124836385250092\n",
      "epoch no.16 train no.191  loss = 0.3568471372127533\n",
      "epoch no.16 train no.201  loss = 0.5559080243110657\n",
      "epoch no.16 train no.211  loss = 0.7290445566177368\n",
      "epoch no.16 train no.221  loss = 0.7094199657440186\n",
      "epoch no.16 train no.231  loss = 0.9047760367393494\n",
      "epoch no.16 train no.241  loss = 0.3612860441207886\n",
      "epoch no.16 train no.251  loss = 0.2678568959236145\n",
      "epoch no.16 train no.261  loss = 0.5776142477989197\n",
      "epoch no.16 train no.271  loss = 0.666500985622406\n",
      "epoch no.16 train no.281  loss = 0.628977358341217\n",
      "epoch no.16 train no.291  loss = 0.8926712274551392\n",
      "epoch no.16 train no.301  loss = 0.29968032240867615\n",
      "epoch no.16 train no.311  loss = 0.3309091627597809\n",
      "epoch no.16 train no.321  loss = 0.7326066493988037\n",
      "epoch no.16 train no.331  loss = 0.43088972568511963\n",
      "epoch no.16 train no.341  loss = 0.6222155094146729\n",
      "epoch no.16 train no.351  loss = 0.44205740094184875\n",
      "epoch no.16 train no.361  loss = 0.4682852625846863\n",
      "epoch no.16 train no.371  loss = 0.6454989910125732\n",
      "epoch no.16 train no.381  loss = 0.5877179503440857\n",
      "epoch no.16 train no.391  loss = 0.7290740609169006\n",
      "epoch no.16 train no.401  loss = 0.19645962119102478\n",
      "epoch no.16 train no.411  loss = 1.050896406173706\n",
      "epoch no.16 train no.421  loss = 0.7584919929504395\n",
      "epoch no.16 train no.431  loss = 0.608654260635376\n",
      "epoch no.16 train no.441  loss = 0.5398953557014465\n",
      "epoch no.16 train no.451  loss = 1.1435788869857788\n",
      "epoch no.16 train no.461  loss = 0.7452906966209412\n",
      "epoch no.16 train no.471  loss = 0.5186335444450378\n",
      "epoch no.16 train no.481  loss = 0.8830554485321045\n",
      "epoch no.16 train no.491  loss = 0.5245798826217651\n",
      "epoch no.16 train no.501  loss = 0.38688263297080994\n",
      "epoch no.16 train no.511  loss = 0.497066468000412\n",
      "epoch no.16 train no.521  loss = 0.6430976986885071\n",
      "epoch no.16 train no.531  loss = 0.4925522804260254\n",
      "epoch no.16 train no.541  loss = 0.7100323438644409\n",
      "epoch no.16 train no.551  loss = 0.6043323278427124\n",
      "epoch no.16 train no.561  loss = 0.6140009164810181\n",
      "epoch no.16 train no.571  loss = 0.9189782738685608\n",
      "epoch no.16 train no.581  loss = 0.8192469477653503\n",
      "epoch no.16 train no.591  loss = 1.0429275035858154\n",
      "epoch no.16 train no.601  loss = 1.935240387916565\n",
      "epoch no.16 train no.611  loss = 0.3995145261287689\n",
      "epoch no.16 train no.621  loss = 0.2502968907356262\n",
      "epoch no.16 train no.631  loss = 0.17644046247005463\n",
      "epoch no.16 train no.641  loss = 0.872584342956543\n",
      "epoch no.16 train no.651  loss = 0.2026830017566681\n",
      "epoch no.16 train no.661  loss = 0.38915956020355225\n",
      "epoch no.16 train no.671  loss = 0.6159877181053162\n",
      "epoch no.16 train no.681  loss = 0.8615301251411438\n",
      "epoch no.16 train no.691  loss = 0.7721788883209229\n",
      "epoch no.16 train no.701  loss = 0.36559244990348816\n",
      "epoch no.16 train no.711  loss = 0.4159323573112488\n",
      "epoch no.16 train no.721  loss = 0.31662192940711975\n",
      "epoch no.16 train no.731  loss = 0.7513883113861084\n",
      "epoch no.16 train no.741  loss = 0.4542936384677887\n",
      "epoch no.16 train no.751  loss = 0.7363550662994385\n",
      "epoch no.16 train no.761  loss = 0.5198209881782532\n",
      "epoch no.16 train no.771  loss = 0.4002443552017212\n",
      "epoch no.16 train no.781  loss = 0.5236464738845825\n",
      "epoch no.16 train no.791  loss = 0.4084378480911255\n",
      "epoch no.16 train no.801  loss = 0.576813817024231\n",
      "epoch no.16 train no.811  loss = 0.8371443152427673\n",
      "epoch no.16 train no.821  loss = 0.3630808889865875\n",
      "epoch no.16 train no.831  loss = 1.0609865188598633\n",
      "epoch no.16 train no.841  loss = 0.5121117830276489\n",
      "epoch no.16 train no.851  loss = 0.28179243206977844\n",
      "epoch no.16 train no.861  loss = 1.1963781118392944\n",
      "epoch no.16 train no.871  loss = 0.2974729835987091\n",
      "epoch no.16 train no.881  loss = 0.5786727666854858\n",
      "epoch no.16 train no.891  loss = 0.7193634510040283\n",
      "epoch no.16 train no.901  loss = 0.3043941259384155\n",
      "epoch no.16 train no.911  loss = 0.4279017746448517\n",
      "epoch no.16 train no.921  loss = 0.41220569610595703\n",
      "epoch no.16 train no.931  loss = 0.6420078873634338\n",
      "epoch no.16 train no.941  loss = 0.4668373167514801\n",
      "epoch no.16 train no.951  loss = 0.36942002177238464\n",
      "epoch no.16 train no.961  loss = 0.4133951961994171\n",
      "epoch no.16 train no.971  loss = 0.46165451407432556\n",
      "epoch no.16 train no.981  loss = 0.7083293795585632\n",
      "epoch no.16 train no.991  loss = 0.636877715587616\n",
      "epoch no.16 train no.1001  loss = 0.6747507452964783\n",
      "epoch no.17 train no.1  loss = 1.0799933671951294\n",
      "epoch no.17 train no.11  loss = 0.5886062383651733\n",
      "epoch no.17 train no.21  loss = 0.49544766545295715\n",
      "epoch no.17 train no.31  loss = 0.52753084897995\n",
      "epoch no.17 train no.41  loss = 0.9175182580947876\n",
      "epoch no.17 train no.51  loss = 0.26604950428009033\n",
      "epoch no.17 train no.61  loss = 0.8041842579841614\n",
      "epoch no.17 train no.71  loss = 0.6356798410415649\n",
      "epoch no.17 train no.81  loss = 0.24050773680210114\n",
      "epoch no.17 train no.91  loss = 1.5155357122421265\n",
      "epoch no.17 train no.101  loss = 0.8028698563575745\n",
      "epoch no.17 train no.111  loss = 0.5868425369262695\n",
      "epoch no.17 train no.121  loss = 0.41968539357185364\n",
      "epoch no.17 train no.131  loss = 0.5241258144378662\n",
      "epoch no.17 train no.141  loss = 1.2320598363876343\n",
      "epoch no.17 train no.151  loss = 0.6053896546363831\n",
      "epoch no.17 train no.161  loss = 0.9976822733879089\n",
      "epoch no.17 train no.171  loss = 0.7892877459526062\n",
      "epoch no.17 train no.181  loss = 1.6245523691177368\n",
      "epoch no.17 train no.191  loss = 0.4580267369747162\n",
      "epoch no.17 train no.201  loss = 0.2767283320426941\n",
      "epoch no.17 train no.211  loss = 0.4308706521987915\n",
      "epoch no.17 train no.221  loss = 0.563484251499176\n",
      "epoch no.17 train no.231  loss = 0.5134972333908081\n",
      "epoch no.17 train no.241  loss = 0.5750459432601929\n",
      "epoch no.17 train no.251  loss = 0.25711140036582947\n",
      "epoch no.17 train no.261  loss = 0.3395296335220337\n",
      "epoch no.17 train no.271  loss = 0.3978777229785919\n",
      "epoch no.17 train no.281  loss = 0.3246782720088959\n",
      "epoch no.17 train no.291  loss = 0.9854785203933716\n",
      "epoch no.17 train no.301  loss = 0.33936217427253723\n",
      "epoch no.17 train no.311  loss = 0.528219997882843\n",
      "epoch no.17 train no.321  loss = 0.6379072070121765\n",
      "epoch no.17 train no.331  loss = 0.5112902522087097\n",
      "epoch no.17 train no.341  loss = 1.0991326570510864\n",
      "epoch no.17 train no.351  loss = 0.23045146465301514\n",
      "epoch no.17 train no.361  loss = 0.8429268002510071\n",
      "epoch no.17 train no.371  loss = 0.5430652499198914\n",
      "epoch no.17 train no.381  loss = 0.17696887254714966\n",
      "epoch no.17 train no.391  loss = 0.7049437165260315\n",
      "epoch no.17 train no.401  loss = 0.7716554403305054\n",
      "epoch no.17 train no.411  loss = 0.32668015360832214\n",
      "epoch no.17 train no.421  loss = 0.5991578698158264\n",
      "epoch no.17 train no.431  loss = 0.5907984972000122\n",
      "epoch no.17 train no.441  loss = 0.4749129116535187\n",
      "epoch no.17 train no.451  loss = 0.446594774723053\n",
      "epoch no.17 train no.461  loss = 0.5713357329368591\n",
      "epoch no.17 train no.471  loss = 0.44595035910606384\n",
      "epoch no.17 train no.481  loss = 0.37688368558883667\n",
      "epoch no.17 train no.491  loss = 0.47998982667922974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.17 train no.501  loss = 0.39899781346321106\n",
      "epoch no.17 train no.511  loss = 0.5302581787109375\n",
      "epoch no.17 train no.521  loss = 0.4185938537120819\n",
      "epoch no.17 train no.531  loss = 0.17440131306648254\n",
      "epoch no.17 train no.541  loss = 0.36800214648246765\n",
      "epoch no.17 train no.551  loss = 1.071103572845459\n",
      "epoch no.17 train no.561  loss = 1.3685632944107056\n",
      "epoch no.17 train no.571  loss = 0.6611344814300537\n",
      "epoch no.17 train no.581  loss = 0.6004503965377808\n",
      "epoch no.17 train no.591  loss = 0.44543471932411194\n",
      "epoch no.17 train no.601  loss = 0.8285259008407593\n",
      "epoch no.17 train no.611  loss = 0.8276584148406982\n",
      "epoch no.17 train no.621  loss = 1.4608986377716064\n",
      "epoch no.17 train no.631  loss = 0.5068823099136353\n",
      "epoch no.17 train no.641  loss = 0.7468194961547852\n",
      "epoch no.17 train no.651  loss = 0.5344914793968201\n",
      "epoch no.17 train no.661  loss = 1.0616170167922974\n",
      "epoch no.17 train no.671  loss = 0.2487785816192627\n",
      "epoch no.17 train no.681  loss = 0.43428266048431396\n",
      "epoch no.17 train no.691  loss = 0.4743911623954773\n",
      "epoch no.17 train no.701  loss = 0.41755908727645874\n",
      "epoch no.17 train no.711  loss = 0.666755199432373\n",
      "epoch no.17 train no.721  loss = 0.31698644161224365\n",
      "epoch no.17 train no.731  loss = 0.33193305134773254\n",
      "epoch no.17 train no.741  loss = 0.3940190374851227\n",
      "epoch no.17 train no.751  loss = 0.5757352709770203\n",
      "epoch no.17 train no.761  loss = 0.45369523763656616\n",
      "epoch no.17 train no.771  loss = 1.3865129947662354\n",
      "epoch no.17 train no.781  loss = 0.2718600034713745\n",
      "epoch no.17 train no.791  loss = 0.42385321855545044\n",
      "epoch no.17 train no.801  loss = 0.47046300768852234\n",
      "epoch no.17 train no.811  loss = 0.5306044220924377\n",
      "epoch no.17 train no.821  loss = 0.32767122983932495\n",
      "epoch no.17 train no.831  loss = 0.8617224097251892\n",
      "epoch no.17 train no.841  loss = 0.7162675261497498\n",
      "epoch no.17 train no.851  loss = 0.3244621455669403\n",
      "epoch no.17 train no.861  loss = 0.658606767654419\n",
      "epoch no.17 train no.871  loss = 0.509391725063324\n",
      "epoch no.17 train no.881  loss = 0.5141358971595764\n",
      "epoch no.17 train no.891  loss = 0.9668958783149719\n",
      "epoch no.17 train no.901  loss = 0.3956165015697479\n",
      "epoch no.17 train no.911  loss = 1.2406798601150513\n",
      "epoch no.17 train no.921  loss = 0.3446425199508667\n",
      "epoch no.17 train no.931  loss = 0.25028929114341736\n",
      "epoch no.17 train no.941  loss = 0.23143427073955536\n",
      "epoch no.17 train no.951  loss = 0.26017042994499207\n",
      "epoch no.17 train no.961  loss = 0.5051484107971191\n",
      "epoch no.17 train no.971  loss = 0.32580292224884033\n",
      "epoch no.17 train no.981  loss = 0.38785237073898315\n",
      "epoch no.17 train no.991  loss = 0.6196718215942383\n",
      "epoch no.17 train no.1001  loss = 0.3214434087276459\n",
      "epoch no.18 train no.1  loss = 0.6595791578292847\n",
      "epoch no.18 train no.11  loss = 0.3139098882675171\n",
      "epoch no.18 train no.21  loss = 0.33838367462158203\n",
      "epoch no.18 train no.31  loss = 0.5184483528137207\n",
      "epoch no.18 train no.41  loss = 0.34689486026763916\n",
      "epoch no.18 train no.51  loss = 0.684228241443634\n",
      "epoch no.18 train no.61  loss = 0.2865554690361023\n",
      "epoch no.18 train no.71  loss = 0.32342255115509033\n",
      "epoch no.18 train no.81  loss = 0.360617071390152\n",
      "epoch no.18 train no.91  loss = 0.554045557975769\n",
      "epoch no.18 train no.101  loss = 0.5143398642539978\n",
      "epoch no.18 train no.111  loss = 0.6291431188583374\n",
      "epoch no.18 train no.121  loss = 0.24932162463665009\n",
      "epoch no.18 train no.131  loss = 0.33631402254104614\n",
      "epoch no.18 train no.141  loss = 0.7949091196060181\n",
      "epoch no.18 train no.151  loss = 0.7722934484481812\n",
      "epoch no.18 train no.161  loss = 1.1081448793411255\n",
      "epoch no.18 train no.171  loss = 0.35462045669555664\n",
      "epoch no.18 train no.181  loss = 0.3466309607028961\n",
      "epoch no.18 train no.191  loss = 0.33899959921836853\n",
      "epoch no.18 train no.201  loss = 0.2535075843334198\n",
      "epoch no.18 train no.211  loss = 0.7379285097122192\n",
      "epoch no.18 train no.221  loss = 0.3540128171443939\n",
      "epoch no.18 train no.231  loss = 0.8419859409332275\n",
      "epoch no.18 train no.241  loss = 0.37167978286743164\n",
      "epoch no.18 train no.251  loss = 0.7692393064498901\n",
      "epoch no.18 train no.261  loss = 0.6286813020706177\n",
      "epoch no.18 train no.271  loss = 0.4186552166938782\n",
      "epoch no.18 train no.281  loss = 0.42816463112831116\n",
      "epoch no.18 train no.291  loss = 0.3007095456123352\n",
      "epoch no.18 train no.301  loss = 0.8893619179725647\n",
      "epoch no.18 train no.311  loss = 0.6769569516181946\n",
      "epoch no.18 train no.321  loss = 0.30399081110954285\n",
      "epoch no.18 train no.331  loss = 0.5828561186790466\n",
      "epoch no.18 train no.341  loss = 0.47319263219833374\n",
      "epoch no.18 train no.351  loss = 0.8615574240684509\n",
      "epoch no.18 train no.361  loss = 0.3283010721206665\n",
      "epoch no.18 train no.371  loss = 1.0243769884109497\n",
      "epoch no.18 train no.381  loss = 0.5924745202064514\n",
      "epoch no.18 train no.391  loss = 0.6860600709915161\n",
      "epoch no.18 train no.401  loss = 0.7376344203948975\n",
      "epoch no.18 train no.411  loss = 0.2552248239517212\n",
      "epoch no.18 train no.421  loss = 1.2495942115783691\n",
      "epoch no.18 train no.431  loss = 0.6909288167953491\n",
      "epoch no.18 train no.441  loss = 0.4230108857154846\n",
      "epoch no.18 train no.451  loss = 0.20713405311107635\n",
      "epoch no.18 train no.461  loss = 0.6263980269432068\n",
      "epoch no.18 train no.471  loss = 0.572449266910553\n",
      "epoch no.18 train no.481  loss = 0.5396841168403625\n",
      "epoch no.18 train no.491  loss = 0.4055805802345276\n",
      "epoch no.18 train no.501  loss = 0.6431031227111816\n",
      "epoch no.18 train no.511  loss = 0.4441570043563843\n",
      "epoch no.18 train no.521  loss = 0.7163078784942627\n",
      "epoch no.18 train no.531  loss = 0.39659470319747925\n",
      "epoch no.18 train no.541  loss = 0.3359028697013855\n",
      "epoch no.18 train no.551  loss = 0.33077549934387207\n",
      "epoch no.18 train no.561  loss = 0.42455175518989563\n",
      "epoch no.18 train no.571  loss = 0.809264063835144\n",
      "epoch no.18 train no.581  loss = 0.40603864192962646\n",
      "epoch no.18 train no.591  loss = 0.5170891880989075\n",
      "epoch no.18 train no.601  loss = 0.5610119700431824\n",
      "epoch no.18 train no.611  loss = 0.34872758388519287\n",
      "epoch no.18 train no.621  loss = 0.7612456679344177\n",
      "epoch no.18 train no.631  loss = 0.9352008104324341\n",
      "epoch no.18 train no.641  loss = 0.48334890604019165\n",
      "epoch no.18 train no.651  loss = 0.6136811375617981\n",
      "epoch no.18 train no.661  loss = 0.8648986220359802\n",
      "epoch no.18 train no.671  loss = 0.8491126298904419\n",
      "epoch no.18 train no.681  loss = 0.17312189936637878\n",
      "epoch no.18 train no.691  loss = 0.3181123733520508\n",
      "epoch no.18 train no.701  loss = 0.6324341893196106\n",
      "epoch no.18 train no.711  loss = 0.6046559810638428\n",
      "epoch no.18 train no.721  loss = 1.2243953943252563\n",
      "epoch no.18 train no.731  loss = 0.44963139295578003\n",
      "epoch no.18 train no.741  loss = 0.3041847050189972\n",
      "epoch no.18 train no.751  loss = 0.517861545085907\n",
      "epoch no.18 train no.761  loss = 0.4028739035129547\n",
      "epoch no.18 train no.771  loss = 0.8459497690200806\n",
      "epoch no.18 train no.781  loss = 0.5441898703575134\n",
      "epoch no.18 train no.791  loss = 0.19500266015529633\n",
      "epoch no.18 train no.801  loss = 0.2824474275112152\n",
      "epoch no.18 train no.811  loss = 0.6054925322532654\n",
      "epoch no.18 train no.821  loss = 0.599734365940094\n",
      "epoch no.18 train no.831  loss = 0.45545658469200134\n",
      "epoch no.18 train no.841  loss = 0.31851261854171753\n",
      "epoch no.18 train no.851  loss = 0.7121118903160095\n",
      "epoch no.18 train no.861  loss = 0.3352309763431549\n",
      "epoch no.18 train no.871  loss = 0.7808853983879089\n",
      "epoch no.18 train no.881  loss = 0.7686583995819092\n",
      "epoch no.18 train no.891  loss = 0.3044902980327606\n",
      "epoch no.18 train no.901  loss = 0.30115917325019836\n",
      "epoch no.18 train no.911  loss = 0.7161856293678284\n",
      "epoch no.18 train no.921  loss = 0.6302192807197571\n",
      "epoch no.18 train no.931  loss = 1.2487424612045288\n",
      "epoch no.18 train no.941  loss = 0.6253394484519958\n",
      "epoch no.18 train no.951  loss = 0.16422300040721893\n",
      "epoch no.18 train no.961  loss = 0.2970435917377472\n",
      "epoch no.18 train no.971  loss = 1.509990930557251\n",
      "epoch no.18 train no.981  loss = 0.38739779591560364\n",
      "epoch no.18 train no.991  loss = 0.31705713272094727\n",
      "epoch no.18 train no.1001  loss = 0.39821964502334595\n",
      "epoch no.19 train no.1  loss = 0.20529773831367493\n",
      "epoch no.19 train no.11  loss = 1.3336217403411865\n",
      "epoch no.19 train no.21  loss = 0.6797081232070923\n",
      "epoch no.19 train no.31  loss = 0.6715742945671082\n",
      "epoch no.19 train no.41  loss = 0.2523547112941742\n",
      "epoch no.19 train no.51  loss = 0.5266350507736206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.19 train no.61  loss = 0.539119303226471\n",
      "epoch no.19 train no.71  loss = 0.189955472946167\n",
      "epoch no.19 train no.81  loss = 0.41102102398872375\n",
      "epoch no.19 train no.91  loss = 0.48336148262023926\n",
      "epoch no.19 train no.101  loss = 0.3603709638118744\n",
      "epoch no.19 train no.111  loss = 0.47132936120033264\n",
      "epoch no.19 train no.121  loss = 0.7802637815475464\n",
      "epoch no.19 train no.131  loss = 0.6986766457557678\n",
      "epoch no.19 train no.141  loss = 0.2806815207004547\n",
      "epoch no.19 train no.151  loss = 0.4574946165084839\n",
      "epoch no.19 train no.161  loss = 0.4057663381099701\n",
      "epoch no.19 train no.171  loss = 0.2087288200855255\n",
      "epoch no.19 train no.181  loss = 0.2719072103500366\n",
      "epoch no.19 train no.191  loss = 0.5131551623344421\n",
      "epoch no.19 train no.201  loss = 0.7641559839248657\n",
      "epoch no.19 train no.211  loss = 0.9308768510818481\n",
      "epoch no.19 train no.221  loss = 0.8149693608283997\n",
      "epoch no.19 train no.231  loss = 0.9192607402801514\n",
      "epoch no.19 train no.241  loss = 0.6664496660232544\n",
      "epoch no.19 train no.251  loss = 0.48032334446907043\n",
      "epoch no.19 train no.261  loss = 0.46360111236572266\n",
      "epoch no.19 train no.271  loss = 0.44914400577545166\n",
      "epoch no.19 train no.281  loss = 0.831602156162262\n",
      "epoch no.19 train no.291  loss = 0.40336450934410095\n",
      "epoch no.19 train no.301  loss = 0.8067148923873901\n",
      "epoch no.19 train no.311  loss = 0.9289984107017517\n",
      "epoch no.19 train no.321  loss = 0.33329808712005615\n",
      "epoch no.19 train no.331  loss = 0.6107944250106812\n",
      "epoch no.19 train no.341  loss = 0.37194445729255676\n",
      "epoch no.19 train no.351  loss = 0.28374215960502625\n",
      "epoch no.19 train no.361  loss = 0.6784151196479797\n",
      "epoch no.19 train no.371  loss = 0.48063820600509644\n",
      "epoch no.19 train no.381  loss = 0.8105663657188416\n",
      "epoch no.19 train no.391  loss = 0.18633966147899628\n",
      "epoch no.19 train no.401  loss = 0.6510465741157532\n",
      "epoch no.19 train no.411  loss = 0.5274524092674255\n",
      "epoch no.19 train no.421  loss = 0.44171494245529175\n",
      "epoch no.19 train no.431  loss = 0.4741377830505371\n",
      "epoch no.19 train no.441  loss = 0.8256515264511108\n",
      "epoch no.19 train no.451  loss = 0.6699782609939575\n",
      "epoch no.19 train no.461  loss = 0.6861157417297363\n",
      "epoch no.19 train no.471  loss = 0.34574392437934875\n",
      "epoch no.19 train no.481  loss = 0.522997260093689\n",
      "epoch no.19 train no.491  loss = 0.3221680223941803\n",
      "epoch no.19 train no.501  loss = 0.535714864730835\n",
      "epoch no.19 train no.511  loss = 0.6654361486434937\n",
      "epoch no.19 train no.521  loss = 0.23326857388019562\n",
      "epoch no.19 train no.531  loss = 0.45599064230918884\n",
      "epoch no.19 train no.541  loss = 0.8322789072990417\n",
      "epoch no.19 train no.551  loss = 0.3343197703361511\n",
      "epoch no.19 train no.561  loss = 0.6155064105987549\n",
      "epoch no.19 train no.571  loss = 0.24754679203033447\n",
      "epoch no.19 train no.581  loss = 0.39975571632385254\n",
      "epoch no.19 train no.591  loss = 0.6370669007301331\n",
      "epoch no.19 train no.601  loss = 0.899207353591919\n",
      "epoch no.19 train no.611  loss = 0.4353564977645874\n",
      "epoch no.19 train no.621  loss = 0.23601026833057404\n",
      "epoch no.19 train no.631  loss = 0.3440282940864563\n",
      "epoch no.19 train no.641  loss = 0.8631728887557983\n",
      "epoch no.19 train no.651  loss = 0.451925665140152\n",
      "epoch no.19 train no.661  loss = 0.8911210894584656\n",
      "epoch no.19 train no.671  loss = 0.3794972598552704\n",
      "epoch no.19 train no.681  loss = 0.6059718132019043\n",
      "epoch no.19 train no.691  loss = 0.19721288979053497\n",
      "epoch no.19 train no.701  loss = 0.6907560229301453\n",
      "epoch no.19 train no.711  loss = 0.44825220108032227\n",
      "epoch no.19 train no.721  loss = 0.7140588760375977\n",
      "epoch no.19 train no.731  loss = 0.6331032514572144\n",
      "epoch no.19 train no.741  loss = 1.3021461963653564\n",
      "epoch no.19 train no.751  loss = 1.062606692314148\n",
      "epoch no.19 train no.761  loss = 0.6971985697746277\n",
      "epoch no.19 train no.771  loss = 0.6046334505081177\n",
      "epoch no.19 train no.781  loss = 0.674409806728363\n",
      "epoch no.19 train no.791  loss = 0.37524157762527466\n",
      "epoch no.19 train no.801  loss = 0.5736430287361145\n",
      "epoch no.19 train no.811  loss = 0.6993269324302673\n",
      "epoch no.19 train no.821  loss = 0.16614864766597748\n",
      "epoch no.19 train no.831  loss = 0.532055675983429\n",
      "epoch no.19 train no.841  loss = 0.43948185443878174\n",
      "epoch no.19 train no.851  loss = 0.38543614745140076\n",
      "epoch no.19 train no.861  loss = 0.32624849677085876\n",
      "epoch no.19 train no.871  loss = 0.4924670159816742\n",
      "epoch no.19 train no.881  loss = 0.35698145627975464\n",
      "epoch no.19 train no.891  loss = 0.6899527907371521\n",
      "epoch no.19 train no.901  loss = 0.5324479937553406\n",
      "epoch no.19 train no.911  loss = 0.4095008373260498\n",
      "epoch no.19 train no.921  loss = 0.720413863658905\n",
      "epoch no.19 train no.931  loss = 0.6381017565727234\n",
      "epoch no.19 train no.941  loss = 0.4759290814399719\n",
      "epoch no.19 train no.951  loss = 0.6174925565719604\n",
      "epoch no.19 train no.961  loss = 0.35862115025520325\n",
      "epoch no.19 train no.971  loss = 0.7260864973068237\n",
      "epoch no.19 train no.981  loss = 0.68043452501297\n",
      "epoch no.19 train no.991  loss = 0.273139089345932\n",
      "epoch no.19 train no.1001  loss = 0.8430164456367493\n",
      "epoch no.20 train no.1  loss = 0.2884354889392853\n",
      "epoch no.20 train no.11  loss = 0.7942782640457153\n",
      "epoch no.20 train no.21  loss = 0.16140642762184143\n",
      "epoch no.20 train no.31  loss = 0.3013678193092346\n",
      "epoch no.20 train no.41  loss = 0.6199888586997986\n",
      "epoch no.20 train no.51  loss = 0.4958437383174896\n",
      "epoch no.20 train no.61  loss = 0.2959659993648529\n",
      "epoch no.20 train no.71  loss = 0.6756883859634399\n",
      "epoch no.20 train no.81  loss = 0.2917117476463318\n",
      "epoch no.20 train no.91  loss = 0.7781686782836914\n",
      "epoch no.20 train no.101  loss = 0.6880956292152405\n",
      "epoch no.20 train no.111  loss = 0.28154346346855164\n",
      "epoch no.20 train no.121  loss = 0.6045569181442261\n",
      "epoch no.20 train no.131  loss = 0.18039920926094055\n",
      "epoch no.20 train no.141  loss = 0.3521445691585541\n",
      "epoch no.20 train no.151  loss = 0.6609712243080139\n",
      "epoch no.20 train no.161  loss = 0.4877496361732483\n",
      "epoch no.20 train no.171  loss = 0.28055280447006226\n",
      "epoch no.20 train no.181  loss = 0.4473859667778015\n",
      "epoch no.20 train no.191  loss = 0.904850423336029\n",
      "epoch no.20 train no.201  loss = 0.7589053511619568\n",
      "epoch no.20 train no.211  loss = 0.23163223266601562\n",
      "epoch no.20 train no.221  loss = 0.5740448832511902\n",
      "epoch no.20 train no.231  loss = 0.41504067182540894\n",
      "epoch no.20 train no.241  loss = 0.8910716772079468\n",
      "epoch no.20 train no.251  loss = 0.5982053279876709\n",
      "epoch no.20 train no.261  loss = 0.4042109549045563\n",
      "epoch no.20 train no.271  loss = 0.641879677772522\n",
      "epoch no.20 train no.281  loss = 0.7295466065406799\n",
      "epoch no.20 train no.291  loss = 0.5298280119895935\n",
      "epoch no.20 train no.301  loss = 0.4083278179168701\n",
      "epoch no.20 train no.311  loss = 0.8387057781219482\n",
      "epoch no.20 train no.321  loss = 0.3198377788066864\n",
      "epoch no.20 train no.331  loss = 0.3027414381504059\n",
      "epoch no.20 train no.341  loss = 0.3926064074039459\n",
      "epoch no.20 train no.351  loss = 0.7820261120796204\n",
      "epoch no.20 train no.361  loss = 0.5947070121765137\n",
      "epoch no.20 train no.371  loss = 0.17444534599781036\n",
      "epoch no.20 train no.381  loss = 0.2994001805782318\n",
      "epoch no.20 train no.391  loss = 0.6387823224067688\n",
      "epoch no.20 train no.401  loss = 0.7030360698699951\n",
      "epoch no.20 train no.411  loss = 0.2796878218650818\n",
      "epoch no.20 train no.421  loss = 0.5449153780937195\n",
      "epoch no.20 train no.431  loss = 0.6660983562469482\n",
      "epoch no.20 train no.441  loss = 0.3044297397136688\n",
      "epoch no.20 train no.451  loss = 0.4320533573627472\n",
      "epoch no.20 train no.461  loss = 0.5255460143089294\n",
      "epoch no.20 train no.471  loss = 0.3572371304035187\n",
      "epoch no.20 train no.481  loss = 0.8393505811691284\n",
      "epoch no.20 train no.491  loss = 0.474610835313797\n",
      "epoch no.20 train no.501  loss = 0.4792373478412628\n",
      "epoch no.20 train no.511  loss = 0.8838921785354614\n",
      "epoch no.20 train no.521  loss = 0.6061474084854126\n",
      "epoch no.20 train no.531  loss = 0.21454662084579468\n",
      "epoch no.20 train no.541  loss = 0.5852793455123901\n",
      "epoch no.20 train no.551  loss = 0.5910206437110901\n",
      "epoch no.20 train no.561  loss = 0.5222851037979126\n",
      "epoch no.20 train no.571  loss = 0.42826661467552185\n",
      "epoch no.20 train no.581  loss = 0.24902714788913727\n",
      "epoch no.20 train no.591  loss = 0.6718975901603699\n",
      "epoch no.20 train no.601  loss = 0.24767059087753296\n",
      "epoch no.20 train no.611  loss = 0.7351466417312622\n",
      "epoch no.20 train no.621  loss = 0.3085959851741791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.20 train no.631  loss = 0.8847419023513794\n",
      "epoch no.20 train no.641  loss = 0.638438880443573\n",
      "epoch no.20 train no.651  loss = 0.24403119087219238\n",
      "epoch no.20 train no.661  loss = 0.47111859917640686\n",
      "epoch no.20 train no.671  loss = 0.7144055366516113\n",
      "epoch no.20 train no.681  loss = 0.325970858335495\n",
      "epoch no.20 train no.691  loss = 0.3282069265842438\n",
      "epoch no.20 train no.701  loss = 0.44913971424102783\n",
      "epoch no.20 train no.711  loss = 0.24055440723896027\n",
      "epoch no.20 train no.721  loss = 0.2909306287765503\n",
      "epoch no.20 train no.731  loss = 0.5017410516738892\n",
      "epoch no.20 train no.741  loss = 1.2333863973617554\n",
      "epoch no.20 train no.751  loss = 0.5275774598121643\n",
      "epoch no.20 train no.761  loss = 0.8548761606216431\n",
      "epoch no.20 train no.771  loss = 0.6516746878623962\n",
      "epoch no.20 train no.781  loss = 1.707770824432373\n",
      "epoch no.20 train no.791  loss = 0.49693167209625244\n",
      "epoch no.20 train no.801  loss = 0.28170472383499146\n",
      "epoch no.20 train no.811  loss = 1.4565341472625732\n",
      "epoch no.20 train no.821  loss = 0.6943997740745544\n",
      "epoch no.20 train no.831  loss = 0.5094683766365051\n",
      "epoch no.20 train no.841  loss = 0.45452794432640076\n",
      "epoch no.20 train no.851  loss = 0.27830594778060913\n",
      "epoch no.20 train no.861  loss = 0.31563282012939453\n",
      "epoch no.20 train no.871  loss = 0.8141368627548218\n",
      "epoch no.20 train no.881  loss = 1.240997076034546\n",
      "epoch no.20 train no.891  loss = 0.44373437762260437\n",
      "epoch no.20 train no.901  loss = 0.3679053485393524\n",
      "epoch no.20 train no.911  loss = 0.6674895882606506\n",
      "epoch no.20 train no.921  loss = 0.7621275186538696\n",
      "epoch no.20 train no.931  loss = 0.4015548527240753\n",
      "epoch no.20 train no.941  loss = 0.7483848333358765\n",
      "epoch no.20 train no.951  loss = 0.43937966227531433\n",
      "epoch no.20 train no.961  loss = 0.45385709404945374\n",
      "epoch no.20 train no.971  loss = 0.6149987578392029\n",
      "epoch no.20 train no.981  loss = 1.3004380464553833\n",
      "epoch no.20 train no.991  loss = 0.5960991382598877\n",
      "epoch no.20 train no.1001  loss = 0.3520142138004303\n",
      "epoch no.21 train no.1  loss = 0.2914563715457916\n",
      "epoch no.21 train no.11  loss = 0.3802907466888428\n",
      "epoch no.21 train no.21  loss = 0.617561399936676\n",
      "epoch no.21 train no.31  loss = 0.9380912780761719\n",
      "epoch no.21 train no.41  loss = 0.36651724576950073\n",
      "epoch no.21 train no.51  loss = 0.38404184579849243\n",
      "epoch no.21 train no.61  loss = 1.2376129627227783\n",
      "epoch no.21 train no.71  loss = 0.5656518340110779\n",
      "epoch no.21 train no.81  loss = 0.4662676453590393\n",
      "epoch no.21 train no.91  loss = 0.3384930491447449\n",
      "epoch no.21 train no.101  loss = 0.7912681102752686\n",
      "epoch no.21 train no.111  loss = 0.6127646565437317\n",
      "epoch no.21 train no.121  loss = 0.7679262757301331\n",
      "epoch no.21 train no.131  loss = 0.3421167731285095\n",
      "epoch no.21 train no.141  loss = 0.661291241645813\n",
      "epoch no.21 train no.151  loss = 0.6379572749137878\n",
      "epoch no.21 train no.161  loss = 0.48412349820137024\n",
      "epoch no.21 train no.171  loss = 0.2820129692554474\n",
      "epoch no.21 train no.181  loss = 0.7215842008590698\n",
      "epoch no.21 train no.191  loss = 0.29548391699790955\n",
      "epoch no.21 train no.201  loss = 0.3995990753173828\n",
      "epoch no.21 train no.211  loss = 0.3339942693710327\n",
      "epoch no.21 train no.221  loss = 0.4483671188354492\n",
      "epoch no.21 train no.231  loss = 0.44315212965011597\n",
      "epoch no.21 train no.241  loss = 0.2327812910079956\n",
      "epoch no.21 train no.251  loss = 0.28159865736961365\n",
      "epoch no.21 train no.261  loss = 0.5929644703865051\n",
      "epoch no.21 train no.271  loss = 0.8850164413452148\n",
      "epoch no.21 train no.281  loss = 0.29415518045425415\n",
      "epoch no.21 train no.291  loss = 0.28230196237564087\n",
      "epoch no.21 train no.301  loss = 0.5225136876106262\n",
      "epoch no.21 train no.311  loss = 0.637927234172821\n",
      "epoch no.21 train no.321  loss = 0.7391098737716675\n",
      "epoch no.21 train no.331  loss = 0.38526618480682373\n",
      "epoch no.21 train no.341  loss = 0.2561659812927246\n",
      "epoch no.21 train no.351  loss = 0.4812990427017212\n",
      "epoch no.21 train no.361  loss = 0.4102853834629059\n",
      "epoch no.21 train no.371  loss = 0.6901413202285767\n",
      "epoch no.21 train no.381  loss = 0.35397329926490784\n",
      "epoch no.21 train no.391  loss = 0.33854296803474426\n",
      "epoch no.21 train no.401  loss = 0.27446311712265015\n",
      "epoch no.21 train no.411  loss = 0.41670215129852295\n",
      "epoch no.21 train no.421  loss = 0.47417500615119934\n",
      "epoch no.21 train no.431  loss = 0.3127027451992035\n",
      "epoch no.21 train no.441  loss = 0.22424623370170593\n",
      "epoch no.21 train no.451  loss = 0.5615513920783997\n",
      "epoch no.21 train no.461  loss = 0.2941383719444275\n",
      "epoch no.21 train no.471  loss = 1.0391253232955933\n",
      "epoch no.21 train no.481  loss = 0.44223079085350037\n",
      "epoch no.21 train no.491  loss = 0.7535776495933533\n",
      "epoch no.21 train no.501  loss = 0.8363444209098816\n",
      "epoch no.21 train no.511  loss = 0.36558395624160767\n",
      "epoch no.21 train no.521  loss = 0.9037255644798279\n",
      "epoch no.21 train no.531  loss = 0.8471027612686157\n",
      "epoch no.21 train no.541  loss = 0.8654567003250122\n",
      "epoch no.21 train no.551  loss = 0.44861096143722534\n",
      "epoch no.21 train no.561  loss = 0.388054758310318\n",
      "epoch no.21 train no.571  loss = 0.8361352682113647\n",
      "epoch no.21 train no.581  loss = 0.5746148228645325\n",
      "epoch no.21 train no.591  loss = 0.5549154877662659\n",
      "epoch no.21 train no.601  loss = 0.4177170991897583\n",
      "epoch no.21 train no.611  loss = 0.5159302353858948\n",
      "epoch no.21 train no.621  loss = 0.5428758859634399\n",
      "epoch no.21 train no.631  loss = 0.5027293562889099\n",
      "epoch no.21 train no.641  loss = 1.0621299743652344\n",
      "epoch no.21 train no.651  loss = 0.675388514995575\n",
      "epoch no.21 train no.661  loss = 0.8576937913894653\n",
      "epoch no.21 train no.671  loss = 0.3450765609741211\n",
      "epoch no.21 train no.681  loss = 1.2082005739212036\n",
      "epoch no.21 train no.691  loss = 0.8234893083572388\n",
      "epoch no.21 train no.701  loss = 0.37528082728385925\n",
      "epoch no.21 train no.711  loss = 0.4917997419834137\n",
      "epoch no.21 train no.721  loss = 0.5431114435195923\n",
      "epoch no.21 train no.731  loss = 1.3954575061798096\n",
      "epoch no.21 train no.741  loss = 0.4124799966812134\n",
      "epoch no.21 train no.751  loss = 0.7610083818435669\n",
      "epoch no.21 train no.761  loss = 0.5247752070426941\n",
      "epoch no.21 train no.771  loss = 0.3527204096317291\n",
      "epoch no.21 train no.781  loss = 0.4222981929779053\n",
      "epoch no.21 train no.791  loss = 0.1696508526802063\n",
      "epoch no.21 train no.801  loss = 0.8211612105369568\n",
      "epoch no.21 train no.811  loss = 0.8933920860290527\n",
      "epoch no.21 train no.821  loss = 0.3314344584941864\n",
      "epoch no.21 train no.831  loss = 0.29455941915512085\n",
      "epoch no.21 train no.841  loss = 0.38387638330459595\n",
      "epoch no.21 train no.851  loss = 0.7725436687469482\n",
      "epoch no.21 train no.861  loss = 0.5172712206840515\n",
      "epoch no.21 train no.871  loss = 1.2961549758911133\n",
      "epoch no.21 train no.881  loss = 0.46872472763061523\n",
      "epoch no.21 train no.891  loss = 0.49522149562835693\n",
      "epoch no.21 train no.901  loss = 0.599186897277832\n",
      "epoch no.21 train no.911  loss = 0.5229305624961853\n",
      "epoch no.21 train no.921  loss = 0.41744112968444824\n",
      "epoch no.21 train no.931  loss = 0.5559788346290588\n",
      "epoch no.21 train no.941  loss = 0.7804579734802246\n",
      "epoch no.21 train no.951  loss = 0.5557022094726562\n",
      "epoch no.21 train no.961  loss = 0.3624212145805359\n",
      "epoch no.21 train no.971  loss = 1.1255431175231934\n",
      "epoch no.21 train no.981  loss = 0.3136771321296692\n",
      "epoch no.21 train no.991  loss = 0.7963276505470276\n",
      "epoch no.21 train no.1001  loss = 0.15778490900993347\n",
      "epoch no.22 train no.1  loss = 0.7564542293548584\n",
      "epoch no.22 train no.11  loss = 0.4524540305137634\n",
      "epoch no.22 train no.21  loss = 0.27758219838142395\n",
      "epoch no.22 train no.31  loss = 0.3487410247325897\n",
      "epoch no.22 train no.41  loss = 0.5778490900993347\n",
      "epoch no.22 train no.51  loss = 0.741362452507019\n",
      "epoch no.22 train no.61  loss = 0.48602697253227234\n",
      "epoch no.22 train no.71  loss = 0.8804110288619995\n",
      "epoch no.22 train no.81  loss = 0.3259614109992981\n",
      "epoch no.22 train no.91  loss = 0.41731342673301697\n",
      "epoch no.22 train no.101  loss = 0.8323750495910645\n",
      "epoch no.22 train no.111  loss = 0.83941251039505\n",
      "epoch no.22 train no.121  loss = 0.3634515702724457\n",
      "epoch no.22 train no.131  loss = 0.8099109530448914\n",
      "epoch no.22 train no.141  loss = 0.4930276870727539\n",
      "epoch no.22 train no.151  loss = 0.38014572858810425\n",
      "epoch no.22 train no.161  loss = 1.0555976629257202\n",
      "epoch no.22 train no.171  loss = 0.5411474108695984\n",
      "epoch no.22 train no.181  loss = 0.6053773760795593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.22 train no.191  loss = 0.5682371258735657\n",
      "epoch no.22 train no.201  loss = 0.2681303322315216\n",
      "epoch no.22 train no.211  loss = 0.2923632860183716\n",
      "epoch no.22 train no.221  loss = 1.280716061592102\n",
      "epoch no.22 train no.231  loss = 0.44784268736839294\n",
      "epoch no.22 train no.241  loss = 0.28963345289230347\n",
      "epoch no.22 train no.251  loss = 0.5053640604019165\n",
      "epoch no.22 train no.261  loss = 0.6291272044181824\n",
      "epoch no.22 train no.271  loss = 0.29244184494018555\n",
      "epoch no.22 train no.281  loss = 0.48942461609840393\n",
      "epoch no.22 train no.291  loss = 0.32050150632858276\n",
      "epoch no.22 train no.301  loss = 0.38151776790618896\n",
      "epoch no.22 train no.311  loss = 0.17940975725650787\n",
      "epoch no.22 train no.321  loss = 0.828920304775238\n",
      "epoch no.22 train no.331  loss = 0.6562354564666748\n",
      "epoch no.22 train no.341  loss = 0.2680119574069977\n",
      "epoch no.22 train no.351  loss = 0.4543781280517578\n",
      "epoch no.22 train no.361  loss = 0.5639548897743225\n",
      "epoch no.22 train no.371  loss = 0.3891828954219818\n",
      "epoch no.22 train no.381  loss = 0.47935372591018677\n",
      "epoch no.22 train no.391  loss = 0.6321432590484619\n",
      "epoch no.22 train no.401  loss = 0.9427987933158875\n",
      "epoch no.22 train no.411  loss = 0.29800817370414734\n",
      "epoch no.22 train no.421  loss = 0.27506211400032043\n",
      "epoch no.22 train no.431  loss = 0.2511054277420044\n",
      "epoch no.22 train no.441  loss = 0.5649814605712891\n",
      "epoch no.22 train no.451  loss = 0.4367537498474121\n",
      "epoch no.22 train no.461  loss = 0.3274421691894531\n",
      "epoch no.22 train no.471  loss = 0.32058852910995483\n",
      "epoch no.22 train no.481  loss = 0.4506285488605499\n",
      "epoch no.22 train no.491  loss = 0.8594822287559509\n",
      "epoch no.22 train no.501  loss = 0.5123042464256287\n",
      "epoch no.22 train no.511  loss = 0.36878645420074463\n",
      "epoch no.22 train no.521  loss = 0.19469624757766724\n",
      "epoch no.22 train no.531  loss = 0.45055413246154785\n",
      "epoch no.22 train no.541  loss = 0.3324222266674042\n",
      "epoch no.22 train no.551  loss = 0.41729989647865295\n",
      "epoch no.22 train no.561  loss = 0.45345833897590637\n",
      "epoch no.22 train no.571  loss = 0.5246537327766418\n",
      "epoch no.22 train no.581  loss = 0.3557198941707611\n",
      "epoch no.22 train no.591  loss = 0.6063851714134216\n",
      "epoch no.22 train no.601  loss = 0.7732231616973877\n",
      "epoch no.22 train no.611  loss = 0.3847367465496063\n",
      "epoch no.22 train no.621  loss = 0.34293419122695923\n",
      "epoch no.22 train no.631  loss = 0.33917927742004395\n",
      "epoch no.22 train no.641  loss = 0.7473613023757935\n",
      "epoch no.22 train no.651  loss = 0.26320093870162964\n",
      "epoch no.22 train no.661  loss = 0.9727525115013123\n",
      "epoch no.22 train no.671  loss = 0.6451252102851868\n",
      "epoch no.22 train no.681  loss = 0.499369353055954\n",
      "epoch no.22 train no.691  loss = 0.5861331820487976\n",
      "epoch no.22 train no.701  loss = 0.5769798755645752\n",
      "epoch no.22 train no.711  loss = 0.35001224279403687\n",
      "epoch no.22 train no.721  loss = 0.38912972807884216\n",
      "epoch no.22 train no.731  loss = 0.24708127975463867\n",
      "epoch no.22 train no.741  loss = 0.5690736770629883\n",
      "epoch no.22 train no.751  loss = 0.3116607666015625\n",
      "epoch no.22 train no.761  loss = 0.41802090406417847\n",
      "epoch no.22 train no.771  loss = 0.6557432413101196\n",
      "epoch no.22 train no.781  loss = 0.3849363923072815\n",
      "epoch no.22 train no.791  loss = 0.8481306433677673\n",
      "epoch no.22 train no.801  loss = 0.31052732467651367\n",
      "epoch no.22 train no.811  loss = 0.5326293706893921\n",
      "epoch no.22 train no.821  loss = 0.27338117361068726\n",
      "epoch no.22 train no.831  loss = 1.0369224548339844\n",
      "epoch no.22 train no.841  loss = 0.8194993734359741\n",
      "epoch no.22 train no.851  loss = 0.4707719385623932\n",
      "epoch no.22 train no.861  loss = 0.990350604057312\n",
      "epoch no.22 train no.871  loss = 0.6513976454734802\n",
      "epoch no.22 train no.881  loss = 0.5822956562042236\n",
      "epoch no.22 train no.891  loss = 0.40148863196372986\n",
      "epoch no.22 train no.901  loss = 0.48400887846946716\n",
      "epoch no.22 train no.911  loss = 1.1371768712997437\n",
      "epoch no.22 train no.921  loss = 0.3202613890171051\n",
      "epoch no.22 train no.931  loss = 0.8953704237937927\n",
      "epoch no.22 train no.941  loss = 1.781718134880066\n",
      "epoch no.22 train no.951  loss = 0.401125967502594\n",
      "epoch no.22 train no.961  loss = 0.25874221324920654\n",
      "epoch no.22 train no.971  loss = 0.46981868147850037\n",
      "epoch no.22 train no.981  loss = 0.664552628993988\n",
      "epoch no.22 train no.991  loss = 0.25978711247444153\n",
      "epoch no.22 train no.1001  loss = 0.380558580160141\n",
      "epoch no.23 train no.1  loss = 0.3334706127643585\n",
      "epoch no.23 train no.11  loss = 0.6619929075241089\n",
      "epoch no.23 train no.21  loss = 0.373723566532135\n",
      "epoch no.23 train no.31  loss = 0.8648399710655212\n",
      "epoch no.23 train no.41  loss = 0.31260398030281067\n",
      "epoch no.23 train no.51  loss = 2.7083535194396973\n",
      "epoch no.23 train no.61  loss = 0.39875686168670654\n",
      "epoch no.23 train no.71  loss = 0.9334921836853027\n",
      "epoch no.23 train no.81  loss = 1.0531326532363892\n",
      "epoch no.23 train no.91  loss = 0.3802337944507599\n",
      "epoch no.23 train no.101  loss = 0.5261428356170654\n",
      "epoch no.23 train no.111  loss = 0.4689449965953827\n",
      "epoch no.23 train no.121  loss = 0.20921123027801514\n",
      "epoch no.23 train no.131  loss = 0.3750116527080536\n",
      "epoch no.23 train no.141  loss = 0.8101739287376404\n",
      "epoch no.23 train no.151  loss = 0.5041704773902893\n",
      "epoch no.23 train no.161  loss = 0.2594432234764099\n",
      "epoch no.23 train no.171  loss = 0.26865071058273315\n",
      "epoch no.23 train no.181  loss = 1.626630187034607\n",
      "epoch no.23 train no.191  loss = 1.0288364887237549\n",
      "epoch no.23 train no.201  loss = 0.42827528715133667\n",
      "epoch no.23 train no.211  loss = 0.5163008570671082\n",
      "epoch no.23 train no.221  loss = 0.3023357391357422\n",
      "epoch no.23 train no.231  loss = 0.3041664659976959\n",
      "epoch no.23 train no.241  loss = 0.3418753743171692\n",
      "epoch no.23 train no.251  loss = 0.2423120141029358\n",
      "epoch no.23 train no.261  loss = 0.4302196502685547\n",
      "epoch no.23 train no.271  loss = 0.4772758483886719\n",
      "epoch no.23 train no.281  loss = 0.5968067646026611\n",
      "epoch no.23 train no.291  loss = 0.4664143919944763\n",
      "epoch no.23 train no.301  loss = 0.4990648031234741\n",
      "epoch no.23 train no.311  loss = 0.3477542996406555\n",
      "epoch no.23 train no.321  loss = 0.42478471994400024\n",
      "epoch no.23 train no.331  loss = 0.5294801592826843\n",
      "epoch no.23 train no.341  loss = 0.7039341926574707\n",
      "epoch no.23 train no.351  loss = 0.2651335299015045\n",
      "epoch no.23 train no.361  loss = 0.5188153982162476\n",
      "epoch no.23 train no.371  loss = 0.5334268808364868\n",
      "epoch no.23 train no.381  loss = 1.260692834854126\n",
      "epoch no.23 train no.391  loss = 0.3722561299800873\n",
      "epoch no.23 train no.401  loss = 0.5486994981765747\n",
      "epoch no.23 train no.411  loss = 0.6902108788490295\n",
      "epoch no.23 train no.421  loss = 0.19906048476696014\n",
      "epoch no.23 train no.431  loss = 0.6313194632530212\n",
      "epoch no.23 train no.441  loss = 0.3330013155937195\n",
      "epoch no.23 train no.451  loss = 0.9000470042228699\n",
      "epoch no.23 train no.461  loss = 0.3348598778247833\n",
      "epoch no.23 train no.471  loss = 0.3292323350906372\n",
      "epoch no.23 train no.481  loss = 0.483100026845932\n",
      "epoch no.23 train no.491  loss = 0.5503354072570801\n",
      "epoch no.23 train no.501  loss = 0.789344847202301\n",
      "epoch no.23 train no.511  loss = 0.47876057028770447\n",
      "epoch no.23 train no.521  loss = 0.45902150869369507\n",
      "epoch no.23 train no.531  loss = 0.38938942551612854\n",
      "epoch no.23 train no.541  loss = 0.26278722286224365\n",
      "epoch no.23 train no.551  loss = 0.6250547766685486\n",
      "epoch no.23 train no.561  loss = 0.9029593467712402\n",
      "epoch no.23 train no.571  loss = 0.5608037114143372\n",
      "epoch no.23 train no.581  loss = 0.2102884203195572\n",
      "epoch no.23 train no.591  loss = 0.9926272034645081\n",
      "epoch no.23 train no.601  loss = 1.6530365943908691\n",
      "epoch no.23 train no.611  loss = 0.40319639444351196\n",
      "epoch no.23 train no.621  loss = 0.43524473905563354\n",
      "epoch no.23 train no.631  loss = 0.4884412884712219\n",
      "epoch no.23 train no.641  loss = 0.42845621705055237\n",
      "epoch no.23 train no.651  loss = 0.49027371406555176\n",
      "epoch no.23 train no.661  loss = 0.34714892506599426\n",
      "epoch no.23 train no.671  loss = 0.43946316838264465\n",
      "epoch no.23 train no.681  loss = 1.597400188446045\n",
      "epoch no.23 train no.691  loss = 0.4496338665485382\n",
      "epoch no.23 train no.701  loss = 0.26050010323524475\n",
      "epoch no.23 train no.711  loss = 0.791037917137146\n",
      "epoch no.23 train no.721  loss = 0.5994792580604553\n",
      "epoch no.23 train no.731  loss = 0.7716785073280334\n",
      "epoch no.23 train no.741  loss = 0.6666349768638611\n",
      "epoch no.23 train no.751  loss = 0.4127475917339325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.23 train no.761  loss = 0.33282002806663513\n",
      "epoch no.23 train no.771  loss = 0.2797936797142029\n",
      "epoch no.23 train no.781  loss = 0.46506500244140625\n",
      "epoch no.23 train no.791  loss = 0.7814712524414062\n",
      "epoch no.23 train no.801  loss = 0.7744285464286804\n",
      "epoch no.23 train no.811  loss = 0.788564145565033\n",
      "epoch no.23 train no.821  loss = 0.21603751182556152\n",
      "epoch no.23 train no.831  loss = 0.8350812792778015\n",
      "epoch no.23 train no.841  loss = 0.4251910150051117\n",
      "epoch no.23 train no.851  loss = 0.45540785789489746\n",
      "epoch no.23 train no.861  loss = 0.9145691990852356\n",
      "epoch no.23 train no.871  loss = 0.31611984968185425\n",
      "epoch no.23 train no.881  loss = 0.5176998972892761\n",
      "epoch no.23 train no.891  loss = 0.2609080970287323\n",
      "epoch no.23 train no.901  loss = 0.29188039898872375\n",
      "epoch no.23 train no.911  loss = 0.28609031438827515\n",
      "epoch no.23 train no.921  loss = 0.22093184292316437\n",
      "epoch no.23 train no.931  loss = 0.39562851190567017\n",
      "epoch no.23 train no.941  loss = 0.3391624391078949\n",
      "epoch no.23 train no.951  loss = 0.6301857829093933\n",
      "epoch no.23 train no.961  loss = 0.35963377356529236\n",
      "epoch no.23 train no.971  loss = 0.4258432686328888\n",
      "epoch no.23 train no.981  loss = 0.6182886958122253\n",
      "epoch no.23 train no.991  loss = 0.28140178322792053\n",
      "epoch no.23 train no.1001  loss = 0.607516884803772\n",
      "epoch no.24 train no.1  loss = 0.6426383256912231\n",
      "epoch no.24 train no.11  loss = 0.38060566782951355\n",
      "epoch no.24 train no.21  loss = 0.18334071338176727\n",
      "epoch no.24 train no.31  loss = 0.23701150715351105\n",
      "epoch no.24 train no.41  loss = 0.8721544146537781\n",
      "epoch no.24 train no.51  loss = 0.7119443416595459\n",
      "epoch no.24 train no.61  loss = 0.5330100655555725\n",
      "epoch no.24 train no.71  loss = 0.2936159074306488\n",
      "epoch no.24 train no.81  loss = 0.6280083060264587\n",
      "epoch no.24 train no.91  loss = 0.4817083179950714\n",
      "epoch no.24 train no.101  loss = 0.3152788579463959\n",
      "epoch no.24 train no.111  loss = 0.36291196942329407\n",
      "epoch no.24 train no.121  loss = 0.2009020745754242\n",
      "epoch no.24 train no.131  loss = 0.4878668189048767\n",
      "epoch no.24 train no.141  loss = 0.5548920035362244\n",
      "epoch no.24 train no.151  loss = 0.7662101984024048\n",
      "epoch no.24 train no.161  loss = 0.5089958906173706\n",
      "epoch no.24 train no.171  loss = 0.44841697812080383\n",
      "epoch no.24 train no.181  loss = 0.37090760469436646\n",
      "epoch no.24 train no.191  loss = 0.779300332069397\n",
      "epoch no.24 train no.201  loss = 0.5536218285560608\n",
      "epoch no.24 train no.211  loss = 0.7606155276298523\n",
      "epoch no.24 train no.221  loss = 0.6487767100334167\n",
      "epoch no.24 train no.231  loss = 0.5524885058403015\n",
      "epoch no.24 train no.241  loss = 0.22903519868850708\n",
      "epoch no.24 train no.251  loss = 2.0081684589385986\n",
      "epoch no.24 train no.261  loss = 0.22443781793117523\n",
      "epoch no.24 train no.271  loss = 0.314083069562912\n",
      "epoch no.24 train no.281  loss = 0.8505808711051941\n",
      "epoch no.24 train no.291  loss = 0.22625982761383057\n",
      "epoch no.24 train no.301  loss = 0.42741745710372925\n",
      "epoch no.24 train no.311  loss = 0.7427068948745728\n",
      "epoch no.24 train no.321  loss = 0.3569478690624237\n",
      "epoch no.24 train no.331  loss = 0.48075389862060547\n",
      "epoch no.24 train no.341  loss = 0.6492977738380432\n",
      "epoch no.24 train no.351  loss = 0.7773244380950928\n",
      "epoch no.24 train no.361  loss = 0.6926099061965942\n",
      "epoch no.24 train no.371  loss = 0.9123836755752563\n",
      "epoch no.24 train no.381  loss = 0.9331656694412231\n",
      "epoch no.24 train no.391  loss = 0.35024988651275635\n",
      "epoch no.24 train no.401  loss = 0.393889456987381\n",
      "epoch no.24 train no.411  loss = 0.570611298084259\n",
      "epoch no.24 train no.421  loss = 0.5082348585128784\n",
      "epoch no.24 train no.431  loss = 0.29553544521331787\n",
      "epoch no.24 train no.441  loss = 0.5590677261352539\n",
      "epoch no.24 train no.451  loss = 0.41082894802093506\n",
      "epoch no.24 train no.461  loss = 0.26334068179130554\n",
      "epoch no.24 train no.471  loss = 0.45952001214027405\n",
      "epoch no.24 train no.481  loss = 0.6038177609443665\n",
      "epoch no.24 train no.491  loss = 0.5551015138626099\n",
      "epoch no.24 train no.501  loss = 0.6045236587524414\n",
      "epoch no.24 train no.511  loss = 0.39386722445487976\n",
      "epoch no.24 train no.521  loss = 0.49686771631240845\n",
      "epoch no.24 train no.531  loss = 0.3877828121185303\n",
      "epoch no.24 train no.541  loss = 0.3517064154148102\n",
      "epoch no.24 train no.551  loss = 0.3446742296218872\n",
      "epoch no.24 train no.561  loss = 0.649348795413971\n",
      "epoch no.24 train no.571  loss = 0.6409327387809753\n",
      "epoch no.24 train no.581  loss = 0.8775062561035156\n",
      "epoch no.24 train no.591  loss = 0.3455726206302643\n",
      "epoch no.24 train no.601  loss = 0.46676212549209595\n",
      "epoch no.24 train no.611  loss = 0.7681080102920532\n",
      "epoch no.24 train no.621  loss = 0.43012192845344543\n",
      "epoch no.24 train no.631  loss = 0.4828660190105438\n",
      "epoch no.24 train no.641  loss = 0.2553216218948364\n",
      "epoch no.24 train no.651  loss = 0.31178948283195496\n",
      "epoch no.24 train no.661  loss = 0.6656742691993713\n",
      "epoch no.24 train no.671  loss = 0.8725795745849609\n",
      "epoch no.24 train no.681  loss = 0.278109073638916\n",
      "epoch no.24 train no.691  loss = 0.6362088322639465\n",
      "epoch no.24 train no.701  loss = 0.4711003601551056\n",
      "epoch no.24 train no.711  loss = 0.3665667772293091\n",
      "epoch no.24 train no.721  loss = 0.3584064245223999\n",
      "epoch no.24 train no.731  loss = 0.7707095146179199\n",
      "epoch no.24 train no.741  loss = 0.45452871918678284\n",
      "epoch no.24 train no.751  loss = 0.8270267844200134\n",
      "epoch no.24 train no.761  loss = 1.1717565059661865\n",
      "epoch no.24 train no.771  loss = 0.1928759217262268\n",
      "epoch no.24 train no.781  loss = 0.2203620970249176\n",
      "epoch no.24 train no.791  loss = 0.3135984539985657\n",
      "epoch no.24 train no.801  loss = 0.9880725741386414\n",
      "epoch no.24 train no.811  loss = 0.2586970329284668\n",
      "epoch no.24 train no.821  loss = 0.5055051445960999\n",
      "epoch no.24 train no.831  loss = 1.0788061618804932\n",
      "epoch no.24 train no.841  loss = 1.325193166732788\n",
      "epoch no.24 train no.851  loss = 0.5297353863716125\n",
      "epoch no.24 train no.861  loss = 0.27830594778060913\n",
      "epoch no.24 train no.871  loss = 0.21607832610607147\n",
      "epoch no.24 train no.881  loss = 0.5728961229324341\n",
      "epoch no.24 train no.891  loss = 1.4217503070831299\n",
      "epoch no.24 train no.901  loss = 0.21582970023155212\n",
      "epoch no.24 train no.911  loss = 0.5440054535865784\n",
      "epoch no.24 train no.921  loss = 0.40517568588256836\n",
      "epoch no.24 train no.931  loss = 0.4939320385456085\n",
      "epoch no.24 train no.941  loss = 0.3869272470474243\n",
      "epoch no.24 train no.951  loss = 0.19486765563488007\n",
      "epoch no.24 train no.961  loss = 0.20475031435489655\n",
      "epoch no.24 train no.971  loss = 1.1067501306533813\n",
      "epoch no.24 train no.981  loss = 0.32945355772972107\n",
      "epoch no.24 train no.991  loss = 0.36471158266067505\n",
      "epoch no.24 train no.1001  loss = 0.16231781244277954\n",
      "epoch no.25 train no.1  loss = 0.4888605773448944\n",
      "epoch no.25 train no.11  loss = 0.1716839224100113\n",
      "epoch no.25 train no.21  loss = 0.3815341293811798\n",
      "epoch no.25 train no.31  loss = 0.941177248954773\n",
      "epoch no.25 train no.41  loss = 0.3964858949184418\n",
      "epoch no.25 train no.51  loss = 0.2425241470336914\n",
      "epoch no.25 train no.61  loss = 0.48793548345565796\n",
      "epoch no.25 train no.71  loss = 0.583296000957489\n",
      "epoch no.25 train no.81  loss = 0.2978064715862274\n",
      "epoch no.25 train no.91  loss = 0.3156943619251251\n",
      "epoch no.25 train no.101  loss = 0.4125918447971344\n",
      "epoch no.25 train no.111  loss = 0.5473853945732117\n",
      "epoch no.25 train no.121  loss = 0.4385389983654022\n",
      "epoch no.25 train no.131  loss = 0.4681491255760193\n",
      "epoch no.25 train no.141  loss = 1.0485345125198364\n",
      "epoch no.25 train no.151  loss = 0.4775561988353729\n",
      "epoch no.25 train no.161  loss = 0.6469628214836121\n",
      "epoch no.25 train no.171  loss = 0.19509971141815186\n",
      "epoch no.25 train no.181  loss = 0.4744449555873871\n",
      "epoch no.25 train no.191  loss = 0.5157754421234131\n",
      "epoch no.25 train no.201  loss = 0.30575254559516907\n",
      "epoch no.25 train no.211  loss = 0.47278788685798645\n",
      "epoch no.25 train no.221  loss = 0.2917785942554474\n",
      "epoch no.25 train no.231  loss = 0.32401224970817566\n",
      "epoch no.25 train no.241  loss = 0.25590699911117554\n",
      "epoch no.25 train no.251  loss = 0.35413187742233276\n",
      "epoch no.25 train no.261  loss = 0.6310470104217529\n",
      "epoch no.25 train no.271  loss = 0.7214464545249939\n",
      "epoch no.25 train no.281  loss = 0.43153610825538635\n",
      "epoch no.25 train no.291  loss = 0.7473767995834351\n",
      "epoch no.25 train no.301  loss = 0.3866339921951294\n",
      "epoch no.25 train no.311  loss = 0.5342637300491333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.25 train no.321  loss = 1.00715970993042\n",
      "epoch no.25 train no.331  loss = 0.9148460030555725\n",
      "epoch no.25 train no.341  loss = 0.7659007906913757\n",
      "epoch no.25 train no.351  loss = 0.29396891593933105\n",
      "epoch no.25 train no.361  loss = 0.5903295874595642\n",
      "epoch no.25 train no.371  loss = 0.2950918972492218\n",
      "epoch no.25 train no.381  loss = 0.5587780475616455\n",
      "epoch no.25 train no.391  loss = 1.7822535037994385\n",
      "epoch no.25 train no.401  loss = 0.408791720867157\n",
      "epoch no.25 train no.411  loss = 0.6272550225257874\n",
      "epoch no.25 train no.421  loss = 0.43762996792793274\n",
      "epoch no.25 train no.431  loss = 0.21939566731452942\n",
      "epoch no.25 train no.441  loss = 0.41066446900367737\n",
      "epoch no.25 train no.451  loss = 0.5636815428733826\n",
      "epoch no.25 train no.461  loss = 0.6404642462730408\n",
      "epoch no.25 train no.471  loss = 0.5448951125144958\n",
      "epoch no.25 train no.481  loss = 0.38892441987991333\n",
      "epoch no.25 train no.491  loss = 0.32923316955566406\n",
      "epoch no.25 train no.501  loss = 0.3018069565296173\n",
      "epoch no.25 train no.511  loss = 0.6782223582267761\n",
      "epoch no.25 train no.521  loss = 0.24561361968517303\n",
      "epoch no.25 train no.531  loss = 0.43160802125930786\n",
      "epoch no.25 train no.541  loss = 1.0097832679748535\n",
      "epoch no.25 train no.551  loss = 0.37628790736198425\n",
      "epoch no.25 train no.561  loss = 0.4108668863773346\n",
      "epoch no.25 train no.571  loss = 0.4231316149234772\n",
      "epoch no.25 train no.581  loss = 0.16519156098365784\n",
      "epoch no.25 train no.591  loss = 0.3667372167110443\n",
      "epoch no.25 train no.601  loss = 1.1635719537734985\n",
      "epoch no.25 train no.611  loss = 0.4620091915130615\n",
      "epoch no.25 train no.621  loss = 0.318092405796051\n",
      "epoch no.25 train no.631  loss = 0.3837001621723175\n",
      "epoch no.25 train no.641  loss = 0.2041621208190918\n",
      "epoch no.25 train no.651  loss = 0.19322192668914795\n",
      "epoch no.25 train no.661  loss = 0.38594746589660645\n",
      "epoch no.25 train no.671  loss = 0.5388298034667969\n",
      "epoch no.25 train no.681  loss = 0.30357176065444946\n",
      "epoch no.25 train no.691  loss = 0.392899751663208\n",
      "epoch no.25 train no.701  loss = 0.28574076294898987\n",
      "epoch no.25 train no.711  loss = 0.36422646045684814\n",
      "epoch no.25 train no.721  loss = 0.8663253784179688\n",
      "epoch no.25 train no.731  loss = 0.2837945222854614\n",
      "epoch no.25 train no.741  loss = 0.864971399307251\n",
      "epoch no.25 train no.751  loss = 0.3531341254711151\n",
      "epoch no.25 train no.761  loss = 0.6794540286064148\n",
      "epoch no.25 train no.771  loss = 0.1911056935787201\n",
      "epoch no.25 train no.781  loss = 1.2594308853149414\n",
      "epoch no.25 train no.791  loss = 0.5579036474227905\n",
      "epoch no.25 train no.801  loss = 1.376197338104248\n",
      "epoch no.25 train no.811  loss = 0.474842369556427\n",
      "epoch no.25 train no.821  loss = 0.3745773136615753\n",
      "epoch no.25 train no.831  loss = 0.9178609848022461\n",
      "epoch no.25 train no.841  loss = 0.3837612271308899\n",
      "epoch no.25 train no.851  loss = 1.1557323932647705\n",
      "epoch no.25 train no.861  loss = 0.3127998113632202\n",
      "epoch no.25 train no.871  loss = 0.4889223277568817\n",
      "epoch no.25 train no.881  loss = 0.7287805676460266\n",
      "epoch no.25 train no.891  loss = 0.27573686838150024\n",
      "epoch no.25 train no.901  loss = 0.2640783488750458\n",
      "epoch no.25 train no.911  loss = 0.6743677258491516\n",
      "epoch no.25 train no.921  loss = 0.32633623480796814\n",
      "epoch no.25 train no.931  loss = 0.2914290726184845\n",
      "epoch no.25 train no.941  loss = 0.43360447883605957\n",
      "epoch no.25 train no.951  loss = 0.2300807237625122\n",
      "epoch no.25 train no.961  loss = 1.2082784175872803\n",
      "epoch no.25 train no.971  loss = 0.7293565273284912\n",
      "epoch no.25 train no.981  loss = 0.7296144366264343\n",
      "epoch no.25 train no.991  loss = 0.5353217720985413\n",
      "epoch no.25 train no.1001  loss = 0.22979766130447388\n",
      "epoch no.26 train no.1  loss = 0.2681695818901062\n",
      "epoch no.26 train no.11  loss = 0.4903108477592468\n",
      "epoch no.26 train no.21  loss = 0.36552587151527405\n",
      "epoch no.26 train no.31  loss = 0.24961131811141968\n",
      "epoch no.26 train no.41  loss = 1.4193861484527588\n",
      "epoch no.26 train no.51  loss = 0.32599347829818726\n",
      "epoch no.26 train no.61  loss = 1.0079156160354614\n",
      "epoch no.26 train no.71  loss = 0.5892367959022522\n",
      "epoch no.26 train no.81  loss = 0.1540689766407013\n",
      "epoch no.26 train no.91  loss = 0.43419572710990906\n",
      "epoch no.26 train no.101  loss = 0.3489660620689392\n",
      "epoch no.26 train no.111  loss = 0.3017871379852295\n",
      "epoch no.26 train no.121  loss = 0.1698485165834427\n",
      "epoch no.26 train no.131  loss = 0.4560709595680237\n",
      "epoch no.26 train no.141  loss = 0.5779199004173279\n",
      "epoch no.26 train no.151  loss = 0.8801780343055725\n",
      "epoch no.26 train no.161  loss = 0.3519197106361389\n",
      "epoch no.26 train no.171  loss = 0.49618685245513916\n",
      "epoch no.26 train no.181  loss = 0.23136602342128754\n",
      "epoch no.26 train no.191  loss = 0.6721987128257751\n",
      "epoch no.26 train no.201  loss = 0.3081311285495758\n",
      "epoch no.26 train no.211  loss = 0.6528245210647583\n",
      "epoch no.26 train no.221  loss = 0.6998056769371033\n",
      "epoch no.26 train no.231  loss = 0.325874924659729\n",
      "epoch no.26 train no.241  loss = 0.588310956954956\n",
      "epoch no.26 train no.251  loss = 0.5324031710624695\n",
      "epoch no.26 train no.261  loss = 0.4783842861652374\n",
      "epoch no.26 train no.271  loss = 0.3170638680458069\n",
      "epoch no.26 train no.281  loss = 0.4794909656047821\n",
      "epoch no.26 train no.291  loss = 1.01886785030365\n",
      "epoch no.26 train no.301  loss = 0.8655494451522827\n",
      "epoch no.26 train no.311  loss = 0.5894319415092468\n",
      "epoch no.26 train no.321  loss = 0.1967267096042633\n",
      "epoch no.26 train no.331  loss = 0.3366478383541107\n",
      "epoch no.26 train no.341  loss = 0.47730207443237305\n",
      "epoch no.26 train no.351  loss = 1.333865761756897\n",
      "epoch no.26 train no.361  loss = 0.27440792322158813\n",
      "epoch no.26 train no.371  loss = 0.2607504725456238\n",
      "epoch no.26 train no.381  loss = 0.46440207958221436\n",
      "epoch no.26 train no.391  loss = 0.6153817176818848\n",
      "epoch no.26 train no.401  loss = 0.6617624163627625\n",
      "epoch no.26 train no.411  loss = 0.5649299025535583\n",
      "epoch no.26 train no.421  loss = 0.945384681224823\n",
      "epoch no.26 train no.431  loss = 0.5923685431480408\n",
      "epoch no.26 train no.441  loss = 0.300716370344162\n",
      "epoch no.26 train no.451  loss = 0.25024184584617615\n",
      "epoch no.26 train no.461  loss = 0.6506323218345642\n",
      "epoch no.26 train no.471  loss = 0.647888720035553\n",
      "epoch no.26 train no.481  loss = 0.2900470495223999\n",
      "epoch no.26 train no.491  loss = 0.69693922996521\n",
      "epoch no.26 train no.501  loss = 0.2679077088832855\n",
      "epoch no.26 train no.511  loss = 0.3295328617095947\n",
      "epoch no.26 train no.521  loss = 0.5546796917915344\n",
      "epoch no.26 train no.531  loss = 0.4480716586112976\n",
      "epoch no.26 train no.541  loss = 0.4321591854095459\n",
      "epoch no.26 train no.551  loss = 0.9922829866409302\n",
      "epoch no.26 train no.561  loss = 0.9983451962471008\n",
      "epoch no.26 train no.571  loss = 0.9464364051818848\n",
      "epoch no.26 train no.581  loss = 0.49101346731185913\n",
      "epoch no.26 train no.591  loss = 0.21310923993587494\n",
      "epoch no.26 train no.601  loss = 0.7247385382652283\n",
      "epoch no.26 train no.611  loss = 0.3971289098262787\n",
      "epoch no.26 train no.621  loss = 0.20793431997299194\n",
      "epoch no.26 train no.631  loss = 0.46408790349960327\n",
      "epoch no.26 train no.641  loss = 0.378873735666275\n",
      "epoch no.26 train no.651  loss = 0.49578002095222473\n",
      "epoch no.26 train no.661  loss = 0.42503324151039124\n",
      "epoch no.26 train no.671  loss = 0.1805567443370819\n",
      "epoch no.26 train no.681  loss = 0.47356876730918884\n",
      "epoch no.26 train no.691  loss = 0.16190533339977264\n",
      "epoch no.26 train no.701  loss = 0.23709075152873993\n",
      "epoch no.26 train no.711  loss = 0.5342150926589966\n",
      "epoch no.26 train no.721  loss = 0.691309928894043\n",
      "epoch no.26 train no.731  loss = 0.2138030081987381\n",
      "epoch no.26 train no.741  loss = 1.0628437995910645\n",
      "epoch no.26 train no.751  loss = 0.6337628364562988\n",
      "epoch no.26 train no.761  loss = 0.25832444429397583\n",
      "epoch no.26 train no.771  loss = 0.42436665296554565\n",
      "epoch no.26 train no.781  loss = 0.3565530478954315\n",
      "epoch no.26 train no.791  loss = 0.3789653480052948\n",
      "epoch no.26 train no.801  loss = 0.5177141427993774\n",
      "epoch no.26 train no.811  loss = 0.7395208477973938\n",
      "epoch no.26 train no.821  loss = 0.9329131245613098\n",
      "epoch no.26 train no.831  loss = 0.6142102479934692\n",
      "epoch no.26 train no.841  loss = 0.3735343813896179\n",
      "epoch no.26 train no.851  loss = 0.29607415199279785\n",
      "epoch no.26 train no.861  loss = 0.19024966657161713\n",
      "epoch no.26 train no.871  loss = 0.17841662466526031\n",
      "epoch no.26 train no.881  loss = 0.6107744574546814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.26 train no.891  loss = 0.7115361094474792\n",
      "epoch no.26 train no.901  loss = 0.3137188255786896\n",
      "epoch no.26 train no.911  loss = 0.5961471199989319\n",
      "epoch no.26 train no.921  loss = 0.8618901371955872\n",
      "epoch no.26 train no.931  loss = 0.6378262639045715\n",
      "epoch no.26 train no.941  loss = 0.40626904368400574\n",
      "epoch no.26 train no.951  loss = 0.580567479133606\n",
      "epoch no.26 train no.961  loss = 0.42605435848236084\n",
      "epoch no.26 train no.971  loss = 0.6921471953392029\n",
      "epoch no.26 train no.981  loss = 0.8511694669723511\n",
      "epoch no.26 train no.991  loss = 0.6347464919090271\n",
      "epoch no.26 train no.1001  loss = 0.191537544131279\n",
      "epoch no.27 train no.1  loss = 0.18753103911876678\n",
      "epoch no.27 train no.11  loss = 0.7883486747741699\n",
      "epoch no.27 train no.21  loss = 0.26311057806015015\n",
      "epoch no.27 train no.31  loss = 0.5828409790992737\n",
      "epoch no.27 train no.41  loss = 0.34285035729408264\n",
      "epoch no.27 train no.51  loss = 0.4306698143482208\n",
      "epoch no.27 train no.61  loss = 0.670984148979187\n",
      "epoch no.27 train no.71  loss = 0.8933606147766113\n",
      "epoch no.27 train no.81  loss = 1.6710622310638428\n",
      "epoch no.27 train no.91  loss = 0.3264102041721344\n",
      "epoch no.27 train no.101  loss = 0.8613176941871643\n",
      "epoch no.27 train no.111  loss = 1.0623432397842407\n",
      "epoch no.27 train no.121  loss = 0.34071218967437744\n",
      "epoch no.27 train no.131  loss = 1.2358722686767578\n",
      "epoch no.27 train no.141  loss = 0.9341407418251038\n",
      "epoch no.27 train no.151  loss = 0.5558173656463623\n",
      "epoch no.27 train no.161  loss = 1.7102779150009155\n",
      "epoch no.27 train no.171  loss = 0.7736018896102905\n",
      "epoch no.27 train no.181  loss = 0.5847076773643494\n",
      "epoch no.27 train no.191  loss = 0.9828532934188843\n",
      "epoch no.27 train no.201  loss = 0.31607532501220703\n",
      "epoch no.27 train no.211  loss = 0.8043663501739502\n",
      "epoch no.27 train no.221  loss = 0.6247152090072632\n",
      "epoch no.27 train no.231  loss = 0.31514057517051697\n",
      "epoch no.27 train no.241  loss = 0.3652108311653137\n",
      "epoch no.27 train no.251  loss = 0.3527217209339142\n",
      "epoch no.27 train no.261  loss = 0.28953370451927185\n",
      "epoch no.27 train no.271  loss = 0.5082866549491882\n",
      "epoch no.27 train no.281  loss = 0.7495290637016296\n",
      "epoch no.27 train no.291  loss = 0.29296982288360596\n",
      "epoch no.27 train no.301  loss = 0.744188666343689\n",
      "epoch no.27 train no.311  loss = 0.5057373642921448\n",
      "epoch no.27 train no.321  loss = 0.4755891263484955\n",
      "epoch no.27 train no.331  loss = 0.6945217251777649\n",
      "epoch no.27 train no.341  loss = 0.9805588126182556\n",
      "epoch no.27 train no.351  loss = 0.5179780125617981\n",
      "epoch no.27 train no.361  loss = 0.5404574275016785\n",
      "epoch no.27 train no.371  loss = 0.7401614189147949\n",
      "epoch no.27 train no.381  loss = 0.626553475856781\n",
      "epoch no.27 train no.391  loss = 0.3151213824748993\n",
      "epoch no.27 train no.401  loss = 0.30524006485939026\n",
      "epoch no.27 train no.411  loss = 0.5990433692932129\n",
      "epoch no.27 train no.421  loss = 0.777046263217926\n",
      "epoch no.27 train no.431  loss = 0.44547438621520996\n",
      "epoch no.27 train no.441  loss = 0.3684544861316681\n",
      "epoch no.27 train no.451  loss = 0.3633615970611572\n",
      "epoch no.27 train no.461  loss = 0.2834729552268982\n",
      "epoch no.27 train no.471  loss = 0.766316294670105\n",
      "epoch no.27 train no.481  loss = 0.29614078998565674\n",
      "epoch no.27 train no.491  loss = 0.36962273716926575\n",
      "epoch no.27 train no.501  loss = 0.17137500643730164\n",
      "epoch no.27 train no.511  loss = 0.20493654906749725\n",
      "epoch no.27 train no.521  loss = 0.620488703250885\n",
      "epoch no.27 train no.531  loss = 0.5600645542144775\n",
      "epoch no.27 train no.541  loss = 0.27135545015335083\n",
      "epoch no.27 train no.551  loss = 0.5501298904418945\n",
      "epoch no.27 train no.561  loss = 0.5750113725662231\n",
      "epoch no.27 train no.571  loss = 0.7202463746070862\n",
      "epoch no.27 train no.581  loss = 1.1775312423706055\n",
      "epoch no.27 train no.591  loss = 0.6702964305877686\n",
      "epoch no.27 train no.601  loss = 0.23824617266654968\n",
      "epoch no.27 train no.611  loss = 0.832491934299469\n",
      "epoch no.27 train no.621  loss = 0.33751145005226135\n",
      "epoch no.27 train no.631  loss = 0.3585340082645416\n",
      "epoch no.27 train no.641  loss = 0.4078305959701538\n",
      "epoch no.27 train no.651  loss = 0.2493901401758194\n",
      "epoch no.27 train no.661  loss = 0.9285796880722046\n",
      "epoch no.27 train no.671  loss = 0.6588467359542847\n",
      "epoch no.27 train no.681  loss = 0.8741390109062195\n",
      "epoch no.27 train no.691  loss = 0.38314905762672424\n",
      "epoch no.27 train no.701  loss = 0.21363386511802673\n",
      "epoch no.27 train no.711  loss = 0.5338084101676941\n",
      "epoch no.27 train no.721  loss = 0.3933078348636627\n",
      "epoch no.27 train no.731  loss = 0.30806297063827515\n",
      "epoch no.27 train no.741  loss = 0.8082820177078247\n",
      "epoch no.27 train no.751  loss = 0.534028172492981\n",
      "epoch no.27 train no.761  loss = 0.9311057329177856\n",
      "epoch no.27 train no.771  loss = 0.38082414865493774\n",
      "epoch no.27 train no.781  loss = 0.5157808661460876\n",
      "epoch no.27 train no.791  loss = 0.4790166914463043\n",
      "epoch no.27 train no.801  loss = 0.5643565058708191\n",
      "epoch no.27 train no.811  loss = 0.4740612208843231\n",
      "epoch no.27 train no.821  loss = 0.44892698526382446\n",
      "epoch no.27 train no.831  loss = 0.48586130142211914\n",
      "epoch no.27 train no.841  loss = 0.3714686632156372\n",
      "epoch no.27 train no.851  loss = 0.3292101323604584\n",
      "epoch no.27 train no.861  loss = 0.623596727848053\n",
      "epoch no.27 train no.871  loss = 0.2991006076335907\n",
      "epoch no.27 train no.881  loss = 0.23953920602798462\n",
      "epoch no.27 train no.891  loss = 0.6585865020751953\n",
      "epoch no.27 train no.901  loss = 0.4417998492717743\n",
      "epoch no.27 train no.911  loss = 0.7639411091804504\n",
      "epoch no.27 train no.921  loss = 0.3001478314399719\n",
      "epoch no.27 train no.931  loss = 0.8817977905273438\n",
      "epoch no.27 train no.941  loss = 0.34668421745300293\n",
      "epoch no.27 train no.951  loss = 0.3998517692089081\n",
      "epoch no.27 train no.961  loss = 0.3777111768722534\n",
      "epoch no.27 train no.971  loss = 0.22840279340744019\n",
      "epoch no.27 train no.981  loss = 0.9862481951713562\n",
      "epoch no.27 train no.991  loss = 0.3761947751045227\n",
      "epoch no.27 train no.1001  loss = 0.5254369974136353\n",
      "epoch no.28 train no.1  loss = 0.5827637314796448\n",
      "epoch no.28 train no.11  loss = 0.4332749545574188\n",
      "epoch no.28 train no.21  loss = 0.888628363609314\n",
      "epoch no.28 train no.31  loss = 0.6237709522247314\n",
      "epoch no.28 train no.41  loss = 0.4136175215244293\n",
      "epoch no.28 train no.51  loss = 0.477871835231781\n",
      "epoch no.28 train no.61  loss = 0.3756949305534363\n",
      "epoch no.28 train no.71  loss = 0.28344419598579407\n",
      "epoch no.28 train no.81  loss = 0.23456032574176788\n",
      "epoch no.28 train no.91  loss = 0.6541313529014587\n",
      "epoch no.28 train no.101  loss = 0.955781102180481\n",
      "epoch no.28 train no.111  loss = 0.7296833395957947\n",
      "epoch no.28 train no.121  loss = 0.23666296899318695\n",
      "epoch no.28 train no.131  loss = 0.8118703365325928\n",
      "epoch no.28 train no.141  loss = 0.5931856632232666\n",
      "epoch no.28 train no.151  loss = 0.3743957579135895\n",
      "epoch no.28 train no.161  loss = 0.6161030530929565\n",
      "epoch no.28 train no.171  loss = 0.4564741849899292\n",
      "epoch no.28 train no.181  loss = 1.0782288312911987\n",
      "epoch no.28 train no.191  loss = 0.22956973314285278\n",
      "epoch no.28 train no.201  loss = 0.8288244605064392\n",
      "epoch no.28 train no.211  loss = 0.30247166752815247\n",
      "epoch no.28 train no.221  loss = 0.4116067886352539\n",
      "epoch no.28 train no.231  loss = 0.6581164598464966\n",
      "epoch no.28 train no.241  loss = 0.34066545963287354\n",
      "epoch no.28 train no.251  loss = 0.6939823031425476\n",
      "epoch no.28 train no.261  loss = 0.4712096154689789\n",
      "epoch no.28 train no.271  loss = 0.48501157760620117\n",
      "epoch no.28 train no.281  loss = 0.528522789478302\n",
      "epoch no.28 train no.291  loss = 1.3226723670959473\n",
      "epoch no.28 train no.301  loss = 0.471245676279068\n",
      "epoch no.28 train no.311  loss = 0.34218984842300415\n",
      "epoch no.28 train no.321  loss = 0.39366790652275085\n",
      "epoch no.28 train no.331  loss = 0.9904611110687256\n",
      "epoch no.28 train no.341  loss = 0.23465117812156677\n",
      "epoch no.28 train no.351  loss = 0.4463234543800354\n",
      "epoch no.28 train no.361  loss = 0.5307232737541199\n",
      "epoch no.28 train no.371  loss = 0.558600127696991\n",
      "epoch no.28 train no.381  loss = 0.5108035206794739\n",
      "epoch no.28 train no.391  loss = 0.4434471130371094\n",
      "epoch no.28 train no.401  loss = 0.20224353671073914\n",
      "epoch no.28 train no.411  loss = 0.6727020144462585\n",
      "epoch no.28 train no.421  loss = 0.6805008053779602\n",
      "epoch no.28 train no.431  loss = 1.11217200756073\n",
      "epoch no.28 train no.441  loss = 0.5908634662628174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.28 train no.451  loss = 0.30101487040519714\n",
      "epoch no.28 train no.461  loss = 0.8013982772827148\n",
      "epoch no.28 train no.471  loss = 0.3423989415168762\n",
      "epoch no.28 train no.481  loss = 1.038887619972229\n",
      "epoch no.28 train no.491  loss = 0.31040525436401367\n",
      "epoch no.28 train no.501  loss = 0.4136413335800171\n",
      "epoch no.28 train no.511  loss = 0.40437591075897217\n",
      "epoch no.28 train no.521  loss = 0.3950178027153015\n",
      "epoch no.28 train no.531  loss = 0.8377379179000854\n",
      "epoch no.28 train no.541  loss = 0.8162409663200378\n",
      "epoch no.28 train no.551  loss = 0.44748422503471375\n",
      "epoch no.28 train no.561  loss = 0.27086150646209717\n",
      "epoch no.28 train no.571  loss = 0.6789461970329285\n",
      "epoch no.28 train no.581  loss = 0.7552570104598999\n",
      "epoch no.28 train no.591  loss = 0.8153665661811829\n",
      "epoch no.28 train no.601  loss = 0.4967116415500641\n",
      "epoch no.28 train no.611  loss = 0.28846657276153564\n",
      "epoch no.28 train no.621  loss = 0.2595047652721405\n",
      "epoch no.28 train no.631  loss = 0.9938064217567444\n",
      "epoch no.28 train no.641  loss = 0.45873233675956726\n",
      "epoch no.28 train no.651  loss = 0.7772331833839417\n",
      "epoch no.28 train no.661  loss = 0.372927725315094\n",
      "epoch no.28 train no.671  loss = 0.4228462874889374\n",
      "epoch no.28 train no.681  loss = 0.894911527633667\n",
      "epoch no.28 train no.691  loss = 0.6331592798233032\n",
      "epoch no.28 train no.701  loss = 0.2808029055595398\n",
      "epoch no.28 train no.711  loss = 0.28502157330513\n",
      "epoch no.28 train no.721  loss = 0.1929217427968979\n",
      "epoch no.28 train no.731  loss = 0.7101069092750549\n",
      "epoch no.28 train no.741  loss = 1.3384801149368286\n",
      "epoch no.28 train no.751  loss = 0.23177622258663177\n",
      "epoch no.28 train no.761  loss = 0.2151765078306198\n",
      "epoch no.28 train no.771  loss = 0.13843472301959991\n",
      "epoch no.28 train no.781  loss = 1.0664328336715698\n",
      "epoch no.28 train no.791  loss = 0.3172385096549988\n",
      "epoch no.28 train no.801  loss = 0.5458298921585083\n",
      "epoch no.28 train no.811  loss = 0.44176122546195984\n",
      "epoch no.28 train no.821  loss = 0.5942316651344299\n",
      "epoch no.28 train no.831  loss = 0.5314508676528931\n",
      "epoch no.28 train no.841  loss = 0.6346143484115601\n",
      "epoch no.28 train no.851  loss = 0.45884445309638977\n",
      "epoch no.28 train no.861  loss = 0.6251657009124756\n",
      "epoch no.28 train no.871  loss = 0.41750848293304443\n",
      "epoch no.28 train no.881  loss = 0.6531341075897217\n",
      "epoch no.28 train no.891  loss = 0.24355612695217133\n",
      "epoch no.28 train no.901  loss = 0.2690393030643463\n",
      "epoch no.28 train no.911  loss = 0.4053063690662384\n",
      "epoch no.28 train no.921  loss = 0.5554018616676331\n",
      "epoch no.28 train no.931  loss = 0.4090633690357208\n",
      "epoch no.28 train no.941  loss = 0.3894907832145691\n",
      "epoch no.28 train no.951  loss = 0.7899535894393921\n",
      "epoch no.28 train no.961  loss = 0.6170977354049683\n",
      "epoch no.28 train no.971  loss = 0.9316516518592834\n",
      "epoch no.28 train no.981  loss = 0.389901727437973\n",
      "epoch no.28 train no.991  loss = 0.23641696572303772\n",
      "epoch no.28 train no.1001  loss = 0.6432014107704163\n",
      "epoch no.29 train no.1  loss = 0.3695164918899536\n",
      "epoch no.29 train no.11  loss = 0.47765225172042847\n",
      "epoch no.29 train no.21  loss = 0.18836639821529388\n",
      "epoch no.29 train no.31  loss = 0.17970934510231018\n",
      "epoch no.29 train no.41  loss = 0.5626823902130127\n",
      "epoch no.29 train no.51  loss = 0.277324914932251\n",
      "epoch no.29 train no.61  loss = 0.1351773738861084\n",
      "epoch no.29 train no.71  loss = 0.283348947763443\n",
      "epoch no.29 train no.81  loss = 0.5755264163017273\n",
      "epoch no.29 train no.91  loss = 1.0413697957992554\n",
      "epoch no.29 train no.101  loss = 0.854598879814148\n",
      "epoch no.29 train no.111  loss = 0.33072590827941895\n",
      "epoch no.29 train no.121  loss = 0.24013711512088776\n",
      "epoch no.29 train no.131  loss = 0.7871853113174438\n",
      "epoch no.29 train no.141  loss = 0.8154299855232239\n",
      "epoch no.29 train no.151  loss = 0.7060266733169556\n",
      "epoch no.29 train no.161  loss = 0.6642423868179321\n",
      "epoch no.29 train no.171  loss = 0.44143715500831604\n",
      "epoch no.29 train no.181  loss = 0.3272573947906494\n",
      "epoch no.29 train no.191  loss = 0.377075731754303\n",
      "epoch no.29 train no.201  loss = 0.8918144106864929\n",
      "epoch no.29 train no.211  loss = 0.5445500016212463\n",
      "epoch no.29 train no.221  loss = 0.35753270983695984\n",
      "epoch no.29 train no.231  loss = 0.6337454319000244\n",
      "epoch no.29 train no.241  loss = 0.6125223636627197\n",
      "epoch no.29 train no.251  loss = 0.5224498510360718\n",
      "epoch no.29 train no.261  loss = 0.7642213106155396\n",
      "epoch no.29 train no.271  loss = 1.530501127243042\n",
      "epoch no.29 train no.281  loss = 0.6960376501083374\n",
      "epoch no.29 train no.291  loss = 0.16143248975276947\n",
      "epoch no.29 train no.301  loss = 0.672118067741394\n",
      "epoch no.29 train no.311  loss = 0.4712555706501007\n",
      "epoch no.29 train no.321  loss = 0.6151249408721924\n",
      "epoch no.29 train no.331  loss = 0.7977303862571716\n",
      "epoch no.29 train no.341  loss = 0.7619926929473877\n",
      "epoch no.29 train no.351  loss = 0.37634342908859253\n",
      "epoch no.29 train no.361  loss = 0.2042817622423172\n",
      "epoch no.29 train no.371  loss = 0.21150152385234833\n",
      "epoch no.29 train no.381  loss = 1.738364815711975\n",
      "epoch no.29 train no.391  loss = 0.27764296531677246\n",
      "epoch no.29 train no.401  loss = 0.29295119643211365\n",
      "epoch no.29 train no.411  loss = 0.1784505695104599\n",
      "epoch no.29 train no.421  loss = 0.4790738523006439\n",
      "epoch no.29 train no.431  loss = 0.4258924722671509\n",
      "epoch no.29 train no.441  loss = 0.3660036325454712\n",
      "epoch no.29 train no.451  loss = 0.3670647144317627\n",
      "epoch no.29 train no.461  loss = 1.0391463041305542\n",
      "epoch no.29 train no.471  loss = 0.5117920637130737\n",
      "epoch no.29 train no.481  loss = 0.33722665905952454\n",
      "epoch no.29 train no.491  loss = 0.36445605754852295\n",
      "epoch no.29 train no.501  loss = 0.6497273445129395\n",
      "epoch no.29 train no.511  loss = 0.4665054380893707\n",
      "epoch no.29 train no.521  loss = 0.5247743129730225\n",
      "epoch no.29 train no.531  loss = 0.6356427669525146\n",
      "epoch no.29 train no.541  loss = 0.5291924476623535\n",
      "epoch no.29 train no.551  loss = 0.44035446643829346\n",
      "epoch no.29 train no.561  loss = 0.7488626837730408\n",
      "epoch no.29 train no.571  loss = 0.544453501701355\n",
      "epoch no.29 train no.581  loss = 0.41825753450393677\n",
      "epoch no.29 train no.591  loss = 0.19039742648601532\n",
      "epoch no.29 train no.601  loss = 0.7798807621002197\n",
      "epoch no.29 train no.611  loss = 0.6152989864349365\n",
      "epoch no.29 train no.621  loss = 0.3423575162887573\n",
      "epoch no.29 train no.631  loss = 0.5536338090896606\n",
      "epoch no.29 train no.641  loss = 0.39588287472724915\n",
      "epoch no.29 train no.651  loss = 0.799730658531189\n",
      "epoch no.29 train no.661  loss = 0.49385133385658264\n",
      "epoch no.29 train no.671  loss = 0.17529381811618805\n",
      "epoch no.29 train no.681  loss = 0.3724411129951477\n",
      "epoch no.29 train no.691  loss = 0.47835901379585266\n",
      "epoch no.29 train no.701  loss = 0.39480385184288025\n",
      "epoch no.29 train no.711  loss = 0.35502028465270996\n",
      "epoch no.29 train no.721  loss = 0.32424572110176086\n",
      "epoch no.29 train no.731  loss = 0.6748470067977905\n",
      "epoch no.29 train no.741  loss = 0.5726598501205444\n",
      "epoch no.29 train no.751  loss = 0.6652563214302063\n",
      "epoch no.29 train no.761  loss = 0.8697171211242676\n",
      "epoch no.29 train no.771  loss = 1.5991700887680054\n",
      "epoch no.29 train no.781  loss = 0.43939682841300964\n",
      "epoch no.29 train no.791  loss = 0.645919680595398\n",
      "epoch no.29 train no.801  loss = 0.42960822582244873\n",
      "epoch no.29 train no.811  loss = 0.47032561898231506\n",
      "epoch no.29 train no.821  loss = 0.5448151230812073\n",
      "epoch no.29 train no.831  loss = 0.3289133906364441\n",
      "epoch no.29 train no.841  loss = 0.2332693338394165\n",
      "epoch no.29 train no.851  loss = 0.3820069134235382\n",
      "epoch no.29 train no.861  loss = 0.6163951754570007\n",
      "epoch no.29 train no.871  loss = 0.48347026109695435\n",
      "epoch no.29 train no.881  loss = 0.5786380171775818\n",
      "epoch no.29 train no.891  loss = 1.0194294452667236\n",
      "epoch no.29 train no.901  loss = 0.4326823949813843\n",
      "epoch no.29 train no.911  loss = 0.1905202865600586\n",
      "epoch no.29 train no.921  loss = 0.6414110064506531\n",
      "epoch no.29 train no.931  loss = 0.2210395187139511\n",
      "epoch no.29 train no.941  loss = 0.37839841842651367\n",
      "epoch no.29 train no.951  loss = 0.2938067615032196\n",
      "epoch no.29 train no.961  loss = 0.4293128252029419\n",
      "epoch no.29 train no.971  loss = 0.7084278464317322\n",
      "epoch no.29 train no.981  loss = 0.9439971446990967\n",
      "epoch no.29 train no.991  loss = 0.5737870335578918\n",
      "epoch no.29 train no.1001  loss = 1.1136468648910522\n"
     ]
    }
   ],
   "source": [
    "print('KoGPT-2 Transfer Learning Start')\n",
    "epoch=30# 원래 200\n",
    "for epoch in range(epoch):\n",
    "  count = 0\n",
    "  for data in novel_data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    data = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
    "\n",
    "    data= data.transpose(1,0)\n",
    "    data= data.to(ctx)\n",
    "    \n",
    "    outputs = model(data, labels=data)\n",
    "    loss, logits = outputs[:2]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if count %10 ==0:\n",
    "      print('epoch no.{} train no.{}  loss = {}' . format(epoch, count+1, loss))\n",
    "      # torch.save(model,save_path+'checkpoint_{}_{}.tar'.format(epoch,count))\n",
    "      # 추론 및 학습 재개를 위한 일반 체크포인트 저장하기\n",
    "    if (count >0 and count%100==0) or (len(data) < batch_size):\n",
    "      torch.save({\n",
    "        'epoch': epoch,\n",
    "        'train_no': count,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss':loss\n",
    "      }, save_path+'narrativeKoGPT2_checkpoint.tar')\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by1zYlUWudTX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN2F5JshFIe4WPyUZzuAN8N",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NarrativeKoGPT2.ipynb",
   "provenance": [
    {
     "file_id": "1ChkS56shytnc2x8rXwaYwhhMJYPEQxaT",
     "timestamp": 1584444701945
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
